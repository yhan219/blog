{"pages":[{"title":"关于","text":"学习笔记 优质博客记录 加入一家创业公司,将用到的技术整理成册","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Alpine镜像中时区的设置","text":"缺省状态下Alpine镜像下的timezone会设定成UTC，相较于东八区的CST北京时间来说本地时间比UTC早了8个小时。本文介绍在Alpine中将时间从UTC设定为CST。 进入容器1docker exec -it containers-id /bin/sh 修改时区123apk add tzdatacp /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeecho &quot;Asia/Shanghai&quot; &gt; /etc/timezone","link":"/alpine-time-zone/"},{"title":"Redis 与 MySQL 双写一致性如何保证？","text":"四月份的时候，有位好朋友去美团面试。他说，被问到Redis与MySQL双写一致性如何保证？这道题其实就是在问缓存和数据库在双写场景下，一致性是如何保证的？本文将跟大家一起来探讨如何回答这个问题。 谈谈一致性 一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。 强一致性：这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大 弱一致性：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态 最终一致性：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型 三个经典的缓存模式缓存可以提升性能、缓解数据库压力，但是使用缓存也会导致数据不一致性的问题。一般我们是如何使用缓存呢？有三种经典的缓存使用模式： Cache-Aside Pattern Read-Through/Write-through Write-behind Cache-Aside PatternCache-Aside Pattern，即旁路缓存模式，它的提出是为了尽可能地解决缓存与数据库的数据不一致问题。 Cache-Aside读流程Cache-Aside Pattern的读请求流程如下： 读的时候，先读缓存，缓存命中的话，直接返回数据 缓存没有命中的话，就去读数据库，从数据库取出数据，放入缓存后，同时返回响应。 Cache-Aside 写流程Cache-Aside Pattern的写请求流程如下： 更新的时候，先更新数据库，然后再删除缓存。 Read-Through/Write-Through（读写穿透）Read/Write-Through模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过抽象缓存层完成的。 Read-ThroughRead-Through的简要流程如下 Read-Through简要流程 从缓存读取数据，读到直接返回 如果读取不到的话，从数据库加载，写入缓存后，再返回响应。 这个简要流程是不是跟Cache-Aside很像呢？其实Read-Through就是多了一层Cache-Provider而已，流程如下： Read-Through流程 Read-Through实际只是在Cache-Aside之上进行了一层封装，它会让程序代码变得更简洁，同时也减少数据源上的负载。 Write-ThroughWrite-Through模式下，当发生写请求时，也是由缓存抽象层完成数据源和缓存数据的更新,流程如下： Write-behind （异步缓存写入）Write-behind 跟Read-Through/Write-Through有相似的地方，都是由Cache Provider来负责缓存和数据库的读写。它们又有个很大的不同：Read/Write-Through是同步更新缓存和数据的，Write-Behind则是只更新缓存，不直接更新数据库，通过批量异步的方式来更新数据库。 Write behind流程 这种方式下，缓存和数据库的一致性不强，对一致性要求高的系统要谨慎使用。但是它适合频繁写的场景，MySQL的InnoDB Buffer Pool机制就使用到这种模式。 操作缓存的时候，到底是删除缓存呢，还是更新缓存？日常开发中，我们一般使用的就是Cache-Aside模式。有些小伙伴可能会问， Cache-Aside在写入请求的时候，为什么是删除缓存而不是更新缓存呢？ 我们在操作缓存的时候，到底应该删除缓存还是更新缓存呢？我们先来看个例子： 线程A先发起一个写操作，第一步先更新数据库 线程B再发起一个写操作，第二步更新了数据库 由于网络等原因，线程B先更新了缓存 线程A更新缓存。 这时候，缓存保存的是A的数据（老数据），数据库保存的是B的数据（新数据），数据不一致了，脏数据出现啦。如果是删除缓存取代更新缓存则不会出现这个脏数据问题。 更新缓存相对于删除缓存，还有两点劣势： 如果你写入的缓存值，是经过复杂计算才得到的话。更新缓存频率高的话，就浪费性能啦。 在写数据库场景多，读数据场景少的情况下，数据很多时候还没被读取到，又被更新了，这也浪费了性能呢(实际上，写多的场景，用缓存也不是很划算的,哈哈) 双写的情况下，先操作数据库还是先操作缓存？Cache-Aside缓存模式中，有些小伙伴还是会有疑问，在写请求过来的时候，为什么是先操作数据库呢？为什么不先操作缓存呢？ 假设有A、B两个请求，请求A做更新操作，请求B做查询读取操作。 线程A发起一个写操作，第一步del cache 此时线程B发起一个读操作，cache miss 线程B继续读DB，读出来一个老数据 然后线程B把老数据设置入cache 线程A写入DB最新的数据 酱紫就有问题啦，缓存和数据库的数据不一致了。缓存保存的是老数据，数据库保存的是新数据。因此，Cache-Aside缓存模式，选择了先操作数据库而不是先操作缓存。 个别小伙伴可能会问，先操作数据库再操作缓存，不一样也会导致数据不一致嘛？它俩又不是原子性操作的。这个是会的，但是这种方式，一般因为删除缓存失败等原因，才会导致脏数据，这个概率就很低。小伙伴们可以画下操作流程图，自己先分析下哈。接下来我们再来分析这种删除缓存失败的情况，如何保证一致性。 数据库和缓存数据保持强一致，可以嘛？实际上，没办法做到数据库与缓存绝对的一致性。 加锁可以嘛？并发写期间加锁，任何读操作不写入缓存？ 缓存及数据库封装CAS乐观锁，更新缓存时通过lua脚本？ 分布式事务，3PC？TCC？ 其实，这是由CAP理论决定的。缓存系统适用的场景就是非强一致性的场景，它属于CAP中的AP。个人觉得，追求绝对一致性的业务场景，不适合引入缓存。 ★ CAP理论，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。 ” 但是，通过一些方案优化处理，是可以保证弱一致性，最终一致性的。 3种方案保证数据库与缓存的一致性缓存延时双删有些小伙伴可能会说，并不一定要先操作数据库呀，采用缓存延时双删策略，就可以保证数据的一致性啦。什么是延时双删呢？ 先删除缓存 再更新数据库 休眠一会（比如1秒），再次删除缓存。 这个休眠一会，一般多久呢？都是1秒？ ★ 这个休眠时间 = 读业务逻辑数据的耗时 + 几百毫秒。为了确保读请求结束，写请求可以删除读请求可能带来的缓存脏数据。 ” 这种方案还算可以，只有休眠那一会（比如就那1秒），可能有脏数据，一般业务也会接受的。但是如果第二次删除缓存失败呢？缓存和数据库的数据还是可能不一致，对吧？给Key设置一个自然的expire过期时间，让它自动过期怎样？那业务要接受过期时间内，数据的不一致咯？还是有其他更佳方案呢？ 删除缓存重试机制不管是延时双删还是Cache-Aside的先操作数据库再删除缓存，都可能会存在第二步的删除缓存失败，导致的数据不一致问题。可以使用这个方案优化：删除失败就多删除几次呀,保证删除缓存成功就可以了呀~ 所以可以引入删除缓存重试机制 删除缓存重试流程 写请求更新数据库 缓存因为某些原因，删除失败 把删除失败的key放到消息队列 消费消息队列的消息，获取要删除的key 重试删除缓存操作 读取biglog异步删除缓存重试删除缓存机制还可以吧，就是会造成好多业务代码入侵。其实，还可以这样优化：通过数据库的binlog来异步淘汰key。 以mysql为例吧 可以使用阿里的canal将binlog日志采集发送到MQ队列里面 然后通过ACK机制确认处理这条更新消息，删除缓存，保证数据缓存一致性","link":"/cache-consistency/"},{"title":"使用arthas修改代码并热交换","text":"在运维过程中，有时候需要临时修改下服务器上已运行项目的代码。此时可使用arthas修改代码并热交换，达到不停机修改的目的。 测试案例假设服务器上已经运行一段代码如下： 12345678910111213141516171819202122package com.example.demo;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@SpringBootApplication@RestControllerpublic class Demo1Application { public static void main(String[] args) { SpringApplication.run(Demo1Application.class, args); } @GetMapping(&quot;/test&quot;) public String test() { return &quot;aaa&quot;; }} 此时放问接口返回aaa,需要修改返回结果为bbb 过程安装12curl -O https://arthas.aliyun.com/arthas-boot.jarjava -jar arthas-boot.jar 输出结果如下： 12345678[INFO] arthas-boot version: 3.5.4[INFO] Process 28783 already using port 3658[INFO] Process 28783 already using port 8563[INFO] Found existing java process, please choose one and input the serial number of the process, eg : 1. Then hit ENTER.* [1]: 28783 target/demo1-0.0.1-SNAPSHOT.jar [2]: 33008 org.jetbrains.jps.cmdline.Launcher [3]: 33009 org.jetbrains.jps.cmdline.Launcher [4]: 75256 输入1选择需要调试的进程 搜索搜索需要修改的类： 1sc *Demo1* 1com.example.demo.Demo1Application “Search-Method” 的简写，这个命令能搜索出所有已经加载了 Class 信息的方法信息。 反编译将class反编译为java文件到当前目录的Demo1Application.java中 1jad --source-only --lineNumber=false com.example.demo.Demo1Application &gt; ./Demo1Application.java jad 命令将 JVM 中实际运行的 class 的 byte code 反编译成 java 代码，便于你理解业务逻辑。 修改代码其他终端使用vim或其他编辑器修改生成在当前目录下的Demo1Application.java为如下： 1234@GetMapping(&quot;/test&quot;)public String test() { return &quot;bbb&quot;;} 查看classloader输入命令 1classloader 如下： 12345678910classloader name numberOfInstances loadedCountTotal org.springframework.boot.loader.LaunchedURLClassLoader 1 4208 BootstrapClassLoader 1 3922 com.taobao.arthas.agent.ArthasClassloader 1 2238 jdk.internal.loader.ClassLoaders$AppClassLoader 1 1198 jdk.internal.loader.ClassLoaders$PlatformClassLoader 1 157 jdk.internal.reflect.DelegatingClassLoader 47 47 Affect(row-cnt:6) cost in 7 ms. classloader 命令将 JVM 中所有的classloader的信息统计出来，并可以展示继承树，urls等。 重新编译选择SpringBoot的classLoader重新编译。 1mc --classLoaderClass=org.springframework.boot.loader.LaunchedURLClassLoader ./Demo1Application.java 1234567Memory compiler output:/Users/yhan219/IdeaProjects/demo1/com/example/demo/Demo1Application.classAffect(row-cnt:1) cost in 280 ms.[arthas@28783]$ retransform com/example/demo/Demo1Application.classretransform success, size: 1, classes:com.example.demo.Demo1Application 生成class文件到当前目录的com/example/demo/目录下。 Memory Compiler/内存编译器，编译.java文件生成.class。 交换1retransform com/example/demo/Demo1Application.class 12retransform success, size: 1, classes:com.example.demo.Demo1Application 结果此时再次访问该接口地址即返回了bbb。","link":"/arthas-hot-swap/"},{"title":"alfred workflow shell编写显示hosts示例","text":"Alfred是macOS上屡获殊荣的应用程序，可通过热键，关键字，文本扩展等功能提高效率。搜索Mac和网络，并通过自定义操作来控制Mac来提高生产力。 新建一个workflow点击左下角+–&gt;Templates–&gt;Essentials–&gt;Script Filter to Script，填写name和icon,新建完成后如下： 修改Script Filter双击第一个图形，进入修改页面，其中 keyword填写关键字，如hosts 参数选项选Argument Optional,即参数选填 language选/bin/bash,with input as argv with input as argv参数通过query=$1获取 with input as query参数通过query=&quot;{query}&quot;获取 编写脚本12345678910111213141516171819#!/bin/bashHOSTS=&quot;&quot;# Handle actionif [[ &quot;$1&quot; != &quot;&quot; ]]; then if [[ &quot;$1&quot; == &quot;Null&quot; ]]; then exit fi HOSTS=`cat /etc/hosts | grep '^[^#].*' | grep $1`else HOSTS=`cat /etc/hosts | grep '^[^#].*'`fiecho &quot;&lt;?xml version='1.0'?&gt;&lt;items&gt;&quot;while read -r HOST; do ARRAY=(${HOST// / }) echo &quot;&lt;item uid='${ARRAY[0]}' arg='${ARRAY[0]}'&gt;&lt;title&gt;${ARRAY[1]}&lt;/title&gt;&lt;subtitle&gt;${ARRAY[0]}&lt;/subtitle&gt;&lt;/item&gt;&quot;done &lt;&lt;&lt; &quot;$HOSTS&quot;echo &quot;&lt;/items&gt;&quot; alfred workflow主要是通过构建如下结构并输入： 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;items&gt; &lt;item uid=&quot;id&quot; arg=&quot;参数，可传递到下一个流程&quot; valid=&quot;yes&quot; autocomplete=&quot;yes&quot;&gt; &lt;title&gt;标题&lt;/title&gt; &lt;subtitle&gt;副标题)&lt;/subtitle&gt; &lt;icon&gt;图标，缺省显示应用图标&lt;/icon&gt; &lt;/item&gt; &lt;/items&gt; 填写完成后如下： 添加粘贴我们需要将选中的ip复制到粘贴板 界面上删除第二个图形 右键新建粘贴图形，右键–&gt;Outputs–&gt;Copy to Clipboard,弹出框直接save即可 界面上从图形一拉一条线到图形二即可完成后如下： 效果图选中选项后，回车，IP即拷贝到粘贴板 githubGitHub - yhan219/show_hosts: show hosts and passing selected to clipboard 参考文献Help and Support &gt; Workflows","link":"/alfred-workflow/"},{"title":"CentOS8更改yum源为阿里云","text":"CentOS 8修改yum源为阿里云 1. 备份原始的yum源12cd /etc/yum.repos.dcp CentOS-Base.repo CentOS-Base.repo.back 2. 下载对应版本的repo文件1wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo 3. 清除旧的yum缓存并生成新的yum缓存12yum clean allyum makecache 4. 更新1yum update -y","link":"/centos8-yum-repo/"},{"title":"CentOS 7安装 MySQL 8.0","text":"CentOS 7安装 MySQL 8.0 详细步骤 1、添加包12wget https://dev.mysql.com/get/mysql80-community-release-el8-1.noarch.rpmrpm -ivh mysql80-community-release-el8-1.noarch.rpm 如果是安装指定版本。到https://downloads.mysql.com/archives/community/ 页面选择版本后，替换下载链接即可，例如安装8.0.20版本 12345678wget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-common-8.0.20-1.el8.x86_64.rpmwget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-libs-8.0.20-1.el8.x86_64.rpmwget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-client-8.0.20-1.el8.x86_64.rpmwget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-server-8.0.20-1.el8.x86_64.rpmrpm -ivh mysql-community-common-8.0.20-1.el8.x86_64.rpmrpm -ivh mysql-community-libs-8.0.20-1.el8.x86_64.rpmrpm -ivh mysql-community-client-8.0.20-1.el8.x86_64.rpmrpm -ivh mysql-community-server-8.0.20-1.el8.x86_64.rpm 1yum install -y mysql-community-server 执行成功后直接跳到第4步配置文件 2、更新 yum 命令1yum clean all &amp;&amp; yum makecache 3、安装1yum install -y mysql-server 4、配置文件修改配置文件1vim /etc/my.cnf 123456789101112131415[mysqld]port = 3306character-set-server=utf8mb4collation-server=utf8mb4_general_ci# 表名不区分大小写(启动前配置)lower_case_table_names=1# 设置日志时区和系统一致log_timestamps=SYSTEM[client]default-character-set=utf8mb4 5、启动服务启动服务1systemctl start mysqld 查看版本信息1mysql -V 查看状态1systemctl status mysqld 开机启动12systemctl enable mysqldsystemctl daemon-reload 6、修改账号密码1、查看MySQL为Root账号生成的临时密码1grep &quot;A temporary password&quot; /var/log/mysqld.log 2、进入MySQL shell1mysql -u root -p 3、修改密码1ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!'; 7 、开启 MySQL 远程连接12345678910111213141516171819# 选择 mysql 数据库：USE mysql;# 在 mysql 数据库的 user 表中查看当前 root 用户的相关信息：SELECT host, user, authentication_string, plugin FROM user;# 设置root 用户远程访问：update user set host = '%' where user ='root';# 刷新权限：FLUSH PRIVILEGES;# 授权的所有权限GRANT ALL PRIVILEGES ON *.* TO 'root'@'%';# 更新 root 用户密码及加密规则（如果客户端不支持加密插件）：ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'MyNewPass4!';# 刷新权限：FLUSH PRIVILEGES; 8、新建远程用户1234567891011121314151617181920# 新建数据库CREATE DATABASE `test` CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_0900_ai_ci'# 新建远程用户CREATE USER 'devops'@'%' IDENTIFIED BY 'MyNewPass3!';# 赋予指定账户指定(数据库名称.表名)远程访问权限GRANT ALL PRIVILEGES ON test.* TO 'devops'@'%';# 查看权限SHOW GRANTS FOR 'devops'@'%';# 收回权限REVOKE ALL PRIVILEGES ON *.* FROM 'devops'@'%';# 删除用户DROP USER 'devops'@'%';# 刷新权限FLUSH PRIVILEGES; 9、 找回密码如果忘记了MySQL root密码，可以用以下方法重新设置： 12345678910111213141516171819202122232425262728293031323334# 1.在`/etc/my.conf`中`[mysqld]`下添加`skip-grant-tables`vim /etc/my.conf[mysqld]skip-grant-tables# 2.重启mysqlsystemctl restart mysqld# 3.无密码进入mysqlmysql -u root# 4.重置登录密码USE mysql;SELECT host, user, authentication_string, plugin FROM user;update user set host = '%' where user ='root';FLUSH PRIVILEGES;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%';ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'MyNewPass4!';FLUSH PRIVILEGES;# 退出exit# 5.在`/etc/my.conf`中`[mysqld]`下删除`skip-grant-tables`，添加#号注释vim /etc/my.conf[mysqld]#skip-grant-tables# 6.重启mysqlsystemctl restart mysqld# 7.使用新密码进入mysqlmysql -u root -p 10、启动失败1234# 权限问题chown mysql:mysql -R /var/run/mysqld/usr/sbin/mysqld --user=mysql &amp; 卸载mysql1yum remove mysql-server 12345rpm -qa|grep -i mysql如下：[root@ecs src]# rpm -qa|grep -i mysqlmysql80-community-release-el7-3.noarch 1rpm -ev mysql80-community-release-el7-3.noarch 12rm -rf /etc/my.cnfrm -rf /etc/my.cnf.d","link":"/centos_mysql8/"},{"title":"CentOS 7安装 docker compose","text":"centos安装docker compose教程docker compose依赖于docker,如未安装,请阅读 CentOS 7安装 docker教程 安装安装最新稳定版1sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 要安装其他版本的Compose，请替换`1.25.5`为要使用的Compose版本。 添加权限1sudo chmod +x /usr/local/bin/docker-compose 查看是否成功12docker-compose --versiondocker-compose version 1.25.5, build 1110ad01 卸载1sudo rm /usr/local/bin/docker-compose 参考文献 https://docs.docker.com/compose/install/ docker compose file官方编写指南","link":"/docker-compose/"},{"title":"CentOS 7安装 docker","text":"centos安装docker教程 卸载旧版本较旧的Docker版本称为docker或docker-engine。如果已安装这些程序，请卸载它们以及相关的依赖项。 12345678sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装1234567sudo yum install -y yum-utilssudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io 安装指定版本docker列出版本1yum list docker-ce --showduplicates | sort -r 1234docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable 安装以安装docker-ce-18.09.1为例 1sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io 启动1sudo systemctl start docker 卸载卸载Docker Engine，CLI和Containerd软件包1sudo yum remove docker-ce docker-ce-cli containerd.io 删除主机上的映像，容器，卷和自定义配置文件1sudo rm -rf /var/lib/docker 开机自启1systemctl enable docker 参考文献 https://docs.docker.com/engine/install/centos/ docker参考手册 docker file官方编写指南","link":"/docker/"},{"title":"docker compose 安装 gitea","text":"Gitea 的首要目标是创建一个极易安装，运行非常快速，安装和使用体验良好的自建 Git 服务。由于码云对免费企业版进行了人数限制,公司需要自建git服务,经对比,从gitlab,gogs和gitea中选中gitea 本安装教程依赖于docker compose,如未安装,请阅读 CentOS 7 安装 docker compose 教程 新建docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142version: '2'networks: gitea: external: falseservices: server: image: gitea/gitea:latest environment: - USER_UID=1000 - USER_GID=1000 - DB_TYPE=mysql - DB_HOST=db:3306 - DB_NAME=gitea - DB_USER=gitea - DB_PASSWD=gitea restart: always networks: - gitea volumes: - ./gitea:/data - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro ports: - '3000:3000' - '222:22' depends_on: - db db: image: mysql:5.7 restart: always environment: - MYSQL_ROOT_PASSWORD=gitea - MYSQL_USER=gitea - MYSQL_PASSWORD=gitea - MYSQL_DATABASE=gitea networks: - gitea volumes: - ./mysql:/var/lib/mysql 根据实际情况进行修改,例如,mac 上没有/etc/timezone和/etc/localtime,所以需要手动设置时区;另外由于系统使用了 mysql8,所以需要升级 mysql,修改后如下: 1234567891011121314151617181920212223242526272829303132333435363738394041version: &quot;2&quot;networks: gitea: external: falseservices: server: image: gitea/gitea:latest environment: - USER_UID=1000 - USER_GID=1000 - DB_TYPE=mysql - DB_HOST=db:3306 - DB_NAME=gitea - DB_USER=gitea - DB_PASSWD=gitea - TZ=Asia/Shanghai restart: always networks: - gitea volumes: - ./gitea:/data ports: - &quot;3000:3000&quot; - &quot;222:22&quot; depends_on: - db db: image: mysql:8.0.20 restart: always environment: - MYSQL_ROOT_PASSWORD=gitea - MYSQL_USER=gitea - MYSQL_PASSWORD=gitea - MYSQL_DATABASE=gitea networks: - gitea volumes: - ./mysql:/var/lib/mysql 最新文件内容从https://docs.gitea.io/en-us/install-with-docker/获取 运行1docker-compose -f docker-compose.yml up -d 安装访问http://localhost:3000/,点击右上角登录,进入安装界面,数据库等配置不用改,其他根据自身情况填写,填写完成后点击立即安装 使用点击注册,注册完成后即可登录,效果如下:","link":"/gitea/"},{"title":"CentOS挂载云盘","text":"今天就写写关于阿里云ESC，在liunx系统下挂载云盘的那点事。 云服务器挂载云盘目的描述1、把新购的云盘挂载到服务器的/www目录下 2、挂载的云盘，用于保存一些网页数据，以及一些数据库信息 3、安装宝塔面板（操作服务器方便，赖的每次SSH） 4.、这里是以CentOS 7.X版本作为演示 云盘挂载过程购买云盘你可以在你的服务器实例下，购买云盘，并绑定到你的服务器实例上，具体过程不再多说 注意：购买的云盘所在区，要与你服务器的所在区一致，购买后，要绑定到你实例中去 连接你的云服务器用SSH工具，连接你的服务器，并登录 阿里云，有服务器连接密码，和服务器登录密码，一定要分清哦 ESC开始挂载云盘1、新建立一个 www 的文件目录 命令： 1mkdir -p /www 2、查找未格式化的硬盘 命令 1fdisk -l 查找如果如下图所示 下面方框里为我自己的要挂载的云盘，为 /dev/xvdb ，这里的名称，可能因为服务器系统不同，或云盘的规格不同，多少有点变化， 不过阿里服务器的大多数，默认为：/dev/vdb Centos挂载磁盘,阿里云ESC挂载磁盘,阿里云挂云盘,宝塔面板挂载磁盘 3、对磁盘进行分区 命令 1fdisk /dev/xvdb 根据提示，分别输入 n p 1 wq 如下图，根据系统版本的不同，可能展示的内容也不同，不过最主要的提示是一样的 Centos挂载磁盘,阿里云ESC挂载磁盘,阿里云挂云盘,宝塔面板挂载磁盘 4、再次查看磁盘情况 命令 1fdisk -l 如下图所指示的，我们要分好区的磁盘列表 5、格式化分区 命令 1mkfs.ext4 /dev/xvdb1 这一步骤，可能处理的时间长一点，请耐心等待 Centos挂载磁盘,阿里云ESC挂载磁盘,阿里云挂云盘,宝塔面板挂载磁盘 6、将分区挂载信息添加到开启动挂载 命令 1echo &quot;/dev/xvdb1 /www ext4 defaults 0 0&quot; &gt;&gt; /etc/fstab 注意，命令中的 /www 为你第一部新建立的目录 7、重新挂载所有分区 命令 1mount -a Centos挂载磁盘,阿里云ESC挂载磁盘,阿里云挂云盘,宝塔面板挂载磁盘 8、检查分区信息 命令 1df Centos挂载磁盘,阿里云ESC挂载磁盘,阿里云挂云盘,宝塔面板挂载磁盘 安装宝塔面板挂载分区注意如果你要安装宝塔的服务器控制面板，一定要注意，挂载磁盘与安装面板的顺序 1、先安装系统 2、先按以上的步骤，挂载好云盘 3、挂载好云盘以后，再安装宝塔服务器面板 # 解除挂载","link":"/centos-mount-disk/"},{"title":"acme.sh生成免费https证书","text":"使用 acme.sh 申请 Let’s Encrypt 泛域名SSL证书教程acme.sh 实现了 acme 协议, 可以从 letsencrypt 生成免费的证书. github地址: https://github.com/acmesh-official/acme.sh 1. 安装 acme.sh安装很简单, 一个命令: 1curl https://get.acme.sh | sh 普通用户和 root 用户都可以安装使用. 安装过程进行了以下几步: 把 acme.sh 安装到你的 home 目录下: 1~/.acme.sh/ 并创建 一个 bash 的 alias, 方便你的使用: 1alias acme.sh=~/.acme.sh/acme.sh 2.生成证书请先前往阿里云后台获取App_Key跟App_Secret : https://ak-console.aliyun.com/#/accesskey 123# 替换成从阿里云后台获取的密钥export Ali_Key=&quot;你的 API KEY&quot;export Ali_Secret=&quot;你的 SECRET KEY&quot; 然后开始生成证书 1acme.sh --issue --dns dns_ali -d example.com -d *.example.com Ali_Key 和 Ali_Secret 信息会保存到 ~/.acme.sh/account.conf 中. 其他dns厂商: https://github.com/Neilpang/acme.sh/wiki/dnsapi 3.安装证书证书生成以后, 需要把证书 copy 到真正需要用它的地方. 1acme.sh --installcert -d example.com -d *.example.com --key-file /usr/local/nginx/conf.d/ssl/example/example.com.key --fullchain-file /usr/local/nginx/conf.d/ssl/example/fullchain.cer --reloadcmd &quot;nginx -s reload&quot; nginx配置 1234567891011121314151617181920212223242526server{ server_name example.com www.example.com; listen 80; return 301 https://$host$request_uri;}server { listen 443 ssl http2; server_name example.com www.example.com; ssl on; ssl_certificate /usr/local/nginx/ssl/example/fullchain.cer; ssl_certificate_key /usr/local/nginx/ssl/example/example.com.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / { proxy_pass http://127.0.0.1:8080; root /var/html/www; index index.html index.htm; }} 4. 更新证书查看定时任务crontab -l 123crontab -l58 0 * * * &quot;/root/.acme.sh&quot;/acme.sh --cron --home &quot;/root/.acme.sh&quot; &gt; /dev/null 目前证书在 60 天以后会自动更新, 你无需任何操作. 今后有可能会缩短这个时间, 不过都是自动的, 你不用关心. 5. 更新acme.sh目前由于acme 协议和 letsencrypt CA 都在频繁的更新, 因此 acme.sh 也经常更新以保持同步. 升级 acme.sh 到最新版 : 1acme.sh --upgrade 如果你不想手动升级, 可以开启自动升级: 1acme.sh --upgrade --auto-upgrade 之后, acme.sh 就会自动保持更新了. 你也可以随时关闭自动更新: 1acme.sh --upgrade --auto-upgrade 0","link":"/acme/"},{"title":"DateGrip或Idea通过ssh连接数据库","text":"上一篇我们介绍了Navicat通过ssh跳转连接数据库，现在介绍DateGrip或Idea的ssh隧道代理连接MySQL。原理是一样的。 假设有两台服务器，ip分别是192.168.1.2和192.168.191.3,我们通过192.168.1.2服务器连接到192.168.191.3的mysql.以DataGrip为例。 File–&gt;new–&gt;Data Source–&gt;MySql 选择ssh，如图： 点击左上角+号，如图： 其中： 主机：跳转机服务器ip 用户名：跳转机服务器用户名，不是数据库用户名 密码：跳转服务器密码 点击Test Connection连接测试，测试成功后点击Apply和OK。返回，选中刚添加的隧道 点击General,如图填写： 其中： 主机：mysql服务器ip 用户名：mysql用户名 密码：mysql密码 应用即可。如果数据库不显示表名，需要设置一下，如图： 然后勾选需要显示的数据库即可。","link":"/idea-ssh-mysql/"},{"title":"CentOS8时间同步","text":"CentOS 8使用chrony替代ntp做时间同步。 安装1yum install -y chrony 使用配置1vim /etc/chrony.conf 末尾添加： 12server s1a.time.edu.cn iburstserver ntp.aliyun.com iburst 设置时区1timedatectl set-timezone Asia/Shanghai 同步1timedatectl set-ntp true 启动12systemctl enable chronydsystemctl start chronyd 查看是否同步成功12# dateThu Jun 10 22:37:27 CST 2021","link":"/centos8-time-sync/"},{"title":"Http重定向状态码区别及对SEO优化网址URL劫持的影响","text":"http的重定向我们经常是张口就来，整个流程也非常简单，服务端HTTP返回码是30x，头里面的Location字段代表新的URL。 如下图所示： 但重定向也还是有需要深入探讨地方，返回码不仅有我们经常使用301和303还有302 307 308 它们有啥区别呢。可以按照是否缓存和重定向方法，两个维度去拆分。 缓存（永久重定向） 不缓存（临时重定向） 转GET 301 302、303 方法保持 308 307 如果是永久重定向那么浏览器客户端就会缓存此次重定向结果，下次如果有请求则直接从缓存读取，譬如我们切换域名，将所有老域名的流量转入新域名，可以使用永久重定向。 如果只是临时重定向那么浏览器则不会缓存，譬如我们的服务临时升级，会使用临时重定向。 方法保持的意思是原请求和重定向的请求是否使用相同的方法，譬如原请求是POST提交一个表单，如果是301重定向的话，重定向的请求会转为GET重新提交，如果是308则会保持原来POST请求不变。 一、状态码的解释1、301 Moved Permanently（永久移动） 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的URI之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。新的永久性的URI应当在响应的Location域中返回。除非这是一个HEAD请求，否则响应的实体中应当包含指向新的URI的超链接及简短说明。如果这不是一个GET或者HEAD请求，因此浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：对于某些使用HTTP/1.0协议的浏览器，当它们发送的POST请求得到了一个301响应的话，接下来的重定向请求将会变成GET方式。 123456789101112// 客户端发出请求GET /blog HTTP/1.1Host:www.example.com// 服务端响应，不带Cache-Control头部HTTP/1.1 302 Moved PermanentlyLocation:http://www.example.org/index.asp// 服务端响应，带Cache-Control头部HTTP/1.1 302 Moved PermanentlyLocation:http://www.example.org/index.aspCache-Control:private;max-age=600; 2、302 Found（发现） 要求客户端执行临时重定向（原始描述短语为“Moved Temporarily”）。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。新的临时性的URI应当在响应的Location域中返回。除非这是一个HEAD请求，否则响应的实体中应当包含指向新的URI的超链接及简短说明。如果这不是一个GET或者HEAD请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用GET方式访问在Location中规定的URI，而无视原先请求的方法。因此状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。 3、307 Temporary Redirect（临时重定向） 在这种情况下，请求应该与另一个URI重复，但后续的请求应仍使用原始的URI。 与302相反，当重新发出原始请求时，不允许更改请求方法。 例如，应该使用另一个POST请求来重复POST请求 4、308 Permanent Redirect （永久重定向） 请求和所有将来的请求应该使用另一个URI重复。 307和308分别重复302和301的行为，但不允许HTTP方法更改。 例如，将表单提交给永久重定向的资源可能会顺利进行。 5、301/302/303/307/308的区别 301，302是http1.0的内容，303、307、308是http1.1的内容。 301和302本来在规范中是不允许重定向时改变请求方法的（将POST改为GET），但是许多浏览器却允许重定向时改变请求方法（这是一种不规范的实现）。 303的出现正是为了给上面的301，302这种行为作出个规范（将错就错吧），也就是允许重定向时改变请求方法。此外303响应禁止被缓存。 大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的，所以303基本用的很少，一般用302。 307和308的出现也是给上面的行为做个规范，不过是不允许重定向时改变请求方法。 总结一下就是： （1）301、308是永久改变地址；302/303、307是临时改变地址； （2）301、302允许改变请求方法（post会改get）；308、307不允许改变请求方法（post方法还是post方法） 注：永久（Permanent）和临时（Temporary）的区别：永久是指原来访问的资源已经永久删除啦，客户端应该根据新的URI访问重定向。临时是指访问的资源可能暂时先用location的URI访问，但旧资源还在的，下次你再来访问的时候可能就不用重定向了。 （3）对SEO的友好性：（这里有些面试会问） 301与302的区别：301表示搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址；302表示旧地址A的资源还在（仍然可以访问），这个重定向只是临时地从旧地址A跳转到地址B，搜索引擎会抓取新的内容而保存旧的网址。 由此可知301对于SEO更友好，下面来详细介绍。 二、301、302对SEO优化的影响及网址URL劫持 301和302跳转，虽然最终看到的效果是一样的，但对搜索引擎来讲，301和302还是有区别的。301的含义是“永久重定向”，而302的含义是“临时重定向”，那么，为什么不能用302呢？302 重定向和网址劫持（URL hijacking）有什么关系呢？这要从搜索引擎如何处理302转向说起。 从定义来说，从网址A做一个302重定向到网址B时，主机服务器的隐含意思是 网址A随时有可能改主意，重新显示本身的内容或转向其他的地方。大部分的搜索引擎在大部分情况下，当收到302重定向时，一般只要去抓取目标网址就可以了，也就是说网址B。实际上如果搜索引擎在遇到302转向时，百分之百的都抓取目标网址B的话，就不用担心网址URL劫持了。 问题就在于，有的时候搜索引擎，尤其是Google，并不能总是抓取目标网址。为什么呢？比如说，有的时候A网址很短，但是它做了一个302重定向到B网 址，而B网址是一个很长的乱七八糟的URL网址，甚至还有可能包含一些问号之类的参数。很自然的，A网址更加用户友好，而B网址既难看，又不用户友好。这 时Google很有可能会仍然显示网址A。 由于搜索引擎排名算法只是程序而不是人，在遇到302重定向的时候，并不能像人一样的去准确判定哪一个网址更适当，这就造成了网址URL劫持的可能性。也就是说，一个不道德的人在他自己的网址A做一个302重定向到你的网址B，出于某种原因， Google搜索结果所显示的仍然是网址A，但是所用的网页内容却是你的网址B上的内容，这种情况就叫做网址URL劫持。你辛辛苦苦所写的内容就这样被别 人偷走了。 其实302的跳转本身是没有错的，但因为被一些作弊者用多了，Google当然对这个就比较敏感了，毕竟Google面对的是如此海量的数据，你难道不怕被误杀吗？Google的官方内容一再强调用301来转移内容，况且，301和302在程序上的设置相差很小，既然如此，何必要冒险用302呢？如果你对上面还是没有看懂看看下面的的内容就知道了。 PR劫持的SEO作弊方法： 1、利用301和302跳转 一般搜索引擎在处理301和302转向的时候，都是把目标URL当作实际应该收录的URL。如果你从域名A做301或者是302跳转到域名B，而域名B的 PR 值比较高，域名A在PR更新后，也会显示域名B的PR值。PR挟持最简单的就是先做301或302跳转到高PR的域名B，等PR更新过后，立刻取消转向， 同时也获得了和B站相同的PR值。这个做假的PR显示值至少维持到下一次PR更新。 我的理解是，从网站A（网站比较烂）上做了一个302跳转到网站B（搜索排名很靠前），这时候有时搜索引擎会使用网站B的内容，但却收录了网站A的地址，这样在不知不觉间，网站B在为网站A作贡献，网站A的排名就靠前了。 302 重定向所造成的网址URL劫持现象，已经存在一段时间了。不过到目前为止，似乎也没有什么更好的解决方法。在正在进行的谷歌数据中心转换中，302 重定向问题也是要被解决的目标之一。从一些搜索结果来看，网址劫持现象有所改善，但是并没有完全解决。 301跳转对查找引擎是一种对照驯良的跳转编制，也是查找引擎能够遭遇的跳转编制，它告诉查找引擎，这个地址弃用了，永远转向一个新地址，可以转移新域名的权重。而302重定向很容易被搜索引擎误认为是利用多个域名指向同一网站，那么你的网站就会被封掉，罪名是“利用重复的内容来干扰Google搜索结果的网站排名”。 2、欺骗Google蜘蛛 通过程序检测到Google蜘蛛，返回301或302转向，对普通访问者和其他蜘蛛都返回正常内容。这样我们看到的是普通网站，只有Google会看到转向，但是这种网站上的链接对PR值没有任何贡献。 三、使用场景 因为301与302的区别，所以导致产生302网址劫持，故不建议使用302重定向（然而浏览器默认是使用302重定向） 1、使用301的场景：（一般是资源位置永久更改）（1）域名到期不想续费（或者发现了更适合网站的域名），想换个域名。 （2）在搜索引擎的搜索结果中出现了不带www的域名，而带www的域名却没有收录，这个时候可以用301重定向来告诉搜索引擎我们目标的域名是哪一个。 （3）空间服务器不稳定，换空间的时候。 注：另外，返回301请求码进行跳转被谷歌认为是将网站地址由 HTTP 迁移到 HTTPS的最佳方法(然而大家都用302。。。。) 2、使用302的场景：（一般是普通的重定向需求：临时跳转）（1）未登录前先使用302重定向到登录页面，登录成功后再跳回到原来请求的页面 举个例子，比如我未登录京东前我就访问京东的个人界面https://home.jd.com/，然后就会重定向到登录界面，我们可以通过浏览器的dev-tool查看状态码，有 我们可以发现响应的状态码为302，并且返回了location为登录界面的url，并且附带了ReturnUrl方便我们登录后跳回到https://home.jd.com/ （2）像微博之类的使用短域名，用户浏览后需要重定向到真实的地址之类。 例如我访问一个微博的秒拍视频链接：http://t.cn/RuUMBnI，然后重定向到了实际的视频地址miaopai.com，状态码为302。 3、使用307或308的场景 307很少用，与302类似，只不过是针对POST方法的请求不允许更改方法，不过我在访问百度时，发现用了307状态码 308也很少用，与301类似，只不过是针对POST方法的请求不允许更改方法。","link":"/http_forwarding/"},{"title":"Jetbrains系列产品无限试用方法","text":"随着Jetbrains产品更新，目前大部分激活工具均已失效。本文介绍Idea等一系列Jetbrains无限试用方法，包含付费插件无限试用。感谢zhile.io站长提供的插件！！！ 感谢zhile.io站长提供的插件！！！ 感谢zhile.io站长提供的插件！！！ 本插件开源地址https://gitee.com/pengzhile/ide-eval-resetter 安装添加第三方插件仓库地址在Settings/Preferences... -&gt; Plugins 内手动添加第三方插件仓库地址：https://plugins.zhile.io 安装插件在插件市场搜索IDE Eval Reset并安装即可。 使用 一般来说，在IDE窗口切出去或切回来时（窗口失去/得到焦点）会触发事件，检测是否长时间（25天）没有重置，给通知让你选择。（初次安装因为无法获取上次重置时间，会直接给予提示） 也可以手动唤出插件的主界面： 如果IDE没有打开项目，在Welcome界面点击菜单：Get Help -&gt; Eval Reset 如果IDE打开了项目，点击菜单：Help -&gt; Eval Reset 唤出的插件主界面中包含了一些显示信息，2个按钮，1个勾选项： 按钮：Reload 用来刷新界面上的显示信息。 按钮：Reset 点击会询问是否重置试用信息并重启IDE。选择Yes则执行重置操作并重启IDE生效，选择No则什么也不做。（此为手动重置方式） 勾选项：Auto reset before per restart 如果勾选了，则自勾选后每次重启/退出IDE时会自动重置试用信息，你无需做额外的事情。（此为自动重置方式） 更新插件更新会收到提示，自行选择更新即可。 付费插件无限试用插件市场付费插件的试用信息也会一并重置。理论上所有付费插件都能无限试用。重置需要重启IDE才能生效！ MyBatisCodeHelperPro插件有两个版本如下，功能完全相同，安装时需安装MyBatisCodeHelperPro (Marketplace Edition)才能充值！ MyBatisCodeHelperPro (Marketplace Edition)，可重置！ MyBatisCodeHelperPro，不可重置！ 对于某些付费插件（如: Iedis 2, MinBatis）来说，你可能需要去取掉javaagent配置（如果有）后重启IDE： 如果IDE没有打开项目，在Welcome界面点击菜单：Configure -&gt; Edit Custom VM Options... -&gt; 移除 -javaagent: 开头的行。 如果IDE打开了项目，点击菜单：Help -&gt; Edit Custom VM Options... -&gt; 移除 -javaagent: 开头的行。 支持的产品 IntelliJ IDEA AppCode CLion DataGrip GoLand PhpStorm PyCharm Rider RubyMine WebStorm","link":"/jetbrains-reset/"},{"title":"线上Java进程OOM被系统杀死问题排查","text":"记录一次线上java进程被系统进程杀死后排查问题的经过。 添加 OOM dump再启动脚本中添加VM参数添加如下： 1-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/log/jvm/ 故障原因排查查看是否是OOM原因导致的进程被杀1grep &quot;Out of memory&quot; /var/log/messages 12# grep &quot;Out of memory&quot; /var/log/messagesJun 11 09:31:44 ecs kernel: Out of memory: Kill process 19845 (java) score 389 or sacrifice child 可以看到在9:31的时候19845进程因为Out of memory被kill掉了。 或使用： 1egrep -i -r 'killed process' /var/log/messages 12egrep -i -r 'killed process' /var/log/messagesJun 11 09:31:44 ecs kernel: Killed process 19845 (java), UID 1000, total-vm:8437188kB, anon-rss:3111856kB, file-rss:0kB, shmem-rss:0kB 或使用： 1dmesg -T | grep java 未完待续","link":"/java-oom/"},{"title":"分布式锁用 Redis 还是 Zookeeper？","text":"分布式锁用 Redis 还是 Zookeeper？ 为什么用分布式锁？在讨论这个问题之前，我们先来看一个业务场景： 系统A是一个电商系统，目前是一台机器部署，系统中有一个用户下订单的接口，但是用户下订单之前一定要去检查一下库存，确保库存足够了才会给用户下单。 由于系统有一定的并发，所以会预先将商品的库存保存在redis中，用户下单的时候会更新redis的库存。 此时系统架构如下： 但是这样一来会产生一个问题：假如某个时刻，redis里面的某个商品库存为1，此时两个请求同时到来，其中一个请求执行到上图的第3步，更新数据库的库存为0，但是第4步还没有执行。 而另外一个请求执行到了第2步，发现库存还是1，就继续执行第3步。 这样的结果，是导致卖出了2个商品，然而其实库存只有1个。 很明显不对啊！这就是典型的库存超卖问题 此时，我们很容易想到解决方案：用锁把2、3、4步锁住，让他们执行完之后，另一个线程才能进来执行第2步。 按照上面的图，在执行第2步时，使用Java提供的synchronized或者ReentrantLock来锁住，然后在第4步执行完之后才释放锁。 这样一来，2、3、4 这3个步骤就被“锁”住了，多个线程之间只能串行化执行。 但是好景不长，整个系统的并发飙升，一台机器扛不住了。现在要增加一台机器，如下图： 增加机器之后，系统变成上图所示，我的天！ 假设此时两个用户的请求同时到来，但是落在了不同的机器上，那么这两个请求是可以同时执行了，还是会出现库存超卖的问题。 为什么呢？因为上图中的两个A系统，运行在两个不同的JVM里面，他们加的锁只对属于自己JVM里面的线程有效，对于其他JVM的线程是无效的。 因此，这里的问题是：Java提供的原生锁机制在多机部署场景下失效了 这是因为两台机器加的锁不是同一个锁(两个锁在不同的JVM里面)。 那么，我们只要保证两台机器加的锁是同一个锁，问题不就解决了吗？ 此时，就该分布式锁隆重登场了，分布式锁的思路是： 在整个系统提供一个全局、唯一的获取锁的“东西”，然后每个系统在需要加锁时，都去问这个“东西”拿到一把锁，这样不同的系统拿到的就可以认为是同一把锁。 至于这个“东西”，可以是Redis、Zookeeper，也可以是数据库。 文字描述不太直观，我们来看下图： 通过上面的分析，我们知道了库存超卖场景在分布式部署系统的情况下使用Java原生的锁机制无法保证线程安全，所以我们需要用到分布式锁的方案。 那么，如何实现分布式锁呢？接着往下看！ 基于Redis实现分布式锁上面分析为啥要使用分布式锁了，这里我们来具体看看分布式锁落地的时候应该怎么样处理。扩展：Redisson是如何实现分布式锁的？ 最常见的一种方案就是使用Redis做分布式锁 使用Redis做分布式锁的思路大概是这样的：在redis中设置一个值表示加了锁，然后释放锁的时候就把这个key删除。 具体代码是这样的： 12345678910111213// 获取锁// NX是指如果key不存在就成功，key存在返回false，PX可以指定过期时间SET anyLock unique_value NX PX 30000// 释放锁：通过执行一段lua脚本// 释放锁涉及到两条指令，这两条指令不是原子性的// 需要用到redis的lua脚本支持特性，redis执行lua脚本是原子性的if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] thenreturn redis.call(&quot;del&quot;,KEYS[1])elsereturn 0end 这种方式有几大要点： 一定要用SET key value NX PX milliseconds 命令 如果不用，先设置了值，再设置过期时间，这个不是原子性操作，有可能在设置过期时间之前宕机，会造成死锁(key永久存在) value要具有唯一性 这个是为了在解锁的时候，需要验证value是和加锁的一致才删除key。 这是避免了一种情况：假设A获取了锁，过期时间30s，此时35s之后，锁已经自动释放了，A去释放锁，但是此时可能B获取了锁。A客户端就不能删除B的锁了。 除了要考虑客户端要怎么实现分布式锁之外，还需要考虑redis的部署问题。 redis有3种部署方式： 单机模式 master-slave + sentinel选举模式 redis cluster模式 使用redis做分布式锁的缺点在于：如果采用单机部署模式，会存在单点问题，只要redis故障了。加锁就不行了。 采用master-slave模式，加锁的时候只对一个节点加锁，即便通过sentinel做了高可用，但是如果master节点故障了，发生主从切换，此时就会有可能出现锁丢失的问题。 基于以上的考虑，其实redis的作者也考虑到这个问题，他提出了一个RedLock的算法，这个算法的意思大概是这样的： 假设redis的部署模式是redis cluster，总共有5个master节点，通过以下步骤获取一把锁： 获取当前时间戳，单位是毫秒 轮流尝试在每个master节点上创建锁，过期时间设置较短，一般就几十毫秒 尝试在大多数节点上建立一个锁，比如5个节点就要求是3个节点（n / 2 +1） 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了 要是锁建立失败了，那么就依次删除这个锁 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁 但是这样的这种算法还是颇具争议的，可能还会存在不少的问题，无法保证加锁的过程一定正确。 另一种方式：Redisson此外，实现Redis的分布式锁，除了自己基于redis client原生api来实现之外，还可以使用开源框架：Redission Redisson是一个企业级的开源Redis Client，也提供了分布式锁的支持。我也非常推荐大家使用，为什么呢？ 回想一下上面说的，如果自己写代码来通过redis设置一个值，是通过下面这个命令设置的。 SET anyLock unique_value NX PX 30000 这里设置的超时时间是30s，假如我超过30s都还没有完成业务逻辑的情况下，key会过期，其他线程有可能会获取到锁。 这样一来的话，第一个线程还没执行完业务逻辑，第二个线程进来了也会出现线程安全问题。所以我们还需要额外的去维护这个过期时间，太麻烦了~ 我们来看看redisson是怎么实现的？先感受一下使用redission的爽： 123456789101112131415Config config = new Config();config.useClusterServers().addNodeAddress(&quot;redis://192.168.31.101:7001&quot;).addNodeAddress(&quot;redis://192.168.31.101:7002&quot;).addNodeAddress(&quot;redis://192.168.31.101:7003&quot;).addNodeAddress(&quot;redis://192.168.31.102:7001&quot;).addNodeAddress(&quot;redis://192.168.31.102:7002&quot;).addNodeAddress(&quot;redis://192.168.31.102:7003&quot;);RedissonClient redisson = Redisson.create(config);RLock lock = redisson.getLock(&quot;anyLock&quot;);lock.lock();lock.unlock(); 就是这么简单，我们只需要通过它的api中的lock和unlock即可完成分布式锁，他帮我们考虑了很多细节： redisson所有指令都通过lua脚本执行，redis支持lua脚本原子性执行 redisson设置一个key的默认过期时间为30s,如果某个客户端持有一个锁超过了30s怎么办？ redisson中有一个watchdog的概念，翻译过来就是看门狗，它会在你获取锁之后，每隔10秒帮你把key的超时时间设为30s 这样的话，就算一直持有锁也不会出现key过期了，其他线程获取到锁的问题了。 redisson的“看门狗”逻辑保证了没有死锁发生。 (如果机器宕机了，看门狗也就没了。此时就不会延长key的过期时间，到了30s之后就会自动过期了，其他线程可以获取到锁) 这里稍微贴出来其实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384// 加锁逻辑private &lt;T&gt; RFuture&lt;Long&gt; tryAcquireAsync(long leaseTime, TimeUnit unit, final long threadId) { if (leaseTime != -1) { return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG); } // 调用一段lua脚本，设置一些key、过期时间 RFuture&lt;Long&gt; ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.addListener(new FutureListener&lt;Long&gt;() { @Override public void operationComplete(Future&lt;Long&gt; future) throws Exception { if (!future.isSuccess()) { return; } Long ttlRemaining = future.getNow(); // lock acquired if (ttlRemaining == null) { // 看门狗逻辑 scheduleExpirationRenewal(threadId); } } }); return ttlRemainingFuture;}&lt;T&gt; RFuture&lt;T&gt; tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand&lt;T&gt; command) { internalLockLeaseTime = unit.toMillis(leaseTime); return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command, &quot;if (redis.call('exists', KEYS[1]) == 0) then &quot; + &quot;redis.call('hset', KEYS[1], ARGV[2], 1); &quot; + &quot;redis.call('pexpire', KEYS[1], ARGV[1]); &quot; + &quot;return nil; &quot; + &quot;end; &quot; + &quot;if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then &quot; + &quot;redis.call('hincrby', KEYS[1], ARGV[2], 1); &quot; + &quot;redis.call('pexpire', KEYS[1], ARGV[1]); &quot; + &quot;return nil; &quot; + &quot;end; &quot; + &quot;return redis.call('pttl', KEYS[1]);&quot;, Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId));}// 看门狗最终会调用了这里private void scheduleExpirationRenewal(final long threadId) { if (expirationRenewalMap.containsKey(getEntryName())) { return; } // 这个任务会延迟10s执行 Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { // 这个操作会将key的过期时间重新设置为30s RFuture&lt;Boolean&gt; future = renewExpirationAsync(threadId); future.addListener(new FutureListener&lt;Boolean&gt;() { @Override public void operationComplete(Future&lt;Boolean&gt; future) throws Exception { expirationRenewalMap.remove(getEntryName()); if (!future.isSuccess()) { log.error(&quot;Can't update lock &quot; + getName() + &quot; expiration&quot;, future.cause()); return; } if (future.getNow()) { // reschedule itself // 通过递归调用本方法，无限循环延长过期时间 scheduleExpirationRenewal(threadId); } } }); } }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); if (expirationRenewalMap.putIfAbsent(getEntryName(), new ExpirationEntry(threadId, task)) != null) { task.cancel(); }} 另外，redisson还提供了对redlock算法的支持, 它的用法也很简单： 1234567RedissonClient redisson = Redisson.create(config);RLock lock1 = redisson.getFairLock(&quot;lock1&quot;);RLock lock2 = redisson.getFairLock(&quot;lock2&quot;);RLock lock3 = redisson.getFairLock(&quot;lock3&quot;);RedissonRedLock multiLock = new RedissonRedLock(lock1, lock2, lock3);multiLock.lock();multiLock.unlock(); 小结： 本节分析了使用redis作为分布式锁的具体落地方案 以及其一些局限性 然后介绍了一个redis的客户端框架redisson， 这也是我推荐大家使用的， 比自己写代码实现会少care很多细节。 基于zookeeper实现分布式锁常见的分布式锁实现方案里面，除了使用redis来实现之外，使用zookeeper也可以实现分布式锁。 在介绍zookeeper(下文用zk代替)实现分布式锁的机制之前，先粗略介绍一下zk是什么东西： Zookeeper是一种提供配置管理、分布式协同以及命名的中心化服务。 zk的模型是这样的：zk包含一系列的节点，叫做znode，就好像文件系统一样每个znode表示一个目录，然后znode有一些特性： 有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点； zookeeper提供了一个可选的有序特性，例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号 也就是说，如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。 临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件： 节点创建 节点删除 节点数据修改 子节点变更 基于以上的一些zk的特性，我们很容易得出使用zk实现分布式锁的落地方案： 使用zk的临时节点和有序节点，每个线程获取锁就是在zk创建一个临时有序的节点，比如在/lock/目录下。 创建节点成功后，获取/lock目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点 如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功。 如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的前一个节点添加一个事件监听。 比如当前线程获取到的节点序号为/lock/003,然后所有的节点列表为[/lock/001,/lock/002,/lock/003],则对/lock/002这个节点添加一个事件监听器。 如果锁释放了，会唤醒下一个序号的节点，然后重新执行第3步，判断是否自己的节点序号是最小。 比如/lock/001释放了，/lock/002监听到时间，此时节点集合为[/lock/002,/lock/003],则/lock/002为最小序号节点，获取到锁。 整个过程如下： 具体的实现思路就是这样，至于代码怎么写，这里比较复杂就不贴出来了。 Curator介绍Curator是一个zookeeper的开源客户端，也提供了分布式锁的实现。 他的使用方式也比较简单： 123InterProcessMutex interProcessMutex = new InterProcessMutex(client,&quot;/anyLock&quot;);interProcessMutex.acquire();interProcessMutex.release(); 其实现分布式锁的核心源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private boolean internalLockLoop(long startMillis, Long millisToWait, String ourPath) throws Exception{ boolean haveTheLock = false; boolean doDelete = false; try { if ( revocable.get() != null ) { client.getData().usingWatcher(revocableWatcher).forPath(ourPath); } while ( (client.getState() == CuratorFrameworkState.STARTED) &amp;&amp; !haveTheLock ) { // 获取当前所有节点排序后的集合 List&lt;String&gt; children = getSortedChildren(); // 获取当前节点的名称 String sequenceNodeName = ourPath.substring(basePath.length() + 1); // +1 to include the slash // 判断当前节点是否是最小的节点 PredicateResults predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases); if ( predicateResults.getsTheLock() ) { // 获取到锁 haveTheLock = true; } else { // 没获取到锁，对当前节点的上一个节点注册一个监听器 String previousSequencePath = basePath + &quot;/&quot; + predicateResults.getPathToWatch(); synchronized(this){ Stat stat = client.checkExists().usingWatcher(watcher).forPath(previousSequencePath); if ( stat != null ){ if ( millisToWait != null ){ millisToWait -= (System.currentTimeMillis() - startMillis); startMillis = System.currentTimeMillis(); if ( millisToWait &lt;= 0 ){ doDelete = true; // timed out - delete our node break; } wait(millisToWait); }else{ wait(); } } } // else it may have been deleted (i.e. lock released). Try to acquire again } } } catch ( Exception e ) { doDelete = true; throw e; } finally{ if ( doDelete ){ deleteOurPath(ourPath); } } return haveTheLock;} 其实curator实现分布式锁的底层原理和上面分析的是差不多的。这里我们用一张图详细描述其原理： 小结： 本节介绍了zookeeperr实现分布式锁的方案以及zk的开源客户端的基本使用，简要的介绍了其实现原理。相关可以参考：肝一下ZooKeeper实现分布式锁的方案，附带实例！ 两种方案的优缺点比较学完了两种分布式锁的实现方案之后，本节需要讨论的是redis和zk的实现方案中各自的优缺点。 对于redis的分布式锁而言，它有以下缺点： 它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。 另外来说的话，redis的设计定位决定了它的数据并不是强一致性的，在某些极端情况下，可能会出现问题。锁的模型不够健壮 即便使用redlock算法来实现，在某些复杂场景下，也无法保证其实现100%没有问题，关于redlock的讨论可以看How to do distributed locking redis分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 但是另一方面使用redis实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的“极端复杂场景” 所以使用redis作为分布式锁也不失为一种好的方案，最重要的一点是redis的性能很高，可以支撑高并发的获取、释放锁操作。 对于zk分布式锁而言: zookeeper天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁。 如果获取不到锁，只需要添加一个监听器就可以了，不用一直轮询，性能消耗较小。 但是zk也有其缺点：如果有较多的客户端频繁的申请加锁、释放锁，对于zk集群的压力会比较大。 小结： 综上所述，redis和zookeeper都有其优缺点。我们在做技术选型的时候可以根据这些问题作为参考因素。 建议通过前面的分析，实现分布式锁的两种常见方案：redis和zookeeper，他们各有千秋。应该如何选型呢？ 就个人而言的话，我比较推崇zk实现的锁： 因为redis是有可能存在隐患的，可能会导致数据不对的情况。但是，怎么选用要看具体在公司的场景了。 如果公司里面有zk集群条件，优先选用zk实现，但是如果说公司里面只有redis集群，没有条件搭建zk集群。 那么其实用redis来实现也可以，另外还可能是系统设计者考虑到了系统已经有redis，但是又不希望再次引入一些外部依赖的情况下，可以选用redis。 这个是要系统设计者基于架构的考虑了","link":"/lock-with-redis-or-zookeeper/"},{"title":"MySQL索引原理及慢查询优化","text":"MySQL凭借着出色的性能、低廉的成本、丰富的资源，已经成为绝大多数互联网公司的首选关系型数据库。虽然性能出色，但所谓“好马配好鞍”，如何能够更好的使用它，已经成为开发工程师的必修课，我们经常会从职位描述上看到诸如“精通MySQL”、“SQL语句优化”、“了解数据库原理”等要求。我们知道一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，所以查询语句的优化显然是重中之重。 背景本人从2013年7月份起，一直在美团核心业务系统部做慢查询的优化工作，共计十余个系统，累计解决和积累了上百个慢查询案例。随着业务的复杂性提升，遇到的问题千奇百怪，五花八门，匪夷所思。本文旨在以开发工程师的角度来解释数据库索引的原理和如何优化慢查询。 一个慢查询引发的思考12345678910select count(*) from task where status=2 and operator_id=20839 and operate_time&gt;1371169729 and operate_time&lt;1371174603 and type=2; 系统使用者反应有一个功能越来越慢，于是工程师找到了上面的SQL。 并且兴致冲冲的找到了我，“这个SQL需要优化，给我把每个字段都加上索引”。 我很惊讶，问道：“为什么需要每个字段都加上索引？” “把查询的字段都加上索引会更快”，工程师信心满满。 “这种情况完全可以建一个联合索引，因为是最左前缀匹配，所以operate_time需要放到最后，而且还需要把其他相关的查询都拿来，需要做一个综合评估。” “联合索引？最左前缀匹配？综合评估？”工程师不禁陷入了沉思。 多数情况下，我们知道索引能够提高查询效率，但应该如何建立索引？索引的顺序如何？许多人却只知道大概。其实理解这些概念并不难，而且索引的原理远没有想象的那么复杂。 MySQL索引原理索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？ 索引原理除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。 数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&gt;、&lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。 磁盘IO与预读前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考： 考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 索引的数据结构前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。 详解b+树 如上图，是一颗b+树，关于b+树的定义可以参见B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。 b+树的查找过程如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。 b+树性质1.通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。 2.当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。 慢查询优化关于MySQL索引原理是比较枯燥的东西，大家只需要有一个感性的认识，并不需要理解得非常透彻和深入。我们回头来看看一开始我们说的慢查询，了解完索引原理之后，大家是不是有什么想法呢？先总结一下索引的几大基本原则： 建索引的几大原则1.最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 2.=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 3.尽量选择区分度高的列作为索引，区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。 4.索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。 5.尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 回到开始的慢查询根据最左匹配原则，最开始的sql语句的索引应该是status、operator_id、type、operate_time的联合索引；其中status、operator_id、type的顺序可以颠倒，所以我才会说，把这个表的所有相关查询都找到，会综合分析；比如还有如下查询： 12select * from task where status = 0 and type = 12 limit 10;select count(*) from task where status = 0 ; 那么索引建立成(status,type,operator_id,operate_time)就是非常正确的，因为可以覆盖到所有情况。这个就是利用了索引的最左匹配的原则 查询优化神器 - explain命令关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网explain-output，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。 慢查询优化基本步骤0.先运行看看是否真的很慢，注意设置SQL_NO_CACHE 1.where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高 2.explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询） 3.order by limit 形式的sql语句让排序的表优先查 4.了解业务方使用场景 5.加索引时参照建索引的几大原则 6.观察结果，不符合预期继续从0分析 几个慢查询案例下面几个例子详细解释了如何分析和优化慢查询。 复杂语句写法很多情况下，我们写SQL只是为了实现功能，这只是第一步，不同的语句书写方式对于效率往往有本质的差别，这要求我们对mysql的执行计划和索引原则有非常清楚的认识，请看下面的语句： 12345678910111213141516171819202122232425262728select distinct cert.emp_id from cm_log cl inner join ( select emp.id as emp_id, emp_cert.id as cert_id from employee emp left join emp_certificate emp_cert on emp.id = emp_cert.emp_id where emp.is_deleted=0 ) cert on ( cl.ref_table='Employee' and cl.ref_oid= cert.emp_id ) or ( cl.ref_table='EmpCertificate' and cl.ref_oid= cert.cert_id ) where cl.last_upd_date &gt;='2013-11-07 15:03:00' and cl.last_upd_date&lt;='2013-11-08 16:00:00'; 0.先运行一下，53条记录 1.87秒，又没有用聚合语句，比较慢 153 rows in set (1.87 sec) 1.explain 12345678+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+| 1 | PRIMARY | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where; Using temporary || 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 63727 | Using where; Using join buffer || 2 | DERIVED | emp | ALL | NULL | NULL | NULL | NULL | 13317 | Using where || 2 | DERIVED | emp_cert | ref | emp_certificate_empid | emp_certificate_empid | 4 | meituanorg.emp.id | 1 | Using index |+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+ 简述一下执行计划，首先mysql根据idx_last_upd_date索引扫描cm_log表获得379条记录；然后查表扫描了63727条记录，分为两部分，derived表示构造表，也就是不存在的表，可以简单理解成是一个语句形成的结果集，后面的数字表示语句的ID。derived2表示的是ID = 2的查询构造了虚拟表，并且返回了63727条记录。我们再来看看ID = 2的语句究竟做了写什么返回了这么大量的数据，首先全表扫描employee表13317条记录，然后根据索引emp_certificate_empid关联emp_certificate表，rows = 1表示，每个关联都只锁定了一条记录，效率比较高。获得后，再和cm_log的379条记录根据规则关联。从执行过程上可以看出返回了太多的数据，返回的数据绝大部分cm_log都用不到，因为cm_log只锁定了379条记录。 如何优化呢？可以看到我们在运行完后还是要和cm_log做join,那么我们能不能之前和cm_log做join呢？仔细分析语句不难发现，其基本思想是如果cm_log的ref_table是EmpCertificate就关联emp_certificate表，如果ref_table是Employee就关联employee表，我们完全可以拆成两部分，并用union连接起来，注意这里用union，而不用union all是因为原语句有“distinct”来得到唯一的记录，而union恰好具备了这种功能。如果原语句中没有distinct不需要去重，我们就可以直接使用union all了，因为使用union需要去重的动作，会影响SQL性能。 优化过的语句如下： 12345678910111213141516171819202122232425262728select emp.id from cm_log cl inner join employee emp on cl.ref_table = 'Employee' and cl.ref_oid = emp.id where cl.last_upd_date &gt;='2013-11-07 15:03:00' and cl.last_upd_date&lt;='2013-11-08 16:00:00' and emp.is_deleted = 0 unionselect emp.id from cm_log cl inner join emp_certificate ec on cl.ref_table = 'EmpCertificate' and cl.ref_oid = ec.id inner join employee emp on emp.id = ec.emp_id where cl.last_upd_date &gt;='2013-11-07 15:03:00' and cl.last_upd_date&lt;='2013-11-08 16:00:00' and emp.is_deleted = 0 4.不需要了解业务场景，只需要改造的语句和改造之前的语句保持结果一致 5.现有索引可以满足，不需要建索引 6.用改造后的语句实验一下，只需要10ms 降低了近200倍！ 1234567891011+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+| 1 | PRIMARY | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where || 1 | PRIMARY | emp | eq_ref | PRIMARY | PRIMARY | 4 | meituanorg.cl.ref_oid | 1 | Using where || 2 | UNION | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where || 2 | UNION | ec | eq_ref | PRIMARY,emp_certificate_empid | PRIMARY | 4 | meituanorg.cl.ref_oid | 1 | || 2 | UNION | emp | eq_ref | PRIMARY | PRIMARY | 4 | meituanorg.ec.emp_id | 1 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | |+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+53 rows in set (0.01 sec) 明确应用场景举这个例子的目的在于颠覆我们对列的区分度的认知，一般上我们认为区分度越高的列，越容易锁定更少的记录，但在一些特殊的情况下，这种理论是有局限性的。 1234567891011select * from stage_poi sp where sp.accurate_result=1 and ( sp.sync_status=0 or sp.sync_status=2 or sp.sync_status=4 ); 0.先看看运行多长时间,951条数据6.22秒，真的很慢。 1951 rows in set (6.22 sec) 1.先explain，rows达到了361万，type = ALL表明是全表扫描。 12345+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | sp | ALL | NULL | NULL | NULL | NULL | 3613155 | Using where |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+ 2.所有字段都应用查询返回记录数，因为是单表查询 0已经做过了951条。 3.让explain的rows 尽量逼近951。 看一下accurate_result = 1的记录数： 12345678select count(*),accurate_result from stage_poi group by accurate_result;+----------+-----------------+| count(*) | accurate_result |+----------+-----------------+| 1023 | -1 || 2114655 | 0 || 972815 | 1 |+----------+-----------------+ 我们看到accurate_result这个字段的区分度非常低，整个表只有-1,0,1三个值，加上索引也无法锁定特别少量的数据。 再看一下sync_status字段的情况： 1234567select count(*),sync_status from stage_poi group by sync_status;+----------+-------------+| count(*) | sync_status |+----------+-------------+| 3080 | 0 || 3085413 | 3 |+----------+-------------+ 同样的区分度也很低，根据理论，也不适合建立索引。 问题分析到这，好像得出了这个表无法优化的结论，两个列的区分度都很低，即便加上索引也只能适应这种情况，很难做普遍性的优化，比如当sync_status 0、3分布的很平均，那么锁定记录也是百万级别的。 4.找业务方去沟通，看看使用场景。业务方是这么来使用这个SQL语句的，每隔五分钟会扫描符合条件的数据，处理完成后把sync_status这个字段变成1,五分钟符合条件的记录数并不会太多，1000个左右。了解了业务方的使用场景后，优化这个SQL就变得简单了，因为业务方保证了数据的不平衡，如果加上索引可以过滤掉绝大部分不需要的数据。 5.根据建立索引规则，使用如下语句建立索引 1alter table stage_poi add index idx_acc_status(accurate_result,sync_status); 6.观察预期结果,发现只需要200ms，快了30多倍。 1952 rows in set (0.20 sec) 我们再来回顾一下分析问题的过程，单表查询相对来说比较好优化，大部分时候只需要把where条件里面的字段依照规则加上索引就好，如果只是这种“无脑”优化的话，显然一些区分度非常低的列，不应该加索引的列也会被加上索引，这样会对插入、更新性能造成严重的影响，同时也有可能影响其它的查询语句。所以我们第4步调差SQL的使用场景非常关键，我们只有知道这个业务场景，才能更好地辅助我们更好的分析和优化查询语句。 无法优化的语句12345678910111213141516171819202122232425262728293031323334353637select c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c inner join contact_branch cb on c.id = cb.contact_id inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 10802 and oei.org_category = - 1 order by c.created_time desc limit 0 , 10; 还是几个步骤。 0.先看语句运行多长时间，10条记录用了13秒，已经不可忍受。 110 rows in set (13.06 sec) 1.explain 12345678+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+| 1 | SIMPLE | oei | ref | idx_category_left_right,idx_data_id | idx_category_left_right | 5 | const | 8849 | Using where; Using temporary; Using filesort || 1 | SIMPLE | bu | ref | PRIMARY,idx_userid_status | idx_userid_status | 4 | meituancrm.oei.data_id | 76 | Using where; Using index || 1 | SIMPLE | cb | ref | idx_branch_id,idx_contact_branch_id | idx_branch_id | 4 | meituancrm.bu.branch_id | 1 | || 1 | SIMPLE | c | eq_ref | PRIMARY | PRIMARY | 108 | meituancrm.cb.contact_id | 1 | |+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+ 从执行计划上看，mysql先查org_emp_info表扫描8849记录，再用索引idx_userid_status关联branch_user表，再用索引idx_branch_id关联contact_branch表，最后主键关联contact表。 rows返回的都非常少，看不到有什么异常情况。我们在看一下语句，发现后面有order by + limit组合，会不会是排序量太大搞的？于是我们简化SQL，去掉后面的order by 和 limit，看看到底用了多少记录来排序。 12345678910111213141516171819202122232425select count(*)from contact c inner join contact_branch cb on c.id = cb.contact_id inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 10802 and oei.org_category = - 1 +----------+| count(*) |+----------+| 778878 |+----------+1 row in set (5.19 sec) 发现排序之前居然锁定了778878条记录，如果针对70万的结果集排序，将是灾难性的，怪不得这么慢，那我们能不能换个思路，先根据contact的created_time排序，再来join会不会比较快呢？ 于是改造成下面的语句，也可以用straight_join来优化： 12345678910111213141516171819202122232425262728293031323334353637383940414243select c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c where exists ( select 1 from contact_branch cb inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 10802 and oei.org_category = - 1 where c.id = cb.contact_id ) order by c.created_time desc limit 0 , 10; 验证一下效果 预计在1ms内，提升了13000多倍！ 110 rows in set (0.00 sec) 本以为至此大工告成，但我们在前面的分析中漏了一个细节，先排序再join和先join再排序理论上开销是一样的，为何提升这么多是因为有一个limit！大致执行过程是：mysql先按索引排序得到前10条记录，然后再去join过滤，当发现不够10条的时候，再次去10条，再次join，这显然在内层join过滤的数据非常多的时候，将是灾难的，极端情况，内层一条数据都找不到，mysql还傻乎乎的每次取10条，几乎遍历了这个数据表！ 用不同参数的SQL试验下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344select sql_no_cache c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c where exists ( select 1 from contact_branch cb inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 2875 and oei.org_category = - 1 where c.id = cb.contact_id ) order by c.created_time desc limit 0 , 10;Empty set (2 min 18.99 sec) 2 min 18.99 sec！比之前的情况还糟糕很多。由于mysql的nested loop机制，遇到这种情况，基本是无法优化的。这条语句最终也只能交给应用系统去优化自己的逻辑了。 通过这个例子我们可以看到，并不是所有语句都能优化，而往往我们优化时，由于SQL用例回归时落掉一些极端情况，会造成比原来还严重的后果。所以，第一：不要指望所有语句都能通过SQL优化，第二：不要过于自信，只针对具体case来优化，而忽略了更复杂的情况。 慢查询的案例就分析到这儿，以上只是一些比较典型的案例。我们在优化过程中遇到过超过1000行，涉及到16个表join的“垃圾SQL”，也遇到过线上线下数据库差异导致应用直接被慢查询拖死，也遇到过varchar等值比较没有写单引号，还遇到过笛卡尔积查询直接把从库搞死。再多的案例其实也只是一些经验的积累，如果我们熟悉查询优化器、索引的内部原理，那么分析这些案例就变得特别简单了。 写在后面的话本文以一个慢查询案例引入了MySQL索引原理、优化慢查询的一些方法论;并针对遇到的典型案例做了详细的分析。其实做了这么长时间的语句优化后才发现，任何数据库层面的优化都抵不上应用系统的优化，同样是MySQL，可以用来支撑Google/FaceBook/Taobao应用，但可能连你的个人网站都撑不住。套用最近比较流行的话：“查询容易，优化不易，且写且珍惜！” 参考文献：1.《高性能MySQL》 2.《数据结构与算法分析》","link":"/mysql-index/"},{"title":"navicat通过ssh连接数据库","text":"有时候连接数据库需要用过跳转机连接，此时可以通过navicat 的ssh隧道代理 假设有两台服务器，ip分别是192.168.1.2和192.168.191.3,我们通过192.168.1.2服务器连接到192.168.191.3的mysql. 如图，选择ssh，其中： 主机：跳转机服务器ip 用户名：跳转机服务器用户名，不是数据库用户名 密码：跳转服务器密码 填写完成后，返回常规，填写如下 其中： 主机：mysql服务器ip 用户名：mysql用户名 密码：mysql密码","link":"/navicat-ssh/"},{"title":"Linux 系统目录结构","text":"简单介绍Linux 系统目录结构 登录系统后，在当前命令窗口下输入命令： 1ls / 你会看到如下图所示: 树状目录结构： 以下是对这些目录的解释： /bin：bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。 /boot：这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ：dev 是 Device(设备) 的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。 /etc：etc 是 Etcetera(等等) 的缩写,这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home：用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的，如上图中的 alice、bob 和 eve。 /lib：lib 是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media：linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux 会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在 /mnt/ 上，然后进入该目录就可以查看光驱里的内容了。 /opt：opt 是 optional(可选) 的缩写，这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc：proc 是 Processes(进程) 的缩写，/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器： 1echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root：该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin：s 就是 Super User 的意思，是 Superuser Binaries (超级用户的二进制文件) 的缩写，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是 Linux2.6 内核的一个很大的变化。该目录下安装了 2.6 内核中新出现的一个文件系统 sysfs 。 sysfs 文件系统集成了下面3种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的 devpts 文件系统。 该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp：tmp 是 temporary(临时) 的缩写这个目录是用来存放一些临时文件的。 /usr： usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。 /usr/bin：系统用户使用的应用程序。 /usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src：内核源代码默认的放置目录。 /var：var 是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。 /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在 /bin/ls 目录下的。 值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给 root 使用的指令。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。","link":"/linux-directory/"},{"title":"求你了，GC 日志打印别再瞎配置了","text":"生产环境上，或者其他要测试 GC 问题的环境上，一定会配置上打印GC日志的参数，便于分析 GC 相关的问题。 但是可能很多人配置的都不够“完美”，要么是打印的内容过少，要么是输出到控制台，要么是一个大文件被覆盖，要么是…… 本文带你一步一步，配置一个完美的 GC 日志打印策略 打印内容为了保留足够多的“现场证据”，最好是把 GC 相关的信息打印的足够完整。而且你的程序真的不差你GC时打印日志I/O消耗的那点性能 打印基本 GC 信息打印 GC 日志的第一步，就是开启 GC 打印的参数了，也是最基本的参数。 1-XX:+PrintGCDetails -XX:+PrintGCDateStamps 打印对象分布为了分析 GC 时的晋升情况和晋升导致的高暂停，不看对象年龄分布日志怎么行 1-XX:+PrintTenuringDistribution 输出内容示例： 12345678910111213141516Desired survivor size 59244544 bytes, new threshold 15 (max 15)- age 1: 963176 bytes, 963176 total- age 2: 791264 bytes, 1754440 total- age 3: 210960 bytes, 1965400 total- age 4: 167672 bytes, 2133072 total- age 5: 172496 bytes, 2305568 total- age 6: 107960 bytes, 2413528 total- age 7: 205440 bytes, 2618968 total- age 8: 185144 bytes, 2804112 total- age 9: 195240 bytes, 2999352 total- age 10: 169080 bytes, 3168432 total- age 11: 114664 bytes, 3283096 total- age 12: 168880 bytes, 3451976 total- age 13: 167272 bytes, 3619248 total- age 14: 387808 bytes, 4007056 total- age 15: 168992 bytes, 4176048 total GC 后打印堆数据每次发生 GC 时，对比一下 GC 前后的堆内存情况，更直观 1-XX:+PrintHeapAtGC 输出内容示例： 1234567891011{Heap before GC invocations=0 (full 0): garbage-first heap total 1024000K, used 324609K [0x0000000781800000, 0x0000000781901f40, 0x00000007c0000000) region size 1024K, 6 young (6144K), 0 survivors (0K) Metaspace used 3420K, capacity 4500K, committed 4864K, reserved 1056768K class space used 371K, capacity 388K, committed 512K, reserved 1048576KHeap after GC invocations=1 (full 1): garbage-first heap total 1024000K, used 21755K [0x0000000781800000, 0x0000000781901f40, 0x00000007c0000000) region size 1024K, 0 young (0K), 0 survivors (0K) Metaspace used 3420K, capacity 4500K, committed 4864K, reserved 1056768K class space used 371K, capacity 388K, committed 512K, reserved 1048576K} 打印 STW 时间暂停时间是 GC 最重要的指标，肯定不能少 1-XX:+PrintGCApplicationStoppedTime 输出内容示例： 1Total time for which application threads were stopped: 0.0254260 seconds, Stopping threads took: 0.0000218 seconds 打印 safepoint 信息进入STW阶段之前，需要要找到一个合适的 safepoint ，这个指标一样很重要（非必选，出现 GC 问题时最好加上此参数调试） 1-XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1 输出内容示例： 1234567 vmop [threads: total initially_running wait_to_block] [time: spin block sync cleanup vmop] page_trap_count0.371: ParallelGCFailedAllocation [ 10 0 0 ] [ 0 0 0 0 7 ] 0 Execute full gc...dataList has been promoted to cms old space vmop [threads: total initially_running wait_to_block] [time: spin block sync cleanup vmop] page_trap_count0.379: ParallelGCSystemGC [ 10 0 0 ] [ 0 0 0 0 16 ] 0 vmop [threads: total initially_running wait_to_block] [time: spin block sync cleanup vmop] page_trap_count0.396: no vm operation [ 9 1 1 ] [ 0 0 0 0 341 ] 0 打印 Reference 处理信息强引用/弱引用/软引用/虚引用/finalize 方法万一有问题，不得打印出来看看？ 1-XX:+PrintReferenceGC 输出内容示例： 123452021-02-19T12:41:30.462+0800: 5072726.605: [SoftReference, 0 refs, 0.0000521 secs]2021-02-19T12:41:30.462+0800: 5072726.605: [WeakReference, 0 refs, 0.0000069 secs]2021-02-19T12:41:30.462+0800: 5072726.605: [FinalReference, 0 refs, 0.0000056 secs]2021-02-19T12:41:30.462+0800: 5072726.605: [PhantomReference, 0 refs, 0 refs, 0.0000059 secs]2021-02-19T12:41:30.462+0800: 5072726.605: [JNI Weak Reference, 0.0000131 secs], 0.4635293 secs] 完整参数1234567891011# requireds-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+PrintReferenceGC -XX:+PrintGCApplicationStoppedTime# optional-XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1 输出方式上面只是定义了打印的内容，默认情况下，这些日志会输出到控制台（标准输出）。那如果你的程序日志也输出到控制台呢，这个日志内容就会很乱，分析起来很麻烦。如果你是追加的方式（比如 tomcat 的 catalina.out 就是追加），这个文件会越来越大，分析起来就要命了。 所以需要一种分割日志的机制，这个机制嘛……JVM自然是提供的。 JVM 的日志分割JVM提供了几个用于分割 GC 日志的参数： 12345678# GC日志输出的文件路径-Xloggc:/path/to/gc.log# 开启日志文件分割-XX:+UseGCLogFileRotation # 最多分割几个文件，超过之后从头开始写-XX:NumberOfGCLogFiles=14# 每个文件上限大小，超过就触发分割-XX:GCLogFileSize=100M 按照这个参数，每个GC日志只要超过20M就会进行分割，最多分割5个文件，文件名依次是gc.log.0,gc.log.1,gc.log.2,gc.log.3,gc.log.4, ..... 看似很美好，几行配置就搞定了输出文件的问题。但是这种方式有一些问题： -Xloggc 方式指定的日志文件，是覆盖写的方式，每次启动都会覆盖，历史日志会丢失 当超过最大分割数后，会从第0个文件开始重新写入，而且是覆盖 -XX:NumberOfGCLogFiles 并不能设置为无限 这个覆盖的问题就有点恶心了，每次启动覆盖之前的历史日志……这谁能忍？ 使用时间戳命名文件于是有另一种解决方案。不使用 JVM 提供的日志分割功能，而是每次启动用时间戳命名日志文件，这样可以每次启动都使用不同的文件，就不会出现覆盖的问题了。 1234# 使用-%t作为日志文件名-XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/path/to/gc-%t.log# 生成的文件名是这种：gc-2021-03-29_20-41-47.log 可是这样就完美吗？ 虽然没有覆盖的问题，但由于没有日志分割的功能，每次启动后只有一个GC日志文件，单个日志文件可能会非常巨大。过大的日志文件分析起来是很麻烦的，必须得分割。 二者结合这里只需要稍微调整一下策略，将 JVM 分割和时间戳命名两种方案结合，就可以得到最优的方式了。 12345678# GC日志输出的文件路径-Xloggc:/path/to/gc-%t.log# 开启日志文件分割-XX:+UseGCLogFileRotation # 最多分割几个文件，超过之后从头开始写-XX:NumberOfGCLogFiles=14# 每个文件上限大小，超过就触发分割-XX:GCLogFileSize=100M 配置时间戳作文 GC 日志文件名的同时，也配置JVM的GC日志分割策略。这样一来，既保证了 GC 文件不会被覆盖，又保证了单个 GC 文件的大小不会过大，完美！ 最终得到的日志文件名会像这个样子： gc-2021-03-29_20-41-47.log.0 gc-2021-03-29_20-41-47.log.1 gc-2021-03-29_20-41-47.log.2 gc-2021-03-29_20-41-47.log.3 …. 最佳实践 - 完整参数1234567891011121314151617181920# 必备-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+PrintReferenceGC -XX:+PrintGCApplicationStoppedTime# 可选-XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1# GC日志输出的文件路径-Xloggc:/path/to/gc-%t.log# 开启日志文件分割-XX:+UseGCLogFileRotation # 最多分割几个文件，超过之后从头文件开始写-XX:NumberOfGCLogFiles=14# 每个文件上限大小，超过就触发分割-XX:GCLogFileSize=100M","link":"/java-gc-log/"},{"title":"nginx代理rabbitmq web management页面","text":"nginx二级域名代理rabbitmq的web页面还是比较简单的，但是代理二级路径需要额外配置下 1vim /etc/rabbitmq/rabbitmq.config 添加配置,例如需要代理到/rabbitmq二级路径 1{rabbitmq_management,[{path_prefix,&quot;/rabbitmq&quot;}]} 重启rabbitmq 1systemctl restart rabbitmq-server 配置nginx 1234567server_name www.yhan219.com;...location /rabbitmq { proxy_pass http://localhost:15672;} 重新加载nginx 1nginx -s reload 访问http://www.yhan219.com/rabbitmq/#/ 参考文献Management Plugin — RabbitMQ","link":"/nginx-proxy-rabbitmq-web/"},{"title":"CentOS 7安装 nginx","text":"CentOS 7 安装 nginx 教程 安装 nginx 编译所需的 lib 库1yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel pcre pcre-devel 查看 pcre(正则库)版本 1pcre-config --version 进入编译目录1cd /usr/local/src 从官网下载最新的 nginx(stable version 稳定版)1wget http://nginx.org/download/nginx-1.19.10.tar.gz 解压 nginx 压缩包1tar -zxvf nginx-1.19.10.tar.gz 进入解压目录 1cd nginx-1.19.10 运行配置脚本--prefix参数指定nginx安装的目录,默认安装在/usr/local/nginx 默认安装 1./configure --prefix=/usr/local/nginx 或添加一些模块安装添加 https,stream 等模块 1./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-stream --with-http_v2_module 编译安装 nginx1make &amp;&amp; make install 将 nginx 执行命令软链接到/usr/bin 1ln -s /usr/local/nginx/sbin/nginx /usr/bin 启动 nginx1nginx 设置开机自启动12echo &quot;/usr/local/nginx/sbin/nginx&quot; &gt;&gt; /etc/rc.d/rc.localchmod +x /etc/rc.d/rc.local 执行 nginx -h 查看相关命令12345678910111213141516nginx -hnginx version: nginx/1.19.10Usage: nginx [-?hvVtTq] [-s signal] [-c filename] [-p prefix] [-g directives]Options: -?,-h : this help -v : show version and exit -V : show version and configure options then exit -t : test configuration and exit -T : test configuration, dump it and exit -q : suppress non-error messages during configuration testing -s signal : send signal to a master process: stop, quit, reopen, reload -p prefix : set prefix path (default: /usr/local/nginx/) -c filename : set configuration file (default: conf/nginx.conf) -g directives : set global directives out of configuration file 查看 nginx 安装目录1whereis nginx vim语法高亮1cp -r /usr/local/src/nginx-1.19.10/contrib/vim/* /usr/share/vim/vimfiles 自定义conf目录，例如/usr/local/nginx/conf.d 1234vim /usr/share/vim/vimfiles/ftdetect/nginx.vim# 添加以下内容au BufRead,BufNewFile */usr/local/nginx/conf.d/* set ft=nginx nginx常用配置 www跳转m 123if ($http_user_agent ~* '(MIDP)|(WAP)|(UP.Browser)|(Smartphone)|(Obigo)|(Mobile)|(AU.Browser)|(wxd.Mms)|(WxdB.Browser)|(CLDC)|(UP.Link)|(KM.Browser)|(UCWEB)|(SEMC-Browser)|(Mini)|(Symbian)|(Palm)|(Nokia)|(Panasonic)|(MOT-)|(SonyEricsson)|(NEC-)|(Alcatel)|(Ericsson)|(BENQ)|(BenQ)|(Amoisonic)|(Amoi-)|(Capitel)|(PHILIPS)|(SAMSUNG)|(Lenovo)|(Mitsu)|(Motorola)|(SHARP)|(WAPPER)|(LG-)|(LG/)|(EG900)|(CECT)|(Compal)|(kejian)|(Bird)|(BIRD)|(G900/V1.0)|(Arima)|(CTL)|(TDG)|(Daxian)|(DAXIAN)|(DBTEL)|(Eastcom)|(EASTCOM)|(PANTECH)|(Dopod)|(Haier)|(HAIER)|(KONKA)|(KEJIAN)|(LENOVO)|(Soutec)|(SOUTEC)|(SAGEM)|(SEC-)|(SED-)|(EMOL-)|(INNO55)|(ZTE)|(iPhone)|(Android)|(Windows CE)|(Opera)') { rewrite ^.+ https://m.test.com/$uri;}","link":"/nginx/"},{"title":"IDEA及付费插件激活(长期更新)","text":"随着Jetbrains产品更新，原无限重置方法已失效，现提供新方法，依旧可以激活所有付费插件。感谢zhile.io站长提供的插件！！！ 感谢jetbra站长提供的插件！！！ 本插件开源地址https://github.com/ja-netfilter/ja-netfilter 安装下载下载激活工具ja-netfilter。解压到合适目录。 修改激活信息修改config目录下mymap.conf文件中的EQUAL,licenseeName-&gt;yhan219,yhan219改为自己想激活的名字。可酌情修改激活时间EQUAL,paidUpTo-&gt;2022-12-31，不建议修改太长，以免有的插件激活失败。 短期激活打开IDEA，如果提示激活，先使用jetbra站长提供的激活码激活。 添加激活补丁修改idea.vmoptions文件，添加 1-javaagent:/Users/yhan219/Documents/ja-netfilter/ja-netfilter.jar 根据实际情况修改路径。 如果添加过其他激活补丁的，需要删除。 重启重启IDEA加载补丁。 付费插件激活安装付费插件后，到jetbra上搜索对应插件的临时激活码，点击激活即可。 或在help - register中用同样方法激活。 激活效果","link":"/idea-active/"},{"title":"一文理清nginx中的location配置","text":"location 指令是 nginx 中最关键的指令之一，location 指令的功能是用来匹配不同的 URI 请求，进而对请求做不同的处理和响应，这其中较难理解的是多个 location 的匹配顺序，本文会作为重点来解释和说明。 开始之前先明确一些约定，我们输入的网址叫做请求 URI，nginx 用请求 URI 与 location 中配置的 URI 做匹配。 nginx文件结构首先我们先简单了解 nginx 的文件结构，nginx 的 HTTP 配置主要包括三个区块，结构如下： 1234567Global: nginx 运行相关Events: 与用户的网络连接相关http http Global: 代理，缓存，日志，以及第三方模块的配置 server server Global: 虚拟主机相关 location: 地址定向，数据缓存，应答控制，以及第三方模块的配置 从上面展示的 nginx 结构中可以看出 location 属于请求级别配置，这也是我们最常用的配置。 配置 location 块location 语法Location 块通过指定模式来与客户端请求的URI相匹配。Location基本语法： 匹配 URI 类型，有四种参数可选，当然也可以不带参数。 命名location，用@来标识，类似于定义goto语句块。 12location [ = | ~ | ~* | ^~ ] /URI { … }location @/name/ { … } location匹配命令解释 参数 解释 空 location 后没有参数直接跟着 标准 URI，表示前缀匹配，代表跟请求中的 URI 从头开始匹配。 = 用于标准 URI 前，要求请求字符串与其精准匹配，成功则立即处理，nginx停止搜索其他匹配。 ^~ 用于标准 URI 前，并要求一旦匹配到就会立即处理，不再去匹配其他的那些个正则 URI，一般用来匹配目录 ~ 用于正则 URI 前，表示 URI 包含正则表达式， 区分大小写 ~\\* 用于正则 URI 前， 表示 URI 包含正则表达式， 不区分大小写 @ @ 定义一个命名的 location，@ 定义的locaiton名字一般用在内部定向，例如error_page, try_files命令中。它的功能类似于编程中的goto。 location匹配顺序nginx有两层指令来匹配请求 URI 。第一个层次是 server 指令，它通过域名、ip 和端口来做第一层级匹配，当找到匹配的 server 后就进入此 server 的 location 匹配。 location 的匹配并不完全按照其在配置文件中出现的顺序来匹配，请求URI 会按如下规则进行匹配： 先精准匹配 = ，精准匹配成功则会立即停止其他类型匹配； 没有精准匹配成功时，进行前缀匹配。先查找带有 ^~ 的前缀匹配，带有 ^~ 的前缀匹配成功则立即停止其他类型匹配，普通前缀匹配（不带参数 ^~ ）成功则会暂存，继续查找正则匹配； = 和 ^~ 均未匹配成功前提下，查找正则匹配 ~ 和 ~\\* 。当同时有多个正则匹配时，按其在配置文件中出现的先后顺序优先匹配，命中则立即停止其他类型匹配； 所有正则匹配均未成功时，返回步骤 2 中暂存的普通前缀匹配（不带参数 ^~ ）结果 以上规则简单总结就是优先级从高到低依次为（序号越小优先级越高）： 1234561. location = # 精准匹配2. location ^~ # 带参前缀匹配3. location ~ # 正则匹配（区分大小写）4. location ~* # 正则匹配（不区分大小写）5. location /a # 普通前缀匹配，优先级低于带参数前缀匹配。6. location / # 任何没有匹配成功的，都会匹配这里处理 上述匹配规则可以用以下伪代码表示，加深理解： 123456789101112131415function match(uri): rv = NULL if uri in exact_match: return exact_match[uri] if uri in prefix_match: if prefix_match[uri] is '^~': return prefix_match[uri] else: rv = prefix_match[uri] // 注意这里没有 return，且这里是最长匹配 if uri in regex_match: return regex_match[uri] // 按文件中顺序，找到即返回 return rv 案例分析接下来，让我们通过一些实际案例来验证上述规则。 案例 11234567891011server { server_name website.com; location /doc { return 701; # 用这样的方式，可以方便的知道请求到了哪里 } location ~* ^/document$ { return 702; }}curl -I website.com:8080/document` 返回 `返回 HTTP/1.1 702 说明：按照上述的规则，显然第二个正则匹配会有更高的优先级 案例 212345678910server { server_name website.com; location /document { return 701; } location ~* ^/document$ { return 702; }}curl -I website.com:8080/document` 返回 `HTTP/1.1 702 说明：第二个匹配了正则表达式，优先级高于第一个普通前缀匹配 案例 312345678910server { server_name website.com; location ^~ /doc { return 701; } location ~* ^/document$ { return 702; }}curl http://website.com/document` 返回 `HTTP/1.1 701 说明：第一个前缀匹配 ^~ 命中以后不会再搜寻正则匹配，所以会第一个命中。 案例 41234567891011121314151617181920server { server_name website.com; location /docu { return 701; } location /doc { return 702; }}curl -I website.com:8080/document` 会返回 `HTTP/1.1 701server { server_name website.com; location /doc { return 702; } location /docu { return 701; }}curl -I website.com:8080/document` 依然返回 `HTTP/1.1 701 说明：前缀匹配下，返回最长匹配的 location，与 location 所在位置顺序无关 案例 512345678910111213server { listen 8080; server_name website.com; location ~ ^/doc[a-z]+ { return 701; } location ~ ^/docu[a-z]+ { return 702; }}curl -I website.com:8080/document` 返回 `HTTP/1.1 701 把顺序换一下 12345678910111213server { listen 8080; server_name website.com; location ~ ^/docu[a-z]+ { return 702; } location ~ ^/doc[a-z]+ { return 701; }}curl -I website.com:8080/document` 返回 `HTTP/1.1 702 说明：可见正则匹配是使用文件中的顺序，先匹配成功的返回。 案例 6我们对一个官方文档中提到例子做一些补充，来看一个相对较完整的例子，假设我们有如下几个请求等待匹配： 123456//index.html/documents/document.html/documents/abc/images/a.gif/documents/a.jpg 以下是 location 配置及其匹配情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556location = / { # 只精准匹配 / 的查询. [ configuration A ] }# 匹配成功： / location / { # 匹配任何请求，因为所有请求都是以”/“开始 # 但是更长字符匹配或者正则表达式匹配会优先匹配 [ configuration B ] }#匹配成功：/index.htmllocation /documents { # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索/ # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条/ [ configuration C ] }# 匹配成功：/documents/document.html# 匹配成功：/documents/abclocation ~ /documents/ABC { # 区分大小写的正则匹配 # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索/ # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条/ [ configuration CC ] }location ^~ /images/ { # 匹配任何以 /images/ 开头的地址，匹配符合以后，立即停止往下搜索正则，采用这一条。/ [ configuration D ] }# 成功匹配：/images/a.giflocation ~* \\.(gif|jpg|jpeg)$ { # 匹配所有以 .gif、.jpg 或 .jpeg 结尾的请求，不区分大小写 # 然而，所有请求 /images/ 下的图片会被 [ config D ] 处理，因为 ^~ 到达不了这一条正则/ [ configuration E ] }# 成功匹配：/documents/a.jpglocation /images/ { # 字符匹配到 /images/，继续往下，会发现 ^~ 存在/ [ configuration F ] }location /images/abc { # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在/ # F与G的放置顺序是没有关系的/ [ configuration G ] }location ~ /images/abc/ { # 只有去掉 [ config D ] 才有效：先最长匹配 [ config G ] 开头的地址，继续往下搜索，匹配到这一条正则，采用/ [ configuration H ] } 其他location配置相关匹配问号后的参数请求 URI 中问号后面的参数是不能在 location 中匹配到的，这些参数存储在 $query_string 变量中，可以用 if 来判断。例如，对于参数中带有单引号 ’ 进行匹配然后重定向到错误页面。 1234/plus/list.php?tid=19&amp;mid=1124‘if ( $query_string ~* “.*[;’&lt;&gt;].*” ){ return 404;} location URI结尾带不带 /关于 URI 尾部的 / 有三点也需要说明一下。第一点与 location 配置有关，其他两点无关。 location 中的字符有没有 / 都没有影响。也就是说 /user/ 和 /user 是一样的。 如果 URI 结构是 https://domain.com/ 的形式，尾部有没有 / 都不会造成重定向。因为浏览器在发起请求的时候，默认加上了 / 。虽然很多浏览器在地址栏里也不会显示 / 。这一点，可以访问baidu验证一下。 如果 URI 的结构是 https://domain.com/some-dir/ 。尾部如果缺少 / 将导致重定向。因为根据约定，URL 尾部的 / 表示目录，没有 / 表示文件。所以访问 /some-dir/ 时，服务器会自动去该目录下找对应的默认文件。如果访问 /some-dir 的话，服务器会先去找 some-dir 文件，找不到的话会将 some-dir 当成目录，重定向到 /some-dir/ ，去该目录下找默认文件。可以去测试一下你的网站是不是这样的。 命名 location带有 @ 的 location 是用来定义一个命名的 location，这种 location 不参与请求匹配，一般用在内部定向。用法如下： 123456location / { try_files $uri $uri/ @custom}location @custom { # ...do something} 上例中，当尝试访问 URI 找不到对应的文件就重定向到我们自定义的命名 location（此处为 custom）。 值得注意的是，命名 location 中不能再嵌套其它的命名 location。 location 实际使用建议所以实际使用中，个人觉得至少有三个匹配规则定义，如下： 直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。 这里是直接转发给后端应用服务器了，也可以是一个静态首页。第一个必选规则： 123location = / { proxy_pass http://tomcat:8080/index} 第二个必选规则是处理静态文件请求，这是 nginx 作为 http 服务器的强项，有两种配置模式，目录匹配或后缀匹配，任选其一或搭配使用： 123456location ^~ /static/ { root /webroot/static/;}location ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ { root /webroot/res/;} 第三个规则就是通用规则，用来转发动态请求到后端应用服务器，非静态文件请求就默认是动态请求，自己根据实际把握，毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了： 123location / { proxy_pass http://tomcat:8080/} nginx 通过 ngx_http_rewrite_module 模块支持 URI 重写、支持 if 条件判断，但不支持 else。 rewrite 只能放在 server { } 、 location { } 、 if { } 中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如http://aaa.com/a/we/index.php?id=1&amp;u=str只对/a/we/index.php重写。语法为 rewrite regex replacement [flag]; 指令执行顺序表面看 rewrite 和 location 功能有点像，都能实现跳转，主要区别在于 rewrite 是在同一域名内更改获取资源的路径，而 location 是对一类路径做控制访问或反向代理，可以 proxy_pass 到其他机器。很多情况下 rewrite 也会写在 location 里，它们的执行顺序是： 执行 server 块的 rewrite 指令（这里的块指的是 server 关键字后{}包围的区域，其它 xx 块类似） 执行location匹配 执行选定的location中的rewrite指令 如果其中某步 URI 被重写，则重新循环执行1-3，直到找到真实存在的文件；如果循环超过 10 次，则返回 500 Internal Server Error 错误。 指令详解if 指令语法：if(condition) {…}作用域： server、location功能：对给定的条件 condition 进行判断。如果为真，大括号内的 rewrite 指令将被执行。if 条件 (conditon) 可以是如下任何内容:if 中的几种判断条件 一个变量名；false 如果这个变量是空字符串或者以0开始的字符串； 使用 = ，!= 比较的一个变量和字符串 是用 ~， ~* 与正则表达式匹配的变量，如果这个正则表达式中包含}，;则整个表达式需要用” 或’ 包围 使用 -f ，!-f 检查一个文件是否存在 使用 -d， !-d 检查一个目录是否存在 使用 -e ，!-e 检查一个文件、目录、符号链接是否存在 使用 -x ， !-x 检查一个文件是否可执行 示例： 1234567891011121314151617181920212223set $variable &quot;0&quot;; if ($variable) { # 不会执行，因为 &quot;0&quot; 为 false break; }# 使用变量与正则表达式匹配 没有问题if ( $http_host ~ &quot;^star\\.igrow\\.cn$&quot; ) { break; }# 字符串与正则表达式匹配 报错if ( &quot;star&quot; ~ &quot;^star\\.igrow\\.cn$&quot; ) { break; }# 检查文件类的 字符串与变量均可if ( !-f &quot;/data.log&quot; ) { break; }if ( !-f $filename ) { break; } return 指令语法：return code [text];return code URL;return URL;作用域：server，location，if功能：停止处理并将指定的 code 码返回给客户端。 非标准 code 码 444 关闭连接而不发送响应报头。 该指令用于检查一个条件是否符合，如果条件符合，则执行大括号内的语句。If 指令不支持嵌套，不支持多个条件 &amp;&amp; 和 || 处理。 从0.8.42版本开始， return 语句可以指定重定向 URI (状态码可以为如下几种 301，302，303，307)， 也可以为其他状态码指定响应的文本内容，并且重定向的 URI 和响应的文本可以包含变量。 有一种特殊情况，就是重定向的url可以指定为此服务器本地的 URI，这样的话，nginx 会依据请求的协议 $scheme， server_name_in_redirect 和 port_in_redirect 自动生成完整的 URI。 示例：如果访问的 URI 以 .sh 或 .bash 结尾，则返回 403 状态码 1234location ~ .*\\.(sh|bash)?${ return 403;} rewrite 指令语法：rewrite regex replacement [flag];作用域：server 、location、if功能：如果一个URI匹配指定的正则表达式regex，URI就按照 replacement 重写。 rewrite 按配置文件中出现的顺序执行。可以使用 flag 标志来终止指令的进一步处理。 如果 replacement 以 http:// 、 https:// 或 $ scheme 开始，将不再继续处理，这个重定向将返回给客户端。 示例：第一种情况，重写的字符串带 http:// 123456location ^~ /redirect { # 当匹配前缀表达式 /redirect/(.*)时 请求将被临时重定向到 http://www.$1.com # 相当于 flag 写为 redirect rewrite ^/(.*)$ http://www.$1.com; return 200 &quot;ok&quot;;} 在浏览器中输入 127.0.0.1:8080/redirect/baidu ，则临时重定向到 www.baidu.com 后面的 return 指令将没有机会执行了。 1234567location ^~ /redirect { rewrite ^/(.*)$ www.$1.com; return 200 &quot;ok&quot;;}# 发送请求如下# curl 127.0.0.1:8080/redirect/baidu# ok 此处没有带 http:// 所以只是简单的重写。请求的 URI 由 /test1/baidu 重写为 www.baidu.com 因为会顺序执行 rewrite 指令，所以 下一步执行 return 指令，响应后返回 ok flag 有四种参数可以选择： last 停止处理后续 rewrite 指令集，然后对当前重写的新 URI 在 rewrite 指令集上重新查找。 break 停止处理后续 rewrite 指令集，并不再重新查找，但是当前location 内剩余非 rewrite 语句和 location 外的 非rewrite 语句可以执行。 redirect 如果 replacement 不是以 http:// 或 https:// 开始，返回 302 临时重定向 permanent 返回 301 永久重定向 示例 1： 123456789101112131415161718# rewrite 后面没有任何 flag 时就顺序执行 # 当 location 中没有 rewrite 模块指令可被执行时 就重写发起新一轮location 匹配location / { # 顺序执行如下两条rewrite指令 rewrite ^/test1 /test2; rewrite ^/test2 /test3; # 此处发起新一轮 location 匹配 URI为/test3}location = /test2 { return 200 “/test2”;} location = /test3 { return 200 “/test3”;}# 发送如下请求# curl 127.0.0.1:8080/test1# /test3 如果正则表达regex式中包含 “}” 或 “;”，那么整个表达式需要用双引号或单引号包围。 示例 2： 1234567891011121314151617location / { rewrite ^/test1 /test2; rewrite ^/test2 /test3 last; # 此处发起新一轮location匹配 uri为/test3 rewrite ^/test3 /test4; proxy_pass http://www.baidu.com;}location = /test2 { return 200 &quot;/test2&quot;;} location = /test3 { return 200 &quot;/test3&quot;;}location = /test4 { return 200 &quot;/test4&quot;;} 发送如下请求 curl 127.0.0.1:8080/test1 返回 /test3 当如果将上面的 location / 改成如下代码 123456789location / { rewrite ^/test1 /test2; # 此处不会发起新一轮location匹配；当是会终止执行后续rewrite模块指令重写后的 URI 为 /more/index.html rewrite ^/test2 /more/index.html break; rewrite /more/index\\.html /test4; # 这条指令会被忽略 # 因为 proxy_pass 不是rewrite模块的指令 所以它不会被 break终止 proxy_pass https://www.baidu.com;} 发送请求 127.0.0.1:8080/test1代理到 https://www.baidu.com rewrite 后的请求参数：如果替换字符串 replacement 包含新的请求参数，则在它们之后附加先前的请求参数。如果你不想要之前的参数，则在替换字符串 replacement 的末尾放置一个问号，避免附加它们。 12# 由于最后加了个 ?，原来的请求参数将不会被追加到 rewrite 之后的 URI 后面*rewrite ^/users/(.*)$ /show?user=$1? last; rewrite_log 指令语法：rewrite_log on | off;默认值：rewrite_log off;作用域：http 、 server 、 location 、 if功能：开启或关闭以 notice 级别打印 rewrite 处理日志到 error log 文件。 set 指令语法：set variable value;默认值：none作用域：server 、 location 、 if定义一个变量并赋值，值可以是文本，变量或者文本变量混合体。 uninitialized_variable_warn 指令作用域：http、 server 、 location 、 if语法：uninitialized_variable_warn on | off;默认值：uninitialized_variable_warn on功能：控制是否记录 有关未初始化变量的警告。","link":"/nginx-location/"},{"title":"Linux nc命令","text":"Linux nc命令用于设置路由器。执行本指令可设置路由器的相关参数。 语法1nc [-hlnruz][-g&lt;网关...&gt;][-G&lt;指向器数目&gt;][-i&lt;延迟秒数&gt;][-o&lt;输出文件&gt;][-p&lt;通信端口&gt;][-s&lt;来源位址&gt;][-v...][-w&lt;超时秒数&gt;][主机名称][通信端口...] 参数想要连接到某处: nc [-options] hostname port[s] [ports] …绑定端口等待连接: nc -l port [-options] [hostname] [port] -g&lt;网关&gt;：设置路由器跃程通信网关，最多设置8个; -G&lt;指向器数目&gt;：设置来源路由指向器，其数值为4的倍数; -h：在线帮助; -i&lt;延迟秒数&gt;：设置时间间隔，以便传送信息及扫描通信端口; -l：使用监听模式，监控传入的资料; -n：直接使用ip地址，而不通过域名服务器; -o&lt;输出文件&gt;：指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存; -p&lt;通信端口&gt;：设置本地主机使用的通信端口; -r：指定源端口和目的端口都进行随机的选择; -s&lt;来源位址&gt;：设置本地主机送出数据包的IP地址; -u：使用UDP传输协议; -v：显示指令执行过程; -w&lt;超时秒数&gt;：设置等待连线的时间; -z：使用0输入/输出模式，只在扫描通信端口时使用。 用法[A Server(192.168.1.1) B Client(192.168.1.2)] 0.连接到远程主机:1$nc -nvv 192.168.x.x 80 连到192.168.x.x的TCP80端口. 监听本地主机: 1$nc -l 80 监听本机的TCP80端口. 超时控制:多数情况我们不希望连接一直保持，那么我们可以使用 -w 参数来指定连接的空闲超时时间，该参数紧接一个数值，代表秒数，如果连接超过指定时间则连接会被终止。 Server1$nc -l 2389 Client1$ nc -w 10 localhost 2389 该连接将在 10 秒后中断。注意: 不要在服务器端同时使用 -w 和 -l 参数，因为 -w 参数将在服务器端无效果。 1.端口扫描端口扫描经常被系统管理员和黑客用来发现在一些机器上开放的端口，帮助他们识别系统中的漏洞。 1$nc -z -v -n 192.168.1.1 21-25 可以运行在TCP或者UDP模式，默认是TCP，-u参数调整为udp.z 参数告诉netcat使用0 IO,连接成功后立即关闭连接， 不进行数据交换.v 参数指详细输出.n 参数告诉netcat 不要使用DNS反向查询IP地址的域名.以上命令会打印21到25 所有开放的端口。 12$nc -v 127.0.0.1 22localhost [127.0.0.1] 22 (ssh) openSSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1.4 “SSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1.4”为Banner信息。Banner是一个文本，Banner是一个你连接的服务发送给你的文本信息。当你试图鉴别漏洞或者服务的类型和版本的时候，Banner信息是非常有用的。但是，并不是所有的服务都会发送banner.一旦你发现开放的端口，你可以容易的使用netcat 连接服务抓取他们的banner。 2.Chat Server假如你想和你的朋友聊聊，有很多的软件和信息服务可以供你使用。但是，如果你没有这么奢侈的配置，比如你在计算机实验室，所有的对外的连接都是被限制的，你怎样和整天坐在隔壁房间的朋友沟通那？不要郁闷了，netcat提供了这样一种方法，你只需要创建一个Chat服务器，一个预先确定好的端口，这样子他就可以联系到你了。 Server1$nc -l 20000 netcat 命令在20000端口启动了一个tcp 服务器，所有的标准输出和输入会输出到该端口。输出和输入都在此shell中展示。 Client1$nc 192.168.1.1 20000 不管你在机器B上键入什么都会出现在机器A上。 3.文件传输大部分时间中，我们都在试图通过网络或者其他工具传输文件。有很多种方法，比如FTP,SCP,SMB等等，但是当你只是需要临时或者一次传输文件，真的值得浪费时间来安装配置一个软件到你的机器上嘛。假设，你想要传一个文件file.txt 从A 到B。A或者B都可以作为服务器或者客户端. Server1$nc -l 20000 &lt; file.txt Client1$nc -n 192.168.1.1 20000 &gt; file.txt 这里我们创建了一个服务器在A上并且重定向netcat的输入为文件file.txt，那么当任何成功连接到该端口，netcat会发送file的文件内容。在客户端我们重定向输出到file.txt，当B连接到A，A发送文件内容，B保存文件内容到file.txt.没有必要创建文件源作为Server，我们也可以相反的方法使用。像下面的我们发送文件从B到A，但是服务器创建在A上，这次我们仅需要重定向netcat的输出并且重定向B的输入文件。B作为Server Server1$nc -l 20000 &gt; file.txt Client1$nc 192.168.1.2 20000 &lt; file.txt 4.目录传输发送一个文件很简单，但是如果我们想要发送多个文件，或者整个目录，一样很简单，只需要使用压缩工具tar，压缩后发送压缩包。如果你想要通过网络传输一个目录从A到B。 Server1$tar -cvf – dir_name | nc -l 20000 Client1$nc -n 192.168.1.1 20000 | tar -xvf - 这里在A服务器上，我们创建一个tar归档包并且通过-在控制台重定向它，然后使用管道，重定向给netcat，netcat可以通过网络发送它。在客户端我们下载该压缩包通过netcat 管道然后打开文件。如果想要节省带宽传输压缩包，我们可以使用bzip2或者其他工具压缩。 Server1$tar -cvf – dir_name| bzip2 -z | nc -l 20000 通过bzip2压缩 Client1$nc -n 192.168.1.1 20000 | bzip2 -d |tar -xvf - 5. 加密你通过网络发送的数据如果你担心你在网络上发送数据的安全，你可以在发送你的数据之前用如mcrypt的工具加密。 Server1$nc localhost 20000 | mcrypt –flush –bare -F -q -d -m ecb &gt; file.txt 使用mcrypt工具加密数据。 Client1$mcrypt –flush –bare -F -q -m ecb &lt; file.txt | nc -l 20000 使用mcrypt工具解密数据。以上两个命令会提示需要密码，确保两端使用相同的密码。这里我们是使用mcrypt用来加密，使用其它任意加密工具都可以。 6.流视频虽然不是生成流视频的最好方法，但如果服务器上没有特定的工具，使用netcat，我们仍然有希望做成这件事。 Server1$cat video.avi | nc -l 20000 这里我们只是从一个视频文件中读入并重定向输出到netcat客户端 Client1$nc 192.168.1.1 20000 | mplayer -vo x11 -cache 3000 - 这里我们从socket中读入数据并重定向到mplayer。 7.克隆一个设备如果你已经安装配置一台Linux机器并且需要重复同样的操作对其他的机器，而你不想在重复配置一遍。不在需要重复配置安装的过程，只启动另一台机器的一些引导可以随身碟和克隆你的机器。克隆Linux PC很简单，假如你的系统在磁盘/dev/sda上 Server1$dd if=/dev/sda | nc -l 20000 Client1$nc -n 192.168.1.1 20000 | dd of=/dev/sda dd是一个从磁盘读取原始数据的工具，我通过netcat服务器重定向它的输出流到其他机器并且写入到磁盘中，它会随着分区表拷贝所有的信息。但是如果我们已经做过分区并且只需要克隆root分区，我们可以根据我们系统root分区的位置，更改sda 为sda1，sda2.等等。 8.打开一个shell我们已经用过远程shell-使用telnet和ssh，但是如果这两个命令没有安装并且我们没有权限安装他们，我们也可以使用netcat创建远程shell。假设你的netcat支持 -c -e 参数(原生 netcat) Server1$nc -l 20000 -e /bin/bash -i Client1$nc 192.168.1.1 20000 这里我们已经创建了一个netcat服务器并且表示当它连接成功时执行/bin/bash假如netcat 不支持-c 或者 -e 参数（openbsd netcat）,我们仍然能够创建远程shell Server1$mkfifo /tmp/tmp_fifo$cat /tmp/tmp_fifo | /bin/sh -i 2&gt;&amp;1 | nc -l 20000 &gt; /tmp/tmp_fifo 这里我们创建了一个fifo文件，然后使用管道命令把这个fifo文件内容定向到shell 2&gt;&amp;1中。是用来重定向标准错误输出和标准输出，然后管道到netcat 运行的端口20000上。至此，我们已经把netcat的输出重定向到fifo文件中。说明：从网络收到的输入写到fifo文件中cat 命令读取fifo文件并且其内容发送给sh命令sh命令进程受到输入并把它写回到netcat。netcat 通过网络发送输出到client至于为什么会成功是因为管道使命令平行执行，fifo文件用来替代正常文件，因为fifo使读取等待而如果是一个普通文件，cat命令会尽快结束并开始读取空文件。在客户端仅仅简单连接到服务器 Client1$nc -n 192.168.1.1 20000 你会得到一个shell提示符在客户端 9.反向shell反向shell是指在客户端打开的shell。反向shell这样命名是因为不同于其他配置，这里服务器使用的是由客户提供的服务。 Server1$nc -l 20000 在客户端，简单地告诉netcat在连接完成后，执行shell。 Client1$nc 192.168.1.1 20000 -e /bin/bash 现在，什么是反向shell的特别之处呢反向shell经常被用来绕过防火墙的限制，如阻止入站连接。例如，我有一个专用IP地址为192.168.1.1，我使用代理服务器连接到外部网络。如果我想从网络外部访问 这台机器如1.2.3.4的shell，那么我会用反向外壳用于这一目的。 10.指定源端口假设你的防火墙过滤除25端口外其它所有端口，你需要使用-p选项指定源端口。 Server1$nc -l 20000 Client1$nc 192.168.1.1 20000 25 使用1024以内的端口需要root权限。该命令将在客户端开启25端口用于通讯，否则将使用随机端口。 11.指定源地址假设你的机器有多个地址，希望明确指定使用哪个地址用于外部数据通讯。我们可以在netcat中使用-s选项指定ip地址。 Server1$nc -u -l 20000 &lt; file.txt Client1$nc -u 192.168.1.1 20000 -s 172.31.100.5 &gt; file.txt 该命令将绑定地址172.31.100.5。 12.静态web页面服务器新建一个网页,命名为somepage.html;新建一个shell script: 1while true; do nc -l 80 -q 1 &lt; somepage.html;done 用root权限执行，然后在浏览器中输入127.0.0.1打开看看是否正确运行。nc 指令通常都是給管理者進行除錯或測試等動作用的，所以如果只是單純需要臨時的網頁伺服器，使用 Python 的 SimpleHTTPServer 模組會比較方便。 13.模拟HTTP Headers123456789$nc www.yhan219.com 80 GET /get HTTP/1.0HTTP/1.1 200 OKDate: Wed, 22 Apr 2020 11:03:46 GMTContent-Type: application/jsonContent-Length: 316Connection: closeAccess-Control-Allow-Origin: *Access-Control-Allow-Credentials: true 注意：内容必须以两个空行结尾。这符合rfc2616的标准，否则不会被正确解码的 在nc命令后，输入内容，然后按两次回车，即可从对方获得HTTP Headers内容。 13.Netcat支持IPv6netcat 的 -4 和 -6 参数用来指定 IP 地址类型，分别是 IPv4 和 IPv6： Server1$ nc -4 -l 2389 Client1$ nc -4 localhost 2389 然后我们可以使用 netstat 命令来查看网络的情况： 1$ netstat | grep 2389tcp 0 0 localhost:2389 localhost:50851 ESTABLISHEDtcp 0 0 localhost:50851 localhost:2389 ESTABLISHED 接下来我们看看IPv6 的情况： Server1$ nc -6 -l 2389 Client1$ nc -6 localhost 2389 再次运行 netstat 命令： 12$ netstat | grep 2389tcp6 0 0 localhost:2389 localhost:33234 ESTABLISHEDtcp6 0 0 localhost:33234 localhost:2389 ESTABLISHED 前缀是 tcp6 表示使用的是 IPv6 的地址。 14.在 Netcat 中禁止从标准输入中读取数据该功能使用 -d 参数，请看下面例子： Server1$ nc -l 2389 Client1$ nc -d localhost 2389Hi 你输入的 Hi 文本并不会送到服务器端。 15.强制 Netcat 服务器端保持启动状态如果连接到服务器的客户端断开连接，那么服务器端也会跟着退出。 Server1$ nc -l 2389 Client1$ nc localhost 2389^C Server1$ nc -l 2389$ 上述例子中，但客户端断开时服务器端也立即退出。我们可以通过 -k 参数来控制让服务器不会因为客户端的断开连接而退出。 Server1$ nc -k -l 2389 Client1$ nc localhost 2389^C Server1$ nc -k -l 2389 16.配置 Netcat 客户端不会因为 EOF 而退出Netcat 客户端可以通过 -q 参数来控制接收到 EOF 后隔多长时间才退出，该参数的单位是秒： Client1$nc -q 5 localhost 2389 现在如果客户端接收到 EOF ，它将等待 5 秒后退出。 17.手动使用 SMTP 协定寄信在测试邮件服务器是否正常时，可以使用这样的方式手动寄送 Email： 123456789$nc localhost 25 &lt;&lt; EOFHELO host.example.comMAIL FROM: &lt;user@host.example.com&gt;RCPT TO: &lt;user2@host.example.com&gt;DATABody of email..QUITEOF 18.投过代理服务器（Proxy）连线这指令会使用 10.2.3.4:8080 这个代理服务器，连线至 host.example.com 的42端口。 1$nc -x10.2.3.4:8080 -Xconnect host.example.com 42 19.使用 Unix Domain Socket这行指令会建立一个 Unix Domain Socket，并接收资料： 1$nc -lU /var/tmp/dsocket 转载说明 本篇文章在搜索引擎上搜索到多个结果，且作者不一致，所以在此不不标明出处。侵删。","link":"/nc/"},{"title":"死磕java同步系列之redis分布式锁进化史","text":"Redis（全称：Remote Dictionary Server 远程字典服务）是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 本章我们将介绍如何基于redis实现分布式锁，并把其实现的进化史从头到尾讲明白，以便大家在面试的时候能讲清楚redis分布式锁的来（忽）龙（悠）去（考）脉（官）。 问题（1）redis如何实现分布式锁？ （2）redis分布式锁有哪些优点？ （3）redis分布式锁有哪些缺点？ （4）redis实现分布式锁有没有现成的轮子可以使用？ 实现锁的条件基于前面关于锁（分布式锁）的学习，我们知道实现锁的条件有三个： （1）状态（共享）变量，它是有状态的，这个状态的值标识了是否已经被加锁，在ReentrantLock中是通过控制state的值实现的，在ZookeeperLock中是通过控制子节点来实现的； （2）队列，它是用来存放排队的线程，在ReentrantLock中是通过AQS的队列实现的，在ZookeeperLock中是通过子节点的有序性实现的； （3）唤醒，上一个线程释放锁之后唤醒下一个等待的线程，在ReentrantLock中结合AQS的队列释放时自动唤醒下一个线程，在ZookeeperLock中是通过其监听机制来实现的； 那么上面三个条件是不是必要的呢？ 其实不然，实现锁的必要条件只有第一个，对共享变量的控制，如果共享变量的值为null就给他设置个值（java中可以使用CAS操作进程内共享变量），如果共享变量有值则不断重复检查其是否有值（重试），待锁内逻辑执行完毕再把共享变量的值设置回null。 说白了，只要有个地方存这个共享变量就行了，而且要保证整个系统（多个进程）内只有这一份即可。 这也是redis实现分布式锁的关键。 redis分布式锁进化史进化史一——set既然上面说了实现分布式锁只需要对共享变量控制到位即可，那么redis我们怎么控制这个共享变量呢？ 首先，我们知道redis的基础命令有get/set/del，通过这三个命令可以实现分布式锁吗？当然可以。 在获取锁之前先 getlock_user_1看这个锁存不存在，如果不存在则再 setlock_user_1 value，如果存在则等待一段时间后再重试，最后使用完成了再删除这个锁 dellock_user_1即可。 但是，这种方案有个问题，如果一开始这个锁是不存在的，两个线程去同时get，这个时候返回的都是null（nil），然后这两个线程都去set，这时候就出问题了，两个线程都可以set成功，相当于两个线程都获取到同一个锁了。 所以，这种方案不可行！ 进化史二——setnx上面的方案不可行的主要原因是多个线程同时set都是可以成功的，所以后来有了 setnx这个命令，它是 setifnotexist的缩写，也就是如果不存在就set。 可以看到，当重复对同一个key进行setnx的时候，只有第一次是可以成功的。 因此，方案二就是先使用 setnx lock_user_1 value命令，如果返回1则表示加锁成功，如果返回0则表示其它线程先执行成功了，那就等待一段时间后重试，最后一样使用 dellock_user_1释放锁。 但是，这种方案也有个问题，如果获取锁的这个客户端断线了怎么办？这个锁不是一直都不会释放吗？是的，是这样的。 所以，这种方案也不可行！ 进化史三——setnx + setex上面的方案不可行的主要原因是获取锁之后客户端断线了无法释放锁的问题，那么，我在setnx之后立马再执行setex可以吗？ 答案是可以的，2.6.12之前的版本使用redis实现分布式锁大家都是这么玩的。 因此，方案三就是先使用 setnx lock_user_1 value命令拿到锁，再立即使用 setex lock_user_130value设置过期时间，最后使用 dellock_user_1释放锁。 在setnx获取到锁之后再执行setex设置过期时间，这样就很大概率地解决了获取锁之后客户端断线不会释放锁的问题。 但是，这种方案依然有问题，如果setnx之后setex之前这个客户端就断线了呢？嗯~，似乎无解，不过这种概率实在是非常小，所以2.6.12之前的版本大家也都这么用，几乎没出现过什么问题。 所以，这种方案基本可用，只是不太好！ 进化史四——set nx ex上面的方案不太好的主要原因是setnx/setex是两条独立的命令，无法解决前者成功之后客户端断线的问题，那么，把两条命令合在一起不就行了吗？ 是的，redis官方也意识到这个问题了，所以2.6.12版本给set命令加了一些参数： 1SET key value [EX seconds] [PX milliseconds] [NX|XX] EX，过期时间，单位秒 PX，过期时间，单位毫秒 NX，not exist，如果不存在才设置成功 XX，exist exist？如果存在才设置成功 通过这个命令我们就再也不怕客户端无故断线了。 因此，方案四就是先使用 setlock_user_1 value nx ex30获取锁，获取锁之后使用，使用完成了最后 dellock_user_1释放锁。 然而，这种方案就没有问题吗？ 当然有问题，其实这里的释放锁只要简单地执行 dellock_user_1即可，并不会检查这个锁是不是当前客户端获取到的。 所以，这种方案还不是很完美。 进化史五——random value + lua script上面的方案不完美的主要原因是释放锁这里控制的还不是很到位，那么有没有其它方法可以控制释放锁的线程和加锁的线程一定是同一个客户端呢？ redis官方给出的方案是这样的： 123456789// 加锁SET resource_name my_random_value NX PX 30000// 释放锁if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end 加锁的时候，设置随机值，保证这个随机值只有当前客户端自己知道。 释放锁的时候，执行一段lua脚本，把这段lua脚本当成一个完整的命令，先检查这个锁对应的值是不是上面设置的随机值，如果是再执行del释放锁，否则直接返回释放锁失败。 我们知道，redis是单线程的，所以这段lua脚本中的get和del不会存在并发问题，但是不能在java中先get再del，这样会当成两个命令，会有并发问题，lua脚本相当于是一个命令一起传输给redis的。 这种方案算是比较完美了，但是还有一点小缺陷，就是这个过期时间设置成多少合适呢？ 设置的过小，有可能上一个线程还没执行完锁内逻辑，锁就自动释放了，导致另一个线程可以获取锁了，就出现并发问题了； 设置的过大，就要考虑客户端断线了，这个锁要等待很长一段时间。 所以，这里又衍生出一个新的问题，过期时间我设置小一点，但是快到期了它能自动续期就好了。 进化史六——redisson(redis2.8+)上面方案的缺陷是过期时间不好把握，虽然也可以自己启一个监听线程来处理续期，但是代码实在不太好写，好在现成的轮子redisson已经帮我们把这个逻辑都实现好了，我们拿过来直接用就可以了。 而且，redisson充分考虑了redis演化过程中留下的各种问题，单机模式、哨兵模式、集群模式，它统统都处理好了，不管是从单机进化到集群还是从哨兵进化到集群，都只需要简单地修改下配置就可以了，不用改动任何代码，可以说是非（业）常（界）方（良）便（心）。 redisson实现的分布式锁内部使用的是Redlock算法，这是官方推荐的一种算法。 另外，redisson还提供了很多分布式对象（分布式的原子类）、分布式集合（分布式的Map/List/Set/Queue等）、分布式同步器（分布式的CountDownLatch/Semaphore等）、分布式锁（分布式的公平锁/非公平锁/读写锁等），有兴趣的可以去看看，下面贴出链接： Redlock介绍：https://redis.io/topics/distlock redisson介绍：https://github.com/redisson/redisson/wiki 代码实现因为前面五种方案都已经过时，所以彤哥这里偷个懒，就不去一一实现的，我们直接看最后一种redisson的实现方式。 pom.xml文件添加spring redis及redisson的依赖，我这里使用的是springboot 2.1.6版本，springboot 1.x版本的自己注意下，查看上面的github可以找到方法。 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson-spring-data-21&lt;/artifactId&gt; &lt;version&gt;3.11.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.11.0&lt;/version&gt;&lt;/dependency&gt; application.yml文件配置redis的连接信息，彤哥这里给出了三种方式。 1234567891011121314151617181920spring: redis: # 单机模式 #host: 192.168.1.102 #port: 6379 # password: &lt;your passowrd&gt; timeout: 6000ms # 连接超时时长（毫秒） # 哨兵模式 【本篇文章由公众号“彤哥读源码”原创】# sentinel:# master: &lt;your master&gt;# nodes: 192.168.1.101:6379,192.168.1.102:6379,192.168.1.103:6379 # 集群模式（三主三从伪集群） cluster: nodes: - 192.168.1.102:30001 - 192.168.1.102:30002 - 192.168.1.102:30003 - 192.168.1.102:30004 - 192.168.1.102:30005 - 192.168.1.102:30006 Locker接口定义Locker接口。 123public interface Locker { void lock(String key, Runnable command);} RedisLocker实现类直接使用RedissonClient获取锁，注意这里不需要再单独配置RedissonClient这个bean，redisson框架会根据配置自动生成RedissonClient的实例，我们后面说它是怎么实现的。 123456789101112131415161718@Componentpublic class RedisLocker implements Locker { @Autowired private RedissonClient redissonClient; @Override public void lock(String key, Runnable command) { RLock lock = redissonClient.getLock(key); try { // 【本篇文章由公众号“彤哥读源码”原创】 lock.lock(); command.run(); } finally { lock.unlock(); } }} 测试类启动1000个线程，每个线程内部打印一句话，然后睡眠1秒。 12345678910111213141516171819202122232425262728@RunWith(SpringRunner.class)@SpringBootTest(classes = Application.class)public class RedisLockerTest { @Autowired private Locker locker; @Test public void testRedisLocker() throws IOException { for (int i = 0; i &lt; 1000; i++) { new Thread(()-&gt;{ locker.lock(&quot;lock&quot;, ()-&gt; { // 可重入锁测试 locker.lock(&quot;lock&quot;, ()-&gt; { System.out.println(String.format(&quot;time: %d, threadName: %s&quot;, System.currentTimeMillis(), Thread.currentThread().getName())); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } }); }); }, &quot;Thread-&quot;+i).start(); } System.in.read(); }} 运行结果： 可以看到稳定在1000ms左右打印一句话，说明这个锁是可用的，而且是可重入的。 12345678910111213time: 1570100167046, threadName: Thread-756time: 1570100168067, threadName: Thread-670time: 1570100169080, threadName: Thread-949time: 1570100170093, threadName: Thread-721time: 1570100171106, threadName: Thread-937time: 1570100172124, threadName: Thread-796time: 1570100173134, threadName: Thread-944time: 1570100174142, threadName: Thread-974time: 1570100175167, threadName: Thread-462time: 1570100176180, threadName: Thread-407time: 1570100177194, threadName: Thread-983time: 1570100178206, threadName: Thread-982... RedissonAutoConfiguration刚才说RedissonClient不需要配置，其实它是在RedissonAutoConfiguration中自动配置的，我们简单看下它的源码，主要看redisson()这个方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149@Configuration@ConditionalOnClass({Redisson.class, RedisOperations.class})@AutoConfigureBefore(RedisAutoConfiguration.class)@EnableConfigurationProperties({RedissonProperties.class, RedisProperties.class})public class RedissonAutoConfiguration { @Autowired private RedissonProperties redissonProperties; @Autowired private RedisProperties redisProperties; @Autowired private ApplicationContext ctx; @Bean @ConditionalOnMissingBean(name = &quot;redisTemplate&quot;) public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;Object, Object&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean(StringRedisTemplate.class) public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean(RedisConnectionFactory.class) public RedissonConnectionFactory redissonConnectionFactory(RedissonClient redisson) { return new RedissonConnectionFactory(redisson); } @Bean(destroyMethod = &quot;shutdown&quot;) @ConditionalOnMissingBean(RedissonClient.class) public RedissonClient redisson() throws IOException { Config config = null; Method clusterMethod = ReflectionUtils.findMethod(RedisProperties.class, &quot;getCluster&quot;); Method timeoutMethod = ReflectionUtils.findMethod(RedisProperties.class, &quot;getTimeout&quot;); Object timeoutValue = ReflectionUtils.invokeMethod(timeoutMethod, redisProperties); int timeout; if(null == timeoutValue){ // 超时未设置则为0 timeout = 0; }else if (!(timeoutValue instanceof Integer)) { // 转毫秒 Method millisMethod = ReflectionUtils.findMethod(timeoutValue.getClass(), &quot;toMillis&quot;); timeout = ((Long) ReflectionUtils.invokeMethod(millisMethod, timeoutValue)).intValue(); } else { timeout = (Integer)timeoutValue; } // 看下是否给redisson单独写了一个配置文件 if (redissonProperties.getConfig() != null) { try { InputStream is = getConfigStream(); config = Config.fromJSON(is); } catch (IOException e) { // trying next format try { InputStream is = getConfigStream(); config = Config.fromYAML(is); } catch (IOException e1) { throw new IllegalArgumentException(&quot;Can't parse config&quot;, e1); } } } else if (redisProperties.getSentinel() != null) { // 如果是哨兵模式 Method nodesMethod = ReflectionUtils.findMethod(Sentinel.class, &quot;getNodes&quot;); Object nodesValue = ReflectionUtils.invokeMethod(nodesMethod, redisProperties.getSentinel()); String[] nodes; // 看sentinel.nodes这个节点是列表配置还是逗号隔开的配置 if (nodesValue instanceof String) { nodes = convert(Arrays.asList(((String)nodesValue).split(&quot;,&quot;))); } else { nodes = convert((List&lt;String&gt;)nodesValue); } // 生成哨兵模式的配置 config = new Config(); config.useSentinelServers() .setMasterName(redisProperties.getSentinel().getMaster()) .addSentinelAddress(nodes) .setDatabase(redisProperties.getDatabase()) .setConnectTimeout(timeout) .setPassword(redisProperties.getPassword()); } else if (clusterMethod != null &amp;&amp; ReflectionUtils.invokeMethod(clusterMethod, redisProperties) != null) { // 如果是集群模式 Object clusterObject = ReflectionUtils.invokeMethod(clusterMethod, redisProperties); Method nodesMethod = ReflectionUtils.findMethod(clusterObject.getClass(), &quot;getNodes&quot;); // 集群模式的cluster.nodes是列表配置 List&lt;String&gt; nodesObject = (List) ReflectionUtils.invokeMethod(nodesMethod, clusterObject); String[] nodes = convert(nodesObject); // 生成集群模式的配置 config = new Config(); config.useClusterServers() .addNodeAddress(nodes) .setConnectTimeout(timeout) .setPassword(redisProperties.getPassword()); } else { // 单机模式的配置 config = new Config(); String prefix = &quot;redis://&quot;; Method method = ReflectionUtils.findMethod(RedisProperties.class, &quot;isSsl&quot;); // 判断是否走ssl if (method != null &amp;&amp; (Boolean)ReflectionUtils.invokeMethod(method, redisProperties)) { prefix = &quot;rediss://&quot;; } // 生成单机模式的配置 config.useSingleServer() .setAddress(prefix + redisProperties.getHost() + &quot;:&quot; + redisProperties.getPort()) .setConnectTimeout(timeout) .setDatabase(redisProperties.getDatabase()) .setPassword(redisProperties.getPassword()); } return Redisson.create(config); } private String[] convert(List&lt;String&gt; nodesObject) { // 将哨兵或集群模式的nodes转换成标准配置 List&lt;String&gt; nodes = new ArrayList&lt;String&gt;(nodesObject.size()); for (String node : nodesObject) { if (!node.startsWith(&quot;redis://&quot;) &amp;&amp; !node.startsWith(&quot;rediss://&quot;)) { nodes.add(&quot;redis://&quot; + node); } else { nodes.add(node); } } return nodes.toArray(new String[nodes.size()]); } private InputStream getConfigStream() throws IOException { // 读取redisson配置文件 Resource resource = ctx.getResource(redissonProperties.getConfig()); InputStream is = resource.getInputStream(); return is; } } 网上查到的资料中很多配置都是多余的（可能是版本问题），看下源码很清楚，这也是看源码的一个好处。 总结（1）redis由于历史原因导致有三种模式：单机、哨兵、集群； （2）redis实现分布式锁的进化史：set -&gt; setnx -&gt; setnx + setex -&gt; set nx ex(或px) -&gt; set nx ex(或px) + lua script -&gt; redisson； （3）redis分布式锁有现成的轮子redisson可以使用； （4）redisson还提供了很多有用的组件，比如分布式集合、分布式同步器、分布式对象； 彩蛋redis分布式锁有哪些优点？ 答： 1）大部分系统都依赖于redis做缓存，不需要额外依赖其它组件（相对于zookeeper来说）； 2）redis可以集群部署，相对于mysql的单点更可靠； 3）不会占用mysql的连接数，不会增加mysql的压力； 4）redis社区相对活跃，redisson的实现更是稳定可靠； 5）利用过期机制解决客户端断线的问题，虽然不太及时； 6）有现成的轮子redisson可以使用，锁的种类比较齐全； redis分布式锁有哪些缺点？ 答： 1）集群模式下会在所有master节点执行加锁命令，大部分（2N+1）成功了则获得锁，节点越多，加锁的过程越慢； 2）高并发情况下，未获得锁的线程会睡眠重试，如果同一把锁竞争非常激烈，会占用非常多的系统资源； 3）历史原因导致的坑挺多的，自己很难实现出来健壮的redis分布式锁； 总之，redis分布式锁的优点是大于缺点的，而且社区活跃，这也是我们大部分系统使用redis作为分布式锁的原因。","link":"/redis-lock/"},{"title":"sdkman安装及使用","text":"sdk管理神器–sdkman安装使用教程 安装下载并安装123yum install zip -ycurl -s &quot;https://get.sdkman.io&quot; | bashsource &quot;$HOME/.sdkman/bin/sdkman-init.sh&quot; 查看安装是否成功1sdk version 安装成功后在home目录下有一个.sdkman文件夹 更新1sdk update 卸载12tar zcvf ~/sdkman-backup_$(date +%F-%kh%M).tar.gz -C ~/ .sdkmanrm -rf ~/.sdkman 查看帮助1sdk help 使用以安装jdk11为例 查找1sdk list java 或 1sdk ls java 查找结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162================================================================================Available Java Versions================================================================================ Vendor | Use | Version | Dist | Status | Identifier-------------------------------------------------------------------------------- AdoptOpenJDK | | 14.0.1.j9 | adpt | | 14.0.1.j9-adpt | | 14.0.1.hs | adpt | | 14.0.1.hs-adpt | | 13.0.2.j9 | adpt | | 13.0.2.j9-adpt | | 13.0.2.hs | adpt | | 13.0.2.hs-adpt | | 12.0.2.j9 | adpt | | 12.0.2.j9-adpt | | 12.0.2.hs | adpt | | 12.0.2.hs-adpt | | 11.0.7.j9 | adpt | | 11.0.7.j9-adpt | &gt;&gt;&gt; | 11.0.7.hs | adpt | installed | 11.0.7.hs-adpt | | 8.0.252.j9 | adpt | | 8.0.252.j9-adpt | | 8.0.252.hs | adpt | | 8.0.252.hs-adpt Amazon | | 11.0.7 | amzn | | 11.0.7-amzn | | 8.0.252 | amzn | | 8.0.252-amzn Azul Zulu | | 14.0.1 | zulu | | 14.0.1-zulu | | 13.0.3 | zulu | | 13.0.3-zulu | | 13.0.3.fx | zulu | | 13.0.3.fx-zulu | | 12.0.2 | zulu | | 12.0.2-zulu | | 11.0.7 | zulu | | 11.0.7-zulu | | 11.0.7.fx | zulu | | 11.0.7.fx-zulu | | 10.0.2 | zulu | | 10.0.2-zulu | | 9.0.7 | zulu | | 9.0.7-zulu | | 8.0.252 | zulu | | 8.0.252-zulu | | 8.0.252.fx | zulu | | 8.0.252.fx-zulu | | 8.0.232.fx | zulu | | 8.0.232.fx-zulu | | 7.0.262 | zulu | | 7.0.262-zulu | | 6.0.119 | zulu | | 6.0.119-zulu BellSoft | | 14.0.1.fx | librca | | 14.0.1.fx-librca | | 14.0.1 | librca | | 14.0.1-librca | | 13.0.2.fx | librca | | 13.0.2.fx-librca | | 13.0.2 | librca | | 13.0.2-librca | | 12.0.2 | librca | | 12.0.2-librca | | 11.0.7.fx | librca | | 11.0.7.fx-librca | | 11.0.7 | librca | | 11.0.7-librca | | 8.0.252.fx | librca | | 8.0.252.fx-librca | | 8.0.252 | librca | | 8.0.252-librca GraalVM | | 20.1.0.r11 | grl | | 20.1.0.r11-grl | | 20.1.0.r8 | grl | | 20.1.0.r8-grl | | 20.0.0.r11 | grl | | 20.0.0.r11-grl | | 20.0.0.r8 | grl | | 20.0.0.r8-grl | | 19.3.1.r11 | grl | | 19.3.1.r11-grl | | 19.3.1.r8 | grl | | 19.3.1.r8-grl Java.net | | 15.ea.23 | open | | 15.ea.23-open | | 14.0.1 | open | | 14.0.1-open | | 13.0.2 | open | | 13.0.2-open | | 12.0.2 | open | | 12.0.2-open | | 11.0.7 | open | | 11.0.7-open | | 10.0.2 | open | | 10.0.2-open | | 9.0.4 | open | | 9.0.4-open | | 8.0.252 | open | | 8.0.252-open SAP | | 14.0.1 | sapmchn | | 14.0.1-sapmchn | | 13.0.2 | sapmchn | | 13.0.2-sapmchn | | 12.0.2 | sapmchn | | 12.0.2-sapmchn | | 11.0.7 | sapmchn | | 11.0.7-sapmchn================================================================================Use the Identifier for installation: $ sdk install java 11.0.3.hs-adpt================================================================================ 安装安装AdoptOpenJDK 11 1sdk install java 11.0.7.hs-adpt 或 1sdk i java 11.0.7.hs-adpt 卸载1sdk uninstall java 11.0.7.hs-adpt 或 1sdk rm java 11.0.7.hs-adpt","link":"/sdkman/"},{"title":"docker-compose安装seaweedfs","text":"SeaweedFS是一个简单且高度可扩展的分布式文件系统，可以快速存储和提供数十亿个文件！SeaweedFS磁盘搜索时间复杂度O(1)github地址：https://github.com/chrislusf/seaweedfs 新建seaweedfs.yml 123456789101112131415161718192021222324252627282930313233343536373839 version: '2'services: master: image: chrislusf/seaweedfs: use a remote image ports: - 9333:9333 - 19333:19333 command: &quot;master -ip=master&quot; volume: image: chrislusf/seaweedfs:1.81 # use a remote image ports: - 8080:8080 - 18080:18080 command: 'volume -mserver=&quot;master:9333&quot; -port=8080' depends_on: - master filer: image: chrislusf/seaweedfs # use a remote image ports: - 8888:8888 - 18888:18888 command: 'filer -master=&quot;master:9333&quot;' tty: true stdin_open: true depends_on: - master - volume cronjob: image: chrislusf/seaweedfs # use a remote image command: 'cronjob' environment: # Run re-replication every 2 minutes CRON_SCHEDULE: '*/2 * * * * *' # Default: '*/5 * * * * *' WEED_MASTER: master:9333 # Default: localhost:9333 depends_on: - master - volume 启动： 1docker-compose -f seaweedfs.yml up -d","link":"/seaweedfs-install/"},{"title":"shell十三问","text":"我在CU的日子并不长，有幸在shell版上与大家结缘。除了跟众前辈学习到不少技巧之外，也常看到不少朋友的问题。然而，在众多问题中，我发现许多瓶颈都源于shell的基础而已。每次要解说，却总有千言万语不知从何起之感… 这次，我不是来回答，而是准备了关于shell基础的十三个问题要问大家﹗希望shell学习者们能够透过寻找答案的过程，好好的将shell基础打扎实一点…当然了，这些问题我也会逐一解说一遍。只是，我不敢保证什么时候能够完成这趟任务。除了时间关系外，个人功力实在有限，很怕匆忙间误导观众就糟糕了。若能抛砖引玉，诱得其他前辈出马补充，那才真的是功德一件﹗ 第1问：为何叫做shell？在介绍 shell 是什么东西之前，不妨让我们重新审视使用者与电脑的关系。我们知道电脑的运作不能离开硬件，但使用者却无法直接对硬件作驱动，硬件的驱动只能透过一个称为“操作系统（Operating System）”的软件来控管，事实上，我们每天所谈的linux，严格来说只是一个操作系统，我们称之为“核心（kernel）”。然而，从使用者的角度来说，使用者也没办法直接操作kernel，而是透过kernel的“外壳”程序，也就是所谓的shell，来与kernel沟通。这也正是kernel跟shell的形像命名关系。 从技术角度来说，shell是一个使用者与系统的互动界面（interface），主要是让使用者透过命令行（command line）来使用系统以完成工作。因此，shell的最简单的定义就是—命令解释器（Command Interpreter）： 将使用者的命令翻译给核心处理， 同时，将核心处理结果翻译给使用者。 每次当我们完成系统登入（log in），我们就取得一个互动模式的shell，也称为login shell或primary shell。若从行程（process）角度来说，我们在shell所下达的命令，均是shell所产生的子行程。这现像，我们暂可称之为fork。如果是执行脚本（shell script）的话，脚本中的命令则是由另外一个非互动模式的子shell（sub shell）来执行的。也就是primary shell产生sub shell的行程，sub shell再产生script中所有命令的行程。（关于行程，我们日后有机会再补充。） 这里，我们必须知道：kernel与shell是不同的两套软件，而且都是可以被替换的： 不同的操作系统使用不同的kernel， 而在同一个kernel之上，也可使用不同的shell。 在linux的预设系统中，通常都可以找到好几种不同的shell，且通常会被列于如下档案里： 1/etc/shells 不同的shell有着不同的功能，且也彼此各异、或说“大同小异”。常见的shell主要分为两大主流：sh： 12burne shell（sh）burne again shell（bash） csh： 123c shell（csh）tc shell（tcsh）korn shell（ksh） 大部份的Linux系统的预设shell都是bash，其原因大致如下两点： 自由软件 功能强大 bash是gnu project最成功的产品之一，自推出以来深受广大Unix用户喜爱，且也逐渐成为不少组织的系统标准。 第2问：shell prompt（PS1）与 Carriage Return（CR）的关系？当你成功登录进一个文字界面之后，大部份情形下，你会在荧幕上看到一个不断闪烁的方块或底线（视不同版本而别），我们称之为游标（cursor）。游标的作用就是告诉你接下来你从键盘输入的按键所插入的位置，且每输入一键游标便向右边移动一个格子，若连续输入太多的话，则自动接在下一行输入。假如你刚完成登录还没输入任何按键之前，你所看到的游标所在位置的同一行的左边部份，我们称之为提示符号（prompt）。提示符号的格式或因不同系统版本而各有不同，在Linux上，只需留意最接近游标的一个可见的提示符号，通常是如下两者之一： 12$：给一般使用者帐号使用#：给root（管理员）帐号使用 事实上，shell prompt的意思很简单：是shell告诉使用者：您现在可以输入命令行了。我们可以说，使用者只有在得到shell prompt才能打命令行，而cursor是指示键盘在命令行所输入的位置，使用者每输入一个键，cursor就往后移动一格，直到碰到命令行读进CR（Carriage Return，由Enter键产生）字符为止。CR的意思也很简单：是使用者告诉shell：老兄你可以执行我的命令了。 严格来说：所谓的命令行，就是在shell prompt与CR字符之间所输入的文字。（思考：为何我们这里坚持使用CR字符而不说Enter键呢？答案在后面的学习中揭晓。）不同的命令可接受的命令行格式或有不同，一般情况下，一个标准的命令行格式为如下所列： 1command-name options argument 若从技术细节来看，shell会依据IFS（Internal Field Seperator）将command line所输入的文字给拆解为“字段”（word）。然后再针对特殊字符（meta）先作处理，最后再重组整行command line。（注意：请务必理解上两句话的意思，我们日后的学习中会常回到这里思考。） 其中的IFS是shell预设使用的字段分隔符号，可以由一个及多个如下按键组成： 空白键（White Space） 表格键（Tab） 回车键（Enter） 系统可接受的命令名称（command-name）可以从如下途径获得： 明确路径所指定的外部命令 命令别名（alias） 自定功能（function） shell内置命令（built-in） $PATH之下的外部命令 每一个命令行均必需含用命令名称，这是不能缺少的。 第3问：别人echo、你也echo，是问echo知多少？承接上一章所介绍的command line，这里我们用echo这个命令加以进一步说明。温习—标准的command line包含三个部件： 1command_name option argument echo是一个非常简单、直接的Linux命令：将argument送出至标准输出（STDOUT），通常就是在显示器（monitor）上输出。（注：stdout我们日后有机会再解说）。为了更好理解，不如先让我们先跑一下echo命令好了： 1$ echo 你会发现只有一个空白行，然后又回到shell prompt上了。这是因为echo在预设上，在显示完argument之后，还会送出一个换行符号（new-line charactor）。但是上面的command并没任何的argument，那结果就只剩一个换行符号了… 若你要取消这个换行符号，可利用echo的-n option： 1$ echo -n 不妨让我们回到command line的概念上来讨论上例的echo命令好了：command line只有command_name（echo）及option（-n），并没有任何argument。 要想看看echo的argument，那还不简单﹗接下来，你可试试如下的输入： 1234$ echo first linefirst line$ echo -n first linefirst line $ 于上两个echo命令中，你会发现argument的部份显示在你的荧幕，而换行符号则视-n option的有无而别。很明显的，第二个echo由于换行符号被取消了，接下来的shell prompt就接在输出结果同一行了… 事实上，echo除了-n options之外，常用选项还有： -e：启用反斜线控制字符的转换（参考下表） -E：关闭反斜线控制字符的转换（预设如此） -n：取消行末之换行符号（与-e选项下的\\c字符同意） 关于echo命令所支持的反斜线控制字符如下表： \\a:ALERT / BELL（从系统喇叭送出铃声） \\b:BACKSPACE，也就是向左退格键 \\c：取消行末之换行符号 \\E:ESCAPE，跳脱键 \\f:FORMFEED，分页字符 \\n:NEWLINE，换行字符 \\r:RETURN，回车键 \\t:TAB，表格跳位键 \\v:VERTICAL TAB，垂直表格跳位键 \\n:ASCII八进位编码（以x开首为十六进位） \\：反斜线本身 （表格数据来自O’Reilly出版社之Learning the Bash Shell，2nd Ed.） 或许，我们可以透过实例来了解echo的选项及控制字符：例一： 123$ echo -e &quot;a\\tb\\tc\\nd\\te\\tf&quot;a b cd e f 上例运用\\t来区隔abc还有def，及用\\n将def换至下一行。 例二： 123$ echo -e &quot;\\141\\011\\142\\011\\143\\012\\144\\011\\145\\011\\146&quot;a b cd e f 与例一的结果一样，只是使用ASCII八进位编码。 例三： 123$ echo -e &quot;\\x61\\x09\\x62\\x09\\x63\\x0a\\x64\\x09\\x65\\x09\\x66&quot;a b cd e f 与例二差不多，只是这次换用ASCII十六进位编码。 例四： 123$ echo -ne &quot;a\\tb\\tc\\nd\\te\\bf\\a&quot;a b cd f $ 因为e字母后面是退格键（\\b），因此输出结果就没有e了。在结束时听到一声铃向，那是\\a的杰作﹗由于同时使用了-n选项，因此shell prompt紧接在第二行之后。若你不用-n的话，那你在\\a后再加个\\c，也是同样的效果。 事实上，在日后的shell操作及shell script设计上，echo命令是最常被使用的命令之一。比方说，用echo来检查变量值： 12345$ A=B$ echo $AB$ echo $？0 （注：关于变量概念，我们留到下两章才跟大家说明。）好了，更多的关于command line的格式，以及echo命令的选项，就请您自行多加练习、运用了… 第4问：“”（双引号）与’’（单引号）差在哪？还是回到我们的command line来吧… 经过前面两章的学习，应该很清楚当你在shell prompt后面敲打键盘、直到按下Enter的时候，你输入的文字就是command line了，然后shell才会以行程的方式执行你所交给它的命令。但是，你又可知道：你在command line输入的每一个文字，对shell来说，是有类别之分的呢？ 简单而言（我不敢说这是精确的定议，注一），command line的每一个charactor，分为如下两种： literal：也就是普通纯文字，对shell来说没特殊功能。 meta：对shell来说，具有特定功能的特殊保留字元。（注一：关于bash shell在处理command line时的顺序说明，请参考O’Reilly出版社之Learning the Bash Shell，2nd Edition，第177 - 180页的说明，尤其是178页的流程图Figure 7-1…） Literal没甚么好谈的，凡举abcd、123456这些“文字”都是literal…（easy？）。但meta却常使我们困惑…..（confused？）事实上，前两章我们在command line中已碰到两个机乎每次都会碰到的meta： IFS：由&lt;space&gt;或&lt;tab&gt;或&lt;enter&gt;三者之一组成（我们常用space）。 CR：由&lt;enter&gt;产生。 IFS是用来拆解command line的每一个词（word）用的，因为shell command line是按词来处理的。而CR则是用来结束command line用的，这也是为何我们敲命令就会跑的原因。除了IFS与CR，常用的meta还有： = : 设定变量。 $ : 作变量或运算替换（请不要与shell prompt搞混了）。 &gt; : 重导向stdout。 &lt; : 重导向stdin。 | : 命令管线。 &amp; : 重导向file descriptor，或将命令置于背境执行。 () : 将其内的命令置于nested subshell执行，或用于运算或命令替换。 {} : 将其内的命令置于non-named function中执行，或用在变量替换的界定范围。 ; : 在前一个命令结束时，而忽略其返回值，继续执行下一个命令。 &amp;&amp; : 在前一个命令结束时，若返回值为true，继续执行下一个命令。 || : 在前一个命令结束时，若返回值为false，继续执行下一个命令。 ！: 执行history列表中的命令… 假如我们需要在command line中将这些保留字元的功能关闭的话，就需要quoting处理了。在bash中，常用的quoting有如下三种方法： hard quote:’ ‘（单引号），凡在hard quote中的所有meta均被关闭。 soft quote:“”（双引号），在soft quote中大部份meta都会被关闭，但某些则保留（如$）。（注二：在soft quote中被豁免的具体meta清单，我不完全知道，有待大家补充，或透过实作来发现及理解。） escape : \\（反斜线），只有紧接在escape（跳脱字符）之后的单一meta才被关闭。 下面的例子将有助于我们对quoting的了解： 1234567$ A=B C #空白键未被关掉，作为IFS处理。$ C: command not found.$ echo $A$ A=&quot;B C&quot; #空白键已被关掉，仅作为空白键处理。$ echo $AB C 在第一次设定A变量时，由于空白键没被关闭，command line将被解读为：A=B然后碰到，再执行C命令 在第二次设定A变量时，由于空白键被置于soft quote中，因此被关闭，不再作为IFS：A=BC 事实上，空白键无论在soft quote还是在hard quote中，均会被关闭。Enter键亦然： 123456$ A='B&gt; C&gt; '$ echo &quot;$A&quot;BC 在上例中，由于被置于hard quote当中，因此不再作为CR字符来处理。这里的单纯只是一个断行符号（new-line）而已，由于command line并没得到CR字符，因此进入第二个shell prompt（PS2，以&gt;符号表示），command line并不会结束，直到第三行，我们输入的并不在hard quote里面，因此并没被关闭，此时，command line碰到CR字符，于是结束、交给shell来处理。上例的要是被置于soft quote中的话，CR也会同样被关闭： 12345$ A=&quot;B&gt; C&gt;&quot;$ echo $AB C 然而，由于echo $A时的变量没至于soft quote中，因此当变量替换完成后并作命令行重组时，会被解释为IFS，而不是解释为New Line字符。 同样的，用escape亦可关闭CR字符： 12345$ A=B\\&gt; C\\&gt;$ echo $ABC 上例中，第一个跟第二个均被escape字符关闭了，因此也不作为CR来处理，但第三个由于没被跳脱，因此作为CR结束command line。 但由于键本身在shell meta中的特殊性，在\\跳脱后面，仅仅取消其CR功能，而不会保留其IFS功能。 您或许发现光是一个键所产生的字符就有可能是如下这些可能：CR、IFS、NL（New Line）、FF（Form Feed）、NULL …至于什么时候会解释为什么字符，这个我就没去深挖了，或是留给读者诸君自行慢慢摸索了… 至于soft quote跟hard quote的不同，主要是对于某些meta的关闭与否，以$来作说明： 12345$ A=B\\ C$ echo &quot;$A&quot;B C$ echo '$A'$A 在第一个echo命令行中，$被置于soft quote中，将不被关闭，因此继续处理变量替换，因此echo将A的变量值输出到荧幕，也就得到B C的结果。在第二个echo命令行中，被置于hard quote中，则被关闭，因此被置于hardquote中，则被关闭，因此只是一个符号，并不会用来作变量替换处理，因此结果是符号，并不会用来作变量替换处理，因此结果是符号后面接一个A字母：$A。 1234567# 练习与思考：如下结果为何不同？$ A=B\\ C$ echo '&quot;$A&quot;' #最外面的是单引号&quot;$A&quot;$ echo &quot;'$A'&quot; #最外面的是双引号'B C'（提示：单引号及双引号，在quoting中均被关闭了。） 在CU的shell版里，我发现有很多初学者的问题，都与quoting理解的有关。比方说，若我们在awk或sed的命令参数中调用之前设定的一些变量时，常会问及为何不能的问题。要解决这些问题，关键点就是：区分出shell meta与command meta 前面我们提到的那些meta，都是在command line中有特殊用途的，比方说{ }是将其内一系列command line置于不具名的函式中执行（可简单视为command block），但是，awk却需要用{ }来区分出awk的命令区段（BEGIN，MAIN，END）。若你在command line中如此输入： 1$ awk {print $0} 1.txt 由于{ }在shell中并没关闭，那shell就将{print $0}视为command block，但同时又没有;符号作命令区隔，因此就出现awk的语法错误结果。要解决之，可用hard quote： 1$ awk '{print $0}' 1.txt 上面的hard quote应好理解，就是将原本的{、&lt;space&gt;、$（注三）、}这几个shell meta关闭，避免掉在shell中遭到处理，而完整的成为awk参数中的command meta。（注三：而其中的0是awk内置的field number，而非awk的变量，awk自身的变量无需使用0是awk内置的fieldnumber，而非awk的变量，awk自身的变量无需使用。） 要是理解了hard quote的功能，再来理解soft quote与escape就不难： 12awk &quot;{print \\$0}&quot; 1.txtawk \\{print\\ \\$0\\} 1.txt 然而，若你要改变awk的0的0值是从另一个shell变量读进呢？比方说：已有变量0的0值是从另一个shell变量读进呢？比方说：已有变量A的值是0，那如何在command line中解决awk的$$A呢？你可以很直接否定掉hard quoe的方案： 1$ awk '{print $$A}' 1.txt 那是因为A的A的在hard quote中是不能替换变量的。聪明的读者（如你！），经过本章学习，我想，应该可以解释为何我们可以使用如下操作了吧： 12345A=0awk &quot;{print \\$$A}&quot; 1.txtawk \\{print\\ \\$$A\\} 1.txtawk '{print $'$A'}' 1.txtawk '{print $'&quot;$A&quot;'}' 1.txt #注：“$A”包在soft quote中 或许，你能举出更多的方案呢…. 练习与思考：请运用本章学到的知识分析如下两串讨论：http://bbs.chinaunix.net/forum/viewtopic.php?t=207178http://bbs.chinaunix.net/forum/viewtopic.php?t=207178 第5问：var=value? export前后差在哪?这次让我们暂时丢开command line，先来了解一下bash变量（variable）吧… 所谓的变量，就是利用一个特定的“名称”（name）来存取一段可以变化的“值”（value）。 设定（set）在bash中，你可以用“=”来设定或重新定义变量的内容： 1name=value 在设定变量的时侯，得遵守如下规则： 等号左右两边不能使用区隔符号（IFS），也应避免使用shell的保留字元（meta charactor）。 变量名称不能使用$符号。 变量名称的第一个字母不能是数字（number）。 变量名称长度不可超过256个字母。 变量名称及变量值之大小写是有区别的（case sensitive）。 如下是一些变量设定时常见的错误： 1234A= B # 不能有IFS1A=B # 不能以数字开头$A=B # 名称不能有$a=B # 这跟a=b是不同的（这不是错误，提醒windows的使用者要特别注意） 如下则是可以接受的设定： 1234A=&quot; B&quot; # IFS被关闭了（请参考前面的quoting章节）A1=B # 并非以数字开头A=$B # $可用在变量值内This_Is_A_Long_Name=b #可用_连接较长的名称或值，且大小写有别。 变量替换（substitution）Shell之所以强大，其中的一个因素是它可以在命令行中对变量作替换（substitution）处理。在命令行中使用者可以使用$符号加上变量名称（除了在用=号定义变量名称之外），将变量值给替换出来，然后再重新组建命令行。比方： 1234$ A=ls$ B=la$ C=/tmp$ $A -$B $C （注意：以上命令行的第一个$是shell prompt，并不在命令行之内。） 必需强调的是，我们所提的变量替换，只发生在command line上面。（是的，让我们再回到command line吧﹗）仔细分析最后那行command line，不难发现在被执行之前（在输入CR字符之前），$符号会对每一个变量作替换处理（将变量值替换出来再重组命令行），最后会得出如下命令行： 1ls -la /tmp 还记得第二章我请大家“务必理解”的那两句吗？若你忘了，那我这里再重贴一遍：若从技术细节来看，shell会依据IFS（Internal Field Seperator）将command line所输入的文字给拆解为“字段”（word）。然后再针对特殊字符（meta）先作处理，最后再重组整行command line。这里的$就是command line中最经典的meta之一了，就是作变量替换的﹗ 在日常的shell操作中，我们常会使用echo命令来查看特定变量的值，例如： 1$ echo $A -$B $C 我们已学过，echo命令只单纯将其argument送至“标准输出”（STDOUT，通常是我们的荧幕）。所以上面的命令会在荧幕上得到如下结果： 1ls -la /tmp 这是由于echo命令在执行时，会先将A（ls）、A（ls）、B（la）、跟$C（/tmp）给替换出来的结果。利用shell对变量的替换处理能力，我们在设定变量时就更为灵活了： 12A=BB=$A 这样，B的变量值就可继承A变量“当时”的变量值了。不过，不要以“数学逻辑”来套用变量的设定，比方说： 12A=BB=C 这样并不会让A的变量值变成C。再如： 123A=BB=$AA=C 同样也不会让B的值换成C。上面是单纯定义了两个不同名称的变量：A与B，它们的值分别是B与C。 若变量被重复定义的话，则原有旧值将被新值所取代。（这不正是“可变的量”吗？） 当我们在设定变量的时侯，请记着这点：用一个名称储存一个数值，仅此而已。此外，我们也可利用命令行的变量替换能力来“扩充”（append）变量值： 12A=B:C:DA=$A:E 这样，第一行我们设定A的值为“B:C:D”，然后，第二行再将值扩充为“B:C:D:E”。上面的扩充示例，我们使用区隔符号（：）来达到扩充目的，要是没有区隔符号的话，如下是有问题的： 12A=BCDA=$AE 因为第二次是将A的值继承AE的提换结果，而非AE的提换结果，而非A再加E！要解决此问题，我们可用更严谨的替换处理： 12A=BCDA=${A}E 上例中，我们使用{}将变量名称的范围给明确定义出来，如此一来，我们就可以将A的变量值从BCD给扩充为BCDE。（提示：关于${name}事实上还可做到更多的变量处理能力，这些均属于比较进阶的变量处理，现阶段暂时不介绍了，请大家自行参考资料。如CU的贴子：http://www.chinaunix.net/forum/viewtopic.php?t=201843 export严格来说，我们在当前shell中所定义的变量，均属于“本地变量”（local variable），只有经过export命令的“输出”处理，才能成为环境变量（environment variable）： 12$ A=B$ export A 或： 1$ export A=B 经过export输出处理之后，变量A就能成为一个环境变量供其后的命令使用。在使用export的时侯，请别忘记shell在命令行对变量的“替换”（substitution）处理，比方说： 123$ A=B$ B=C$ export $A 上面的命令并未将A输出为环境变量，而是将B作输出，这是因为在这个命令行中，$A会首先被替换为B，然后再“塞回”作export的参数。要理解这个export，事实上需要从process的角度来理解才能透彻。我将于下一章为大家说明process的观念，敬请留意。 取消变量要取消一个变量，在bash中可使用unset命令来处理： 1unset A 与export一样，unset命令行也同样会作变量替换（这其实就是shell的功能之一），因此： 123$ A=B$ B=C$ unset $A 事实上所取消的变量是B而不是A。 此外，变量一旦经过unset取消之后，其结果是将整个变量拿掉，而不仅是取消其变量值。如下两行其实是很不一样的： 12$ A=$ unset A 第一行只是将变量A设定为“空值”（null value），但第二行则让变量A不在存在。虽然用眼睛来看，这两种变量状态在如下命令结果中都是一样的： 12345$ A=$ echo $A$ unset A$ echo $A 请学员务必能识别null value与unset的本质区别，这在一些进阶的变量处理上是很严格的。比方说： 123456789101112$ str= #设为null$ var=${str=expr} #定义var$ echo $var$ echo $str$ unset str #取消$ var=${str=expr} #定义var$ echo $varexpr$ echo $strexpr 聪明的读者（yes，you！），稍加思考的话，应该不难发现为何同样的var=${str=expr}在null与unset之下的不同吧？若你看不出来，那可能是如下原因之一： 你太笨了 不了解var=${str=expr}这个进阶处理 对本篇说明还没来得及消化吸收 我讲得不好 不知，你选哪个呢？…. 嗯… 好吧，我就解釋一下 var=${str=expr} ： 首先，var=str 這個大家都可理解吧。而接下來的思考方向是，究竟str這個大家都可理解吧。而接下來的思考方向是，究竟str 這個變量是如下哪一種情況呢： unset null not null 假如是 unset ，那麼 var=${str=expr} 的結果將是： 12var=exprstr=expr 假如是 null ，那 var=${str=expr} 的結果是： 12var=str= 假如是 not null (比方為 xyz )，那 var=${str=expr} 之結果是： 12var=xyzstr=xyz 第6问：exec跟source差在哪？这次先让我们从CU Shell版的一个实例贴子来谈起吧：（论坛改版后原连接已经失效）例中的提问原文如下： cd /etc/aa/bb/cc 可以执行，但是把这条命令写入shell时shell不执行！这是什么原因呀！（意思是：运行脚本后并没有移到/etc/aa/bb/cc目录） 我当时如何回答暂时别去深究，先让我们了解一下行程（process）的观念好了。首先，我们所执行的任何程序，都是由父行程（parent process）所产生出来的一个子行程（child process），子行程在结束后，将返回到父行程去。此一现像在Linux系统中被称为fork。（为何要程为fork呢？嗯，画一下图或许比较好理解…） 当子行程被产生的时候，将会从父行程那里获得一定的资源分配、及（更重要的是）继承父行程的环境﹗让我们回到上一章所谈到的“环境变量”吧： 所谓环境变量其实就是那些会传给子行程的变量。简单而言，“遗传性”就是区分本地变量与环境变量的决定性指标。 然而，从遗传的角度来看，我们也不难发现环境变量的另一个重要特征： 环境变量只能从父行程到子行程单向继承。换句话说：在子行程中的环境如何变更，均不会影响父行程的环境。 接下来，再让我们了解一下命令脚本（shell script）的概念。所谓的shell script讲起来很简单，就是将你平时在shell prompt后所输入的多行command line依序写入一个文件去而已。其中再加上一些条件判断、互动界面、参数运用、函数调用等等技巧，得以让script更加“聪明”的执行，但若撇开这些技巧不谈，我们真的可以简单的看成script只不过依次执行预先写好的命令行而已。 再结合以上两个概念（process + script），那应该就不难理解如下这句话的意思了： 正常来说，当我们执行一个shell script时，其实是先产生一个sub-shell的子行程，然后sub-shell再去产生命令行的子行程。 然则，那让我们回到本章开始时所提到的例子再从新思考：cd /etc/aa/bb/cc可以执行，但是把这条命令写入shell时shell不执行！这是什么原因呀！我当时的答案是这样的： 因为，一般我们跑的shell script是用subshell去执行的。从process的观念来看，是parent process产生一个child process去执行，当child结束后，会返回parent，但parent的环境是不会因child的改变而改变的。所谓的环境元数很多，凡举effective id，variable，workding dir等等…其中的workding dir（PWD）正是楼主的疑问所在：当用subshell来跑script的话，sub shell的PWD）正是楼主的疑问所在：当用subshell来跑script的话，subshell的PWD会因为cd而变更，但当返回primary shell时，$PWD是不会变更的。 能够了解问题的原因及其原理是很好的，但是？如何解决问题恐怕是我们更感兴趣的﹗是吧？ 那好，接下来，再让我们了解一下source命令好了。当你有了fork的概念之后，要理解source就不难： 所谓source就是让script在当前shell内执行、而不是产生一个sub-shell来执行。 由于所有执行结果均于当前shell内完成，若script的环境有所改变，当然也会改变当前环境了﹗因此，只要我们将原本单独输入的script命令行变成source命令的参数，就可轻易解决前例提到的问题了。 比方说，原本我们是如此执行script的： 1./my.script 现在改成这样即可： 1source ./my.script 或： 1../my.script 说到这里，我想，各位有兴趣看看/etc底下的众多设定文件，应该不难理解它们被定义后，如何让其他script读取并继承了吧？若然，日后你有机会写自己的script，应也不难专门指定一个设定文件以供不同的script一起“共享”了… okay，到这里，若你搞得懂fork与source的不同，那接下来再接受一个挑战： 那exec又与source/fork有何不同呢？ 哦…要了解exec或许较为复杂，尤其扯上File Descriptor的话…不过，简单来说： exec也是让script在同一个行程上执行，但是原有行程则被结束了。也就是简而言之：原有行程会否终止，就是exec与source/fork的最大差异了。 嗯，光是从理论去理解，或许没那么好消化，不如动手“实作+思考”来的印象深刻哦。下面让我们写两个简单的script，分别命名为1.sh及2.sh： 1.sh 123456789101112131415161718#!/bin/bashA=Becho &quot;PID for 1.sh before exec/source/fork:$$&quot;export Aecho &quot;1.sh: \\$A is $A&quot;case $1 in exec) echo &quot;using exec…&quot; exec ./2.sh;; source) echo &quot;using source…&quot; ../2.sh;; *) echo &quot;using fork by default…&quot; ./2.sh;;esacecho &quot;PID for 1.sh after exec/source/fork:$$&quot;echo &quot;1.sh: \\$A is $A&quot; 2.sh 123456#!/bin/bashecho &quot;PID for 2.sh: $$&quot;echo &quot;2.sh get \\$A=$A from 1.sh&quot;A=Cexport Aecho &quot;2.sh: \\$A is $A&quot; 然后，分别跑如下参数来观察结果： 123$ ./1.sh fork$ ./1.sh source$ ./1.sh exec 好了，别忘了仔细比较输出结果的不同及背后的原因哦…若有疑问，欢迎提出来一起讨论讨论 happy scripting！ 第7问：( ) 与 { } 差在哪？嗯，这次轻松一下，不讲太多… 先说一下，为何要用（）或{ }好了。许多时候，我们在shell操作上，需要在一定条件下一次执行多个命令，也就是说，要么不执行，要么就全执行，而不是每次依序的判断是否要执行下一个命令。或是，需要从一些命令执行优先次顺中得到豁免，如算术的2*（3+4）那样… 这时候，我们就可引入“命令群组”（command group）的概念：将多个命令集中处理。在shell command line中，一般人或许不太计较（）与{ }这两对符号的差异，虽然两者都可将多个命令作群组化处理，但若从技术细节上，却是很不一样的： ( ) 将command group置于sub-shell去执行，也称nested sub-shell。 { } 则是在同一个shell内完成，也称为non-named command group。 若，你对上一章的fork与source的概念还记得了的话，那就不难理解两者的差异了。 要是在command group中扯上变量及其他环境的修改，我们可以根据不同的需求来使用（）或{ }。 通常而言，若所作的修改是临时的，且不想影响原有或以后的设定，那我们就nested sub-shell，反之，则用non-named command group。 是的，光从command line来看，()与{}的差别就讲完了，够轻松吧然而，若这两个meta用在其他command meta或领域中（如Regular Expression），还是有很多差别的。只是，我不打算再去说明了，留给读者自己慢慢发掘好了…我这里只想补充一个概念，就是function。所谓的function，就是用一个名字去命名一个command group，然后再调用这个名字去执行command group。从non-named command group来推断，大概你也可以猜到我要说的是{ }了吧？（yes！你真聪明﹗） 在bash中，function的定义方式有两种：方式一： 123456function function_name {command1command2command3…} 方式二： 123456fuction_name() {command1command2command3…} 用哪一种方式无所谓，只是若碰到所定意的名称与现有的命令或别名（Alias）冲突的话，方式二或许会失败。但方式二起码可以少打function这一串英文字母，对懒人来说（如我），又何乐不为呢？… function在某一程度来说，也可称为“函式”，但请不要与传统编程所使用的函式（library）搞混了，毕竟两者差异很大。惟一相同的是，我们都可以随时用“已定义的名称”来调用它们… 若我们在shell操作中，需要不断的重覆执行某些命令，我们首先想到的，或许是将命令写成命令脚本（shell script）。不过，我们也可以写成function，然后在command line中打上function_name就可当一舨的script来使用了。 只是若你在shell中定义的function，除了可用unset function_name取消外，一旦退出shell，function也跟着取消。 然而，在script中使用function却有许多好处，除了可以提高整体script的执行性能外（因为已被加载），还可以节省许多重覆的代码… 简单而言，若你会将多个命令写成script以供调用的话，那，你可以将function看成是script中的script… 而且，透过上一章介绍的source命令，我们可以自行定义许许多多好用的function，再集中写在特定文件中，然后，在其他的script中用source将它们加载并反复执行。 若你是RedHat Linux的使用者，或许，已经猜得出/etc/rc.d/init.d/functions这个文件是作啥用的了 okay，说要轻松点的嘛，那这次就暂时写到这吧。祝大家学习愉快﹗ 第8问：(( )) 与(())与( ) 还有${ } 差在哪？我们上一章介绍了( )与{ }的不同，这次让我们扩展一下，看看更多的变化：（）与（）与{ }又是啥玩意儿呢？ 在bash shell中，$() 与 （反引号）都是用来做命令替换用（command substitution）的。所谓的命令替换与我们第五章学过的变量替换差不多，都是用来重组命令行： 完成引号里的命令行，然后将其结果替换出来，再重组命令行。 例如： 1$ echo the last sunday is $(date -d &quot;last sunday&quot; +%Y-%m-%d) 如此便可方便得到上一星期天的日期了… 在操作上，用$()或 都无所谓，只是我”个人”比较喜欢用$()，理由是： 很容易与’ ‘（单引号）搞混乱，尤其对初学者来说。有时在一些奇怪的字形显示中，两种符号是一模一样的（直竖两点）。当然了，有经验的朋友还是一眼就能分辩两者。只是，若能更好的避免混乱，又何乐不为呢？ 在多层次的复合替换中， 须要额外的跳脱（`）处理，而$() 则比较直观。例如：这是错的： 1command1 `command2 `command3` ` 原本的意图是要在command2 command3先将command3提换出来给command2处理，然后再将结果传给command1 command2…来处理。然而，真正的结果在命令行中却是分成了command2与 两段。正确的输入应该如下： 1command1 `command2 \\`command3\\` ` 要不然，换成$()就没问题了： 1command1 $(command2 $(command3)) 只要你喜欢，做多少层的替换都没问题啦 不过，$()并不是没有斃端的… 首先，``基本上可用在全部的unix shell中使用，若写成shell script，其移植性比较高。而$()并不见的每一种shell都能使用，我只能跟你说，若你用bash2的话，肯定没问题… 接下来，再让我们看${}吧…它其实就是用来作变量替换用的啦。一般情况下，$var与${var}并没有啥不一样。但是用${}会比较精确的界定变量名称的范围，比方说： 12$ A=B$ echo $AB 原本是打算先将$A的结果替换出来，然后再补一个B字母于其后，但在命令行上，真正的结果却是只会提换变量名称为AB的值出来…若使用${}就没问题了： 12$ echo ${A}BBB 不过，假如你只看到${}只能用来界定变量名称的话，那你就实在太小看bash了﹗为了完整起见，我这里再用一些例子加以说明${}的一些特异功能。 假设我们定义了一个变量为： 1file=/dir1/dir2/dir3/my.file.txt 我们可以用${}分别替换获得不同的值： 12345678${file#*/} # 拿掉第一条/及其左边的字串：dir1/dir2/dir3/my.file.txt${file##*/} # 拿掉最后一条/及其左边的字串：my.file.txt${file#*.} # 拿掉第一个.及其左边的字串：file.txt${file##*.} # 拿掉最后一个.及其左边的字串：txt${file%/*} # 拿掉最后条/及其右边的字串：/dir1/dir2/dir3${file%%/*} # 拿掉第一条/及其右边的字串：（空值）${file%.*} # 拿掉最后一个.及其右边的字串：/dir1/dir2/dir3/my.file${file%%.*} #拿掉第一个.及其右边的字串：/dir1/dir2/dir3/my 记忆的方法为： 12345# 是去掉左边（在键盘上#在$之左边）% 是去掉右边（在键盘上%在$之右边）单一符号是最小匹配,两个符号是最大匹配。${file:0:5} # 提取最左边的5个字节：/dir1${file:5:5} # 提取第5个字节右边的连续5个字节：/dir2 我们也可以对变量值里的字串作替换： 12${file/dir/path} # 将第一个dir提换为path:/path1/dir2/dir3/my.file.txt${file//dir/path} # 将全部dir提换为path:/path1/path2/path3/my.file.txt 利用${}还可针对不同的变数状态赋值（没设定、空值、非空值） 12345678${file-my.file.txt} # 假如$file没有设定，则使用my.file.txt作传回值。（空值及非空值时不作处理）${file:-my.file.txt} # 假如$file没有设定或为空值，则使用my.file.txt作传回值。（非空值时不作处理）${file+my.file.txt} # 假如$file设为空值或非空值，均使用my.file.txt作传回值。（没设定时不作处理）${file:+my.file.txt} # 若$file为非空值，则使用my.file.txt作传回值。（没设定及空值时不作处理）${file=my.file.txt} # 若$file没设定，则使用my.file.txt作传回值，同时将$file赋值为my.file.txt。（空值及非空值时不作处理）${file:=my.file.txt} # 若$file没设定或为空值，则使用my.file.txt作传回值，同时将$file赋值为my.file.txt。（非空值时不作处理）${file?my.file.txt} # 若$file没设定，则将my.file.txt输出至STDERR。（空值及非空值时不作处理）${file:?my.file.txt} # 若$file没设定或为空值，则将my.file.txt输出至STDERR。（非空值时不作处理） Tips: 以上的理解在于，你一定要分清楚unset与null及non-null这三种赋值状态。一般而言，: 与null有关，若不带 : 的话，null不受影响，若带 : 则连null也受影响。 还有哦，${#var} 可计算出变量值的长度： 1${#file} # 可得到27，因为/dir1/dir2/dir3/my.file.txt刚好是27个字节… 接下来，再为大家介稍一下bash的数组（array）处理方法。 一般而言，A=&quot;a b c def&quot;这样的变量只是将$A替换为一个单一的字串，但是改为A=(a b c def)，则是将$A定义为数组…bash的数组替换方法可参考如下方法: 12345${A[@]}或${A[*]} # 可得到a b c def（全部组数）${A[0]} # 可得到a（第一个组数），${A[1]}则为第二个组数…${#A[@]}或${#A[*]} #可得到4（全部组数数量）${#A[0]} #可得到1（即第一个组数（a）的长度），${#A[3]}可得到3（第四个组数（def）的长度）A[3]=xyz # 则是将第四个组数重新定义为xyz… 能够善用bash的$()与${}可大大提高及简化shell在变量上的处理能力哦 好了，最后为大家介绍$(())的用途吧：它是用来作整数运算的。在bash中，$(())的整数运算符号大致有这些：+ - * /：分别为“加、减、乘、除”。%：余数运算。&amp; | ^！：分别为“AND、OR、XOR、NOT”运算。 例： 1234567$ a=5;b=7;c=2$ echo $((a+b*c))19$ echo $(((a+b)/c))6$ echo $(((a*b)%c))1 在$(())中的变量名称，可于其前面加$符号来替换，也可以不用，如：$(($a + $b * $c))也可得到19的结果 此外，$(())还可作不同进位（如二进位、八进位、十六进位）作运算呢，只是，输出结果皆为十进制而已： 1echo $((16#2a)) # 结果为42（16进位转十进制） 以一个实用的例子来看看吧：假如当前的umask是022，那么新建文件的权限即为： 123$ umask 022echo &quot;obase=8;echo&quot;obase=8;((8#666 &amp; (8#777 ^ 8#$(umask))))&quot; | bc644 事实上，单纯用(())也可重定义变量值，或作testing： a=5；((a++)) #可将$a重定义为6a=5；((a–)) 则为a=4a=5；b=7；((a &lt; b)) 会得到0（true）的返回值。 常见的用于(())的测试符号有如下这些：&lt;：小于&gt;：大于&lt;=：小于或等于&gt;=：大于或等于==：等于!=：不等于 不过，使用(())作整数测试时，请不要跟[ ]的整数测试搞混乱了。（更多的测试我将于第十章为大家介绍）怎样？好玩吧.. okay，这次暂时说这么多… 上面的介绍，并没有详列每一种可用的状态，更多的，就请读者参考手册文件啰… 第9问：@ 与@与* 差在哪？要说$@与$*之前，需得先从shell script的positional parameter谈起… 我们都已经知道变量（variable）是如何定义及替换的，这个不用再多讲了。但是，我们还需要知道有些变量是shell内定的，且其名称是我们不能随意修改的，其中就有positional parameter在内。在shell script中，我们可用0，0，1，2，2，3…这样的变量分别提取命令行中的如下部份： 1script_name parameter1 parameter2 parameter3… 我们很容易就能猜出$0就是代表shell script名称（路径）本身，而$1就是其后的第一个参数，如此类推… 须得留意的是IFS的作用，也就是，若IFS被quoting处理后，那么positional parameter也会改变。如下例： 1my.sh p1 &quot;p2 p3&quot; p4 由于在p2与p3之间的空白键被soft quote所关闭了，因此my.sh中的$2是”p2 p3”而$3则是p4… 还记得前两章我们提到function时，我不是说过它是script中的script吗？ 是的，function一样可以读取自己的（有别于script的）postitional parameter，惟一例外的是$0而已。举例而言：假设my.sh里有一个fucntion叫my_fun，若在script中跑my_fun fp1 fp2 fp3，那么，function内的$0是my.sh，而$1则是fp1而非p1了…不如写个简单的my.sh script看看吧： 12345678910#!/bin/bashmy_fun() {echo '$0 inside function is '$0echo '$1 inside function is '$1echo '$2 inside function is '$2}echo '$0 outside function is '$0echo '$1 outside function is '$1echo '$2 outside function is '$2my_fun fp1 &quot;fp2 fp3&quot; 然后在command line中跑一下script就知道了： 12345678chmod +x my.sh./my.sh p1 &quot;p2 p3&quot;$0 outside function is ./my.sh$1 outside function is p1$2 outside function is p2 p3$0 inside function is ./my.sh$1 inside function is fp1$2 inside function is fp2 fp3 然而，在使用positional parameter的时候，我们要注意一些陷井哦： 10不是替换第10个参数，而是替换第一个参数（10不是替换第10个参数，而是替换第一个参数（1）然后再补一个0于其后! 也就是，my.sh one two three four five six seven eigth nine ten这样的command line，my.sh里的$10不是ten而是one0哦…小心小心!要抓到ten的话，有两种方法： 方法一是使用我们上一章介绍的${ }，也就是用${10}即可。 方法二，就是shift了。用通俗的说法来说，所谓的shift就是取消positional parameter中最左边的参数（$0不受影响）。其预设值为1，也就是shift或shift 1都是取消$1，而原本的$2则变成$1、$3变成$2…若shift 3则是取消前面三个参数，也就是原本的$4将变成$1… 那，亲爱的读者，你说要shift掉多少个参数，才可用1取得1取得{10}呢？ okay，当我们对positional parameter有了基本概念之后，那再让我们看看其他相关变量吧。 首先是$#：它可抓出positional parameter的数量。以前面的my.sh p1 &quot;p2 p3&quot;为例：由于p2与p3之间的IFS是在soft quote中，因此$#可得到2的值。但如果p2与p3没有置于quoting中话，那$#就可得到3的值了。同样的道理在function中也是一样的… 因此，我们常在shell script里用如下方法测试script是否有读进参数： 1[ $# = 0 ] 假如为0，那就表示script没有参数，否则就是有带参数… 接下来就是$@与$*：精确来讲，两者只有在soft quote中才有差异，否则，都表示“全部参数”（$0除外）。举例来说好了：若在command line上跑my.sh p1 &quot;p2 p3&quot; p4的话，不管是$@还是$*，都可得到p1 p2 p3 p4就是了。但是，如果置于soft quote中的话： &quot;$@&quot; 则可得到”p1” “p2 p3” “p4”这三个不同的词段（word） &quot;$*&quot; 则可得到”p1 p2 p3 p4”这一整串单一的词段。 我们可修改一下前面的my.sh，使之内容如下： 123456#!/bin/bashmy_fun() {echo &quot;$#&quot;}echo 'the number of parameter in &quot;$@&quot; is '$(my_fun &quot;$@&quot;)echo 'the number of parameter in &quot;$*&quot; is '$(my_fun &quot;$*&quot;) 然后再执行./my.sh p1 “p2 p3” p4就知道@与@与*差在哪了… 12the number of parameter in &quot;$@&quot; is 3the number of parameter in &quot;$*&quot; is 1 第10问：&amp;&amp; 与 || 差在哪？好不容易，进入两位数的章节了…一路走来，很辛苦吧？也很快乐吧？ 在解答本章题目之前，先让我们了解一个概念：return value! 我们在shell下跑的每一个command或function，在结束的时候都会传回父行程一个值，称为return value。在shell command line中可用$?这个变量得到最&quot;新&quot;的一个return value，也就是刚结束的那个行程传回的值。Return Value（RV）的取值为0-255之间，由程序（或script）的作者自行定议： 若在script里，用exit RV来指定其值，若没指定，在结束时以最后一道命令之RV为值。 若在function里，则用return RV来代替exit RV即可。 Return Value的作用，是用来判断行程的退出状态（exit status），只有两种： 0的话为“真”（true） 非0的话为“假”（false） 举个例子来说明好了：假设当前目录内有一份my.file的文件，而no.file是不存在的： 12345678910$ touch my.file$ ls my.file$ echo $? # first echo0$ ls no.filels: no.file: No such file or directory$ echo $? # second echo1$ echo $? # third echo0 上例的第一个echo是关于ls my.file的RV，可得到0的值，因此为true;第二个echo是关于ls no.file的RV，则得到非0的值，因此为false;第三个echo是关于第二个echo $?的RV，为0的值，因此也为true。 请记住：每一个command在结束时都会送回return value的! 不管你跑甚么样的命令…然而，有一个命令却是”专门”用来测试某一条件而送出return value以供true或false的判断，它就是test命令了! 若你用的是bash，请在command line下打man test或man bash来了解这个test的用法。这是你可用作参考的最精确的文件了，要是听别人说的，仅作参考就好…下面我只简单作一些辅助说明，其余的一律以man为准： 首先，test的表示式我们称为expression，其命令格式有两种： 123test expression or:[ expression ] （请务必注意[ ]之间的空白键﹗） 用哪一种格式没所谓，都是一样的效果。（我个人比较喜欢后者…）其次，bash的test目前支持的测试对像只有三种： string：字串，也就是纯文字。 integer：整数（0或正整数，不含负数或小数点）。 file：文件。 请初学者一定要搞清楚这三者的差异，因为test所用的expression是不一样的。 以A=123这个变量为例： 123[ &quot;$A&quot; = 123 ] # 是字串的测试，以测试$A是否为1、2、3这三个连续的“文字”。[ &quot;A&quot; -eq 123 ] # 是整数的测试，以测试$A是否等于“一百二十三”。[ -e &quot;$A&quot; ] # 是关于文件的测试，以测试123这份“文件”是否存在。 第三，当expression测试为“真”时，test就送回0（true）的return value，否则送出非0（false）。若在expression之前加上一个!（感叹号），则是当expression为“假时”才送出0，否则送出非0。 同时，test也允许多重的复合测试： 12expression1 -a expression2 # 当两个exrepssion都为true，才送出0，否则送出非0。expression1 -o expression2 # 只需其中一个exrepssion为true，就送出0，只有两者都为false才送出非0。 例如： 1[ -d &quot;$file&quot; -a -x &quot;$file&quot; ] 是表示当$file是一个目录、且同时具有x权限时，test才会为true。 第四，在command line中使用test时，请别忘记命令行的“重组”特性，也就是在碰到meta时会先处理meta再重新组建命令行。（这个特性我在第二及第四章都曾反复强调过）比方说，若test碰到变量或命令替换时，若不能满足expression格式时，将会得到语法错误的结果。举例来说好了：关于[ string1 = string2 ]这个test格式，在=号两边必须要有字串，其中包括空（null）字串（可用soft quote或hard quote取得）。 假如$A目前没有定义，或被定议为空字串的话，那如下的写法将会失败： 123$ unset A$ [ $A = abc ][：=：unary operator expected 这是因为命令行碰到这个meta时，会替换这个meta时，会替换A的值，然后再重组命令行，那就变成了：[ = abc ] 如此一来=号左边就没有字串存在了，因此造成test的语法错误﹗但是，下面这个写法则是成立的： 123$ [ &quot;$A&quot; = abc ]$ echo $?1 这是因为在命令行重组后的结果为：[ &quot;&quot; = abc ]。由于=左边我们用soft quote得到一个空字串，而让test语法得以通过…读者诸君请务必留意这些细节哦，因为稍一不慎，将会导至test的结果变了个样! 若您对test还不是很有经验的话，那在使用test时不妨先采用如下这一个”法则”： 假如在test中碰到变量替换，用soft quote是最保险的﹗若你对quoting不熟的话，请重新温习第四章的内容吧… okay，关于更多的test用法，老话一句：请看man page吧！ 虽然洋洋洒洒讲了一大堆，或许你还在嘀咕….那…那个return value有啥用啊？！问得好﹗ 告诉你：return value的作用可大了﹗若你想让你的shell变“聪明”的话，就全靠它了： 有了return value，我们可以让shell跟据不同的状态做不同的时情… 这时候，才让我来揭晓本章的答案吧 &amp;&amp; 与 || 都是用来“组建”多个command line用的： command1 &amp;&amp; command2：其意思是command2只有在RV为0（true）的条件下执行。 command1 || command2：其意思是command2只有在RV为非0（false）的条件下执行。 来，以例子来说好了： 1234567$ A=123$ [ -n &quot;$A&quot; ] &amp;&amp; echo &quot;yes!it's ture.&quot;yes!it's ture.$ unset A$ [ -n &quot;$A&quot; ] &amp;&amp; echo &quot;yes!it's ture.&quot;$ [ -n &quot;$A&quot; ] || echo &quot;no,it's NOT ture.&quot;no,it's NOT ture. （注：[ -n string ]是测试string长度大于0则为true。） 上例的第一个&amp;&amp;命令行之所以会执行其右边的echo命令，是因为上一个test送回了0的RV值﹔但第二次就不会执行，因为test送回非0的结果…同理，||右边的echo会被执行，却正是因为左边的test送回非0所引起的。 事实上，我们在同一命令行中，可用多个&amp;&amp;或||来组建呢： 123456$ A=123$ [ -n &quot;$A&quot; ] &amp;&amp; echo &quot;yes！it's ture.&quot; || echo &quot;no，it's NOT ture.&quot;yes!it's ture.$ unset A$ [ -n &quot;$A&quot; ] &amp;&amp; echo &quot;yes!it's ture.&quot; || echo &quot;no,it's NOT ture.&quot;no,it's NOT ture. 怎样，从这一刻开始，你是否觉得我们的shell是“很聪明”的呢？ 好了，最后，布置一道习题给大家做做看… 下面的判断是：当$A被赋与值时，再看是否小于100，否则送出too big!： 123$ A=123$ [ -n &quot;$A&quot; ] &amp;&amp; [ &quot;$A&quot; -lt 100 ] || echo 'too big!'too big! 若我将A取消，照理说，应该不会送文字才对啊（因为第一个条件就不成立了）… 123$ unset A$ [ -n &quot;$A&quot; ] &amp;&amp; [ &quot;$A&quot; -lt 100 ] || echo 'too big!'too big! 为何上面的结果也可得到呢？又，如何解决之呢？（提示：修改方法很多，其中一种方法可利用第七章介绍过的command group…）快﹗告我我答案﹗其余免谈… 第11问：&gt; 与 &lt; 差在哪？11.1谈到I/O redirection，不妨先让我们认识一下File Descriptor（FD）。程序的运算，在大部份情况下都是进行数据（data）的处理，这些数据从哪读进？又，送出到哪里呢？这就是File descriptor（FD）的功用了。 在shell程序中，最常使用的FD大概有三个，分别为： 0: Standard Input（STDIN） 1: Standard Output（STDOUT） 2: Standard Error Output（STDERR） 在标准情况下，这些FD分别跟如下设备（device）关联： stdin（0）：keyboard stdout（1）：monitor stderr（2）：monitor 我们可以用如下下命令测试一下： 1234$ mail -s test rootthis is a test mail.please skip.^d（同时按crtl跟d键） 很明显，mail程序所读进的数据，就是从stdin也就是keyboard读进的。不过，不见得每个程序的stdin都跟mail一样从keyboard读进，因为程序作者可以从档案参数读进stdin，如： 1$ cat /etc/passwd 但，要是cat之后没有档案参数则又如何呢？哦，请您自己玩玩看啰…. 1$ cat （请留意数据输出到哪里去了，最后别忘了按^d离开…）至于stdout与stderr，嗯…等我有空再续吧… 还是，有哪位前辈要来玩接龙呢？ 11.2沿文再续，书接上一回… 相信，经过上一个练习后，你对stdin与stdout应该不难理解吧？然后，让我们继续看stderr好了。 事实上，stderr没甚么难理解的：说穿了就是“错误信息”要往哪边送而已…比方说，若读进的档案参数是不存在的，那我们在monitor上就看到了： 12$ ls no.such.filels: no.such.file: No such file or directory 若，一个命令同时产生stdout与stderr呢？那还不简单，都送到monitor来就好了： 1234$ touch my.file$ ls my.file no.such.filels: no.such.file: No such file or directorymy.file okay，至此，关于FD及其名称、还有相关联的设备，相信你已经没问题了吧？那好，接下来让我们看看如何改变这些FD的预设数据信道，我们可用&lt;来改变读进的数据信道（stdin），使之从指定的档案读进。我们可用&gt;来改变送出的数据信道（stdout，stderr），使之输出到指定的档案。比方说： 1$ cat &lt; my.file 就是从my.file读进数据 1$ mail -s test root &lt; /etc/passwd 则是从/etc/passwd读进… 这样一来，stdin将不再是从keyboard读进，而是从档案读进了…严格来说，&lt;符号之前需要指定一个FD的（之间不能有空白），但因为0是&lt;的预设值，因此&lt;与0&lt;是一样的!okay，这个好理解吧？ 那，要是用两个&lt;&lt;又是啥呢？这是所谓的HERE Document，它可以让我们输入一段文本，直到读到&lt;&lt;后指定的字串。比方说： 12345$ cat &lt;&lt;FINISHfirst line heresecond line therethird line nowhereFINISH 这样的话，cat会读进3行句子，而无需从keyboard读进数据且要等^d结束输入。至于&gt;又如何呢？且听下回分解… 11.3okay，又到讲古时间~当你搞懂了0&lt;原来就是改变stdin的数据输入信道之后，相信要理解如下两个redirection就不难了：1&gt; 2&gt; 。前者是改变stdout的数据输出信道，后者是改变stderr的数据输出信道。两者都是将原本要送出到monitor的数据转向输出到指定档案去。 由于1是&gt;的预设值，因此，1&gt;与&gt;是相同的，都是改变stdout。用上次的ls例子来说明一下好了： 12$ ls my.file no.such.file 1&gt;file.outls: no.such.file: No such file or directory 这样monitor就只剩下stderr而已。因为stdout给写进file.out去了。 12$ ls my.file no.such.file 2&gt;file.errmy.file 这样monitor就只剩下stdout，因为stderr写进了file.err。 1$ ls my.file no.such.file 1&gt;file.out 2&gt;file.err 这样monitor就啥也没有，因为stdout与stderr都给转到档案去了… 呵~看来要理解&gt;一点也不难啦﹗是不？没骗你吧？ 不过，有些地方还是要注意一下的。首先，是同时写入的问题。比方如下这个例子： 1$ ls my.file no.such.file 1&gt;file.both 2&gt;file.both 假如stdout（1）与stderr（2）都同时在写入file.both的话，则是采取“覆盖”方式：后来写入的覆盖前面的。让我们假设一个stdout与stderr同时写入file.out的情形好了： 首先stdout写入10个字元 然后stderr写入6个字元 那么，这时候原本stdout输出的10个字元就被stderr覆盖掉了。那，如何解决呢？所谓山不转路转、路不转人转嘛，我们可以换一个思维：将stderr导进stdout或将stdout导进sterr，而不是大家在抢同一份档案，不就行了﹗bingo﹗就是这样啦： 2&gt;&amp;1就是将stderr并进stdout作输出 1&gt;&amp;2或&gt;&amp;2就是将stdout并进stderr作输出 于是，前面的错误操作可以改为： 123$ ls my.file no.such.file 1&gt;file.both 2&gt;&amp;1或$ ls my.file no.such.file 2&gt;file.both &gt;&amp;2 这样，不就皆大欢喜了吗？呵 不过，光解决了同时写入的问题还不够，我们还有其他技巧需要了解的。故事还没结束，别走开﹗广告后，我们再回来…﹗ 11.4okay，这次不讲I/O Redirction，讲佛吧…（有没搞错？﹗网中人是否头壳烧坏了？…）嘻 学佛的最高境界，就是“四大皆空”。至于是空哪四大块？我也不知，因为我还没到那境界…但这个“空”字，却非常值得我们返复把玩的：—色即是空、空即是色﹗好了，施主要是能够领会“空”的禅意，那离修成正果不远矣~ 在Linux档案系统里，有个设备档位于/dev/null。许多人都问过我那是甚么玩意儿？我跟你说好了：那就是“空”啦﹗没错﹗空空如也的空就是null了….请问施主是否忽然有所顿误了呢？然则恭喜了 这个null在I/O Redirection中可有用得很呢： 若将FD1跟FD2转到/dev/null去，就可将stdout与stderr弄不见掉。 若将FD0接到/dev/null来，那就是读进nothing。 比方说，当我们在执行一个程序时，画面会同时送出stdout跟stderr，假如你不想看到stderr（也不想存到档案去），那可以： 12$ ls my.file no.such.file 2&gt;/dev/nullmy.file 若要相反：只想看到stderr呢？还不简单﹗将stdout弄到null就行： 12$ ls my.file no.such.file &gt;/dev/nullls: no.such.file: No such file or directory 那接下来，假如单纯只跑程序，不想看到任何输出结果呢？哦，这里留了一手上次节目没讲的法子，专门赠予有缘人﹗… 除了用&gt;/dev/null 2&gt;&amp;1之外，你还可以如此： 1$ ls my.file no.such.file &amp;&gt;/dev/null （提示：将&amp;&gt;换成&gt;&amp;也行啦~！） okay？讲完佛，接下来，再让我们看看如下情况： 123456$ echo &quot;1&quot; &gt; file.out$ cat file.out1$ echo &quot;2&quot; &gt; file.out$ cat file.out2 看来，我们在重导stdout或stderr进一份档案时，似乎永远只获得最后一次导入的结果。那，之前的内容呢？呵要解决这个问提很简单啦，将&gt;换成&gt;&gt;就好： 1234$ echo &quot;3&quot; &gt;&gt; file.out$ cat file.out23 如此一来，被重导的目标档案之内容并不会失去，而新的内容则一直增加在最后面去。easy？呵… 但，只要你再一次用回单一的&gt;来重导的话，那么，旧的内容还是会被“洗”掉的﹗这时，你要如何避免呢？—-备份﹗yes，我听到了﹗不过….还有更好的吗？既然与施主这么有缘份，老纳就送你一个锦囊妙法吧： 123$ set -o noclobber$ echo &quot;4&quot; &gt; file.out-bash: file: cannot overwrite existing file 那，要如何取消这个“限制”呢？哦，将set -o换成set +o就行： 1234$ set +o noclobber$ echo &quot;5&quot; &gt; file.out$ cat file.out5 再问：那…有办法不取消而又“临时”盖写目标档案吗？哦，佛曰：不可告也﹗啊开玩笑的、开玩笑的啦唉，早就料到人心是不足的了﹗ 1234$ set -o noclobber$ echo &quot;6&quot; &gt;| file.out$ cat file.out6 留意到没有：在&gt;后面再加个“|”就好（注意：&gt;与|之间不能有空白哦）… 呼…（深呼吸吐纳一下吧）再来还有一个难题要你去参透的呢： 12345678$ echo &quot;some text here&quot; &gt; file$ cat &lt; filesome text here$ cat &lt; file &gt; file.bak$ cat &lt; file.baksome text here$ cat &lt; file &gt; file$ cat &lt; file 嗯？！注意到没有？！！—-怎么最后那个cat命令看到的file竟是空的？﹗why？why？why？同学们：下节课不要迟到啰！ 11.5当当当上课啰前面提到：$ cat &lt; file &gt; file 之后原本有内容的档案结果却被洗掉了﹗要理解这一现像其实不难，这只是priority的问题而已： 在IO Redirection中，stdout与stderr的管道会先准备好，才会从stdin读进数据。也就是说，在上例中，&gt; file会先将file清空，然后才读进&lt; file，但这时候档案已经被清空了，因此就变成读不进任何数据了…哦原来如此~ 那…如下两例又如何呢？ 12$ cat &lt;&gt; file$ cat &lt; file &gt;&gt; file 嗯…同学们，这两个答案就当练习题啰，下节课之前请交作业﹗好了，I/O Redirection也快讲完了，sorry，因为我也只知道这么多而已啦嘻~ 不过，还有一样东东是一定要讲的，各位观众（请自行配乐~！#@！$%）：—-就是pipe line也！ 谈到pipe line，我相信不少人都不会陌生：我们在很多command line上常看到的“|”符号就是pipe line了。不过，究竟pipe line是甚么东东呢？别急别急…先查一下英汉字典，看看pipe是甚么意思？没错﹗它就是“水管”的意思…那么，你能想像一下水管是怎么一根接着一根的吗？又，每根水管之间的input跟output又如何呢？嗯？？灵光一闪：原来pipe line的I/O跟水管的I/O是一模一样的：上一个命令的stdout接到下一个命令的stdin去了!的确如此…不管在command line上你使用了多少个pipe line，前后两个command的I/O都是彼此连接的﹗（恭喜：你终于开窍了﹗） 不过…然而…但是……stderr呢？好问题﹗不过也容易理解：若水管漏水怎么办？也就是说：在pipe line之间，前一个命令的stderr是不会接进下一命令的stdin的，其输出，若不用2&gt;导到file去的话，它还是送到摄像头上面来﹗这点请你在pipe line运用上务必要注意的。那，或许你又会问：有办法将stderr也喂进下一个命令的stdin去吗？（贪得无厌的家伙﹗）方法当然是有，而且你早已学过了﹗ 我提示一下就好：请问你如何将stderr合并进stdout一同输出呢？若你答不出来，下课之后再来问我吧…（如果你脸皮真够厚的话…） 或许，你仍意尤未尽﹗或许，你曾经碰到过下面的问题： 在cm1 | cm2 | cm3…这段pipe line中，若要将cm2的结果存到某一档案呢？ 若你写成cm1 | cm2 &gt; file | cm3的话，那你肯定会发现cm3的stdin是空的﹗（当然啦，你都将水管接到别的水池了﹗）聪明的你或许会如此解决：cm1 | cm2 &gt; file; cm3 &lt; file 是的，你的确可以这样做，但最大的坏处是：这样一来，file I/O会变双倍﹗在command执行的整个过程中，file I/O是最常见的最大性能杀手。凡是有经验的shell操作者，都会尽量避免或降低file I/O的频率。那，上面问题还有更好方法吗？有的，那就是tee命令了。 所谓tee命令是在不影响原本I/O的情况下，将stdout复制一份到档案去。因此，上面的命令行可以如此打： 1cm1 | cm2 | tee file | cm3 在预设上，tee会改写目标档案，若你要改为增加内容的话，那可用-a参数达成。基本上，pipe line的应用在shell操作上是非常广泛的，尤其是在text filtering方面，凡举cat，more，head，tail，wc，expand，tr，grep，sed，awk，…等等文字处理工具，搭配起pipe line来使用，你会惊觉command line原来是活得如此精彩的﹗常让人有“众里寻他千百度，蓦然回首，那人却在灯火阑珊处﹗”之感… 好了，关于I/O Redirection的介绍就到此告一段落。若日后有空的话，再为大家介绍其它在shell上好玩的东西﹗bye… 第12问：你要 if 还是 case 呢？放了一个愉快的春节假期，人也变得懒懒散散的…只是，答应了大家的作业，还是要坚持完成就是了~ 还记得我们在第10章所介绍的return value吗？是的，接下来介绍的内容与之有关，若你的记忆也被假期的欢乐时光所抵消掉的话，那，建议您还是先回去温习温习再回来… 若你记得return value，我想你也应该记得了&amp;&amp;与||是甚么意思吧？用这两个符号再配搭command group的话，我们可让shell script变得更加聪明哦。比方说： 12345678comd1 &amp;&amp; { comd2 comd3 :} || { comd4 comd5} 意思是说：假如comd1的return value为true的话，然则执行comd2与comd3，否则执行comd4与comd5。 事实上，我们在写shell script的时候，经常需要用到这样那样的条件以作出不同的处理动作。用&amp;&amp;与||的确可以达成条件执行的效果，然而，从“人类语言”上来理解，却不是那么直观。更多时候，我们还是喜欢用if….then…else…这样的keyword来表达条件执行。在bash shell中，我们可以如此修改上一段代码： 12345678if comd1then comd2 comd3else comd4 comd5fi 这也是我们在shell script中最常用到的if判断式：只要if后面的command line返回true的return value（我们最常用test命令来送出return value），然则就执行then后面的命令，否则执行else后的命令;fi则是用来结束判断式的keyword。 在if判断式中，else部份可以不用，但then是必需的。（若then后不想跑任何command，可用:这个null command代替）。当然，then或else后面，也可以再使用更进一层的条件判断式，这在shell script设计上很常见。 若有多项条件需要“依序”进行判断的话，那我们则可使用elif这样的keyword： 1234567if comd1; then comd2elif comd3; then comd4else comd5fi 意思是说：若comd1为true，然则执行comd2;否则再测试comd3，然则执行comd4;倘若comd1与comd3均不成立，那就执行comd5。 if判断式的例子很常见，你可从很多shell script中看得到，我这里就不再举例子了…接下来要为大家介绍的是case判断式。 虽然if判断式已可应付大部份的条件执行了，然而，在某些场合中，却不够灵活，尤其是在string式样的判断上，比方如下： 1234567891011QQ() { echo -n &quot;Do you want to continue?(Yes/No):&quot; read YN if [ &quot;$YN&quot; = Y -o &quot;$YN&quot; = y -o &quot;$YN&quot; = &quot;Yes&quot; -o &quot;$YN&quot; = &quot;yes&quot; -o &quot;$YN&quot; = &quot;YES&quot; ] then QQ else exit 0 fi}QQ 从例中，我们看得出来，最麻烦的部份是在于判断YN的值可能有好几种式样。聪明的你或许会如此修改： 1if echo &quot;$YN&quot; | grep -q '^[Yy]\\([Ee][Ss]\\)*$' 也就是用Regular Expression来简化代码。（我们有机会再来介绍RE）只是…是否有其它更方便的方法呢?有的，就是用case判断式即可： 12345678910111213QQ() { echo -n &quot;Do you want to continue?(Yes/No):&quot; read YN case &quot;$YN&quot; in [Yy]|[Yy][Ee][Ss]) QQ ;; *) exit 0 ;; esac}QQ 我们常用case的判断式来判断某一变量在不同的值（通常是string）时作出不同的处理，比方说，判断script参数以执行不同的命令。若你有兴趣、且用Linux系统的话，不妨挖一挖/etc/init.d/*里那堆script中的case用法。如下就是一例： 1234567891011121314151617181920case &quot;$1&quot; in start) start ;; stop) stop ;; status) rhstatus ;; restart|reload) restart ;; condrestart) [ -f /var/lock/subsys/syslog ] &amp;&amp; restart || : ;; *) echo $&quot;Usage: $0 {start|stop|status|restart|condrestart}&quot; exit 1 esac （若你对positional parameter的印像已经模糊了，请重看第9章吧。）okay，十三问还剩一问而已，过几天再来搞定之…. 第13问：for what？while与until差在哪？终于，来到shell十三问的最后一问了…长长吐一口气~ 最后要介绍的是shell script设计中常见的“循环”（loop）。所谓的loop就是script中的一段在一定条件下反复执行的代码。bash shell中常用的loop有如下三种：for while until for loop是从一个清单列表中读进变量值，并“依次”的循环执行do到done之间的命令行。例： 123456for var in one two three four fivedo echo ----------- echo '$var is '$var echodone 上例的执行结果将会是： for会定义一个叫var的变量，其值依次是one two three four five。 因为有5个变量值，因此do与done之间的命令行会被循环执行5次。 每次循环均用echo产生三行句子。而第二行中不在hard quote之内的$var会依次被替换为one two three four five。 当最后一个变量值处理完毕，循环结束。 我们不难看出，在for loop中，变量值的多寡，决定循环的次数。然而，变量在循环中是否使用则不一定，得视设计需求而定。倘若for loop没有使用in这个keyword来指定变量值清单的话，其值将从$@（或$*）中继承： 123for var; do…done （若你忘记了positional parameter，请温习第9章…） for loop用于处理“清单”（list）项目非常方便，其清单除了可明确指定或从positional parameter取得之外，也可从变量替换或命令替换取得…（再一次提醒：别忘了命令行的“重组”特性！） 然而，对于一些“累计变化”的项目（如整数加减），for亦能处理： 1234for ((i=1;i&lt;=10;i++))do echo &quot;num is $i&quot;done 除了for loop，上面的例子我们也可改用while loop来做到： 12345num=1while [ &quot;$num&quot; -le 10 ]; do echo &quot;num is $num&quot; num=$(($num + 1))done while loop的原理与for loop稍有不同：它不是逐次处理清单中的变量值，而是取决于while后面的命令行之return value： 若为ture，则执行do与done之间的命令，然后重新判断while后的return value。 若为false，则不再执行do与done之间的命令而结束循环。 分析上例： 在while之前，定义变量num=1。 然后测试（test）$num是否小于或等于10。 结果为true，于是执行echo并将num的值加一。 再作第二轮测试，此时num的值为1+1=2，依然小于或等于10，因此为true，继续循环。 直到num为10+1=11时，测试才会失败…于是结束循环。 我们不难发现：若while的测试结果永远为true的话，那循环将一直永久执行下去： 123while : ;do echo looping…done 上例的:是bash的null command，不做任何动作，除了送回true的return value。因此这个循环不会结束，称作死循环。死循环的产生有可能是故意设计的（如跑daemon），也可能是设计错误。若要结束死循环，可透过signal来终止（如按下ctrl-c）。（关于process与signal，等日后有机会再补充，十三问暂时略过。） 一旦你能够理解while loop的话，那，就能理解until loop： 与while相反，until是在return value为false时进入循环，否则结束。 因此，前面的例子我们也可以轻松的用until来写： 12345num=1until [ ! &quot;$num&quot; -le 10 ]; do echo &quot;num is $num&quot; num=$(($num + 1))done 或是： 12345num=1until [ &quot;$num&quot; -gt 10 ]; do echo &quot;num is $num&quot; num=$(($num + 1))done okay，关于bash的三个常用的loop暂时介绍到这里。在结束本章之前，再跟大家补充两个与loop有关的命令：break continue 这两个命令常用在复合式循环里，也就是在do…done之间又有更进一层的loop，当然，用在单一循环中也未尝不可啦… break是用来打断循环，也就是“强迫结束”循环。若break后面指定一个数值n的话，则“从里向外”打断第n个循环，预设值为break 1，也就是打断当前的循环。在使用break时需要注意的是，它与return及exit是不同的： break是结束loop return是结束function exit是结束script/shell 而continue则与break相反：强迫进入下一次循环动作。若你理解不来的话，那你可简单的看成：在continue到done之间的句子略过而返回循环顶端…与break相同的是：continue后面也可指定一个数值n，以决定继续哪一层（从里向外计算）的循环，预设值为continue 1，也就是继续当前的循环。在shell script设计中，若能善用loop，将能大幅度提高script在复杂条件下的处理能力。请多加练习吧…. 好了，该是到了结束的时候了。 婆婆妈妈的跟大家罗唆了一堆关于shell的基础概念，目的不是要告诉大家“答案”，而是要带给大家“启发”…在日后关于shell的讨论中，我或许会经常用“连接”方式指引回来十三问中的内容，以便我们在进行技术探讨时彼此能有一些讨论基础，而不至于各说各话、徒费时力。但，更希望十三问能带给你更多的思考与乐趣，至为重要的是透过实作来加深理解。 是的，我很重视“实作”与“独立思考”这两项学习要素，若你能够掌握其中真义，那请容我说声：—恭喜﹗十三问你没白看了﹗ p.s. 至于补充问题部份，我暂时不写了。而是希望： 大家扩充题目。 一起来写心得。 Good luck and happy studying！ 本文整理并转自CU上的帖子[学习共享] shell 十三問?，此贴是2003年发表的，但却是相当不错的linux基础知识汇集贴，原帖主使用的台湾风格，本文加以简体化和整理，转载自P_Chou水冗","link":"/shell-13-question/"},{"title":"CentOS rpm安装Oracle","text":"CentOS下使用rpm安装Oracle19C 下载下载database到oracle官网下载地址下载最新的oracle，选择Linux x86-64中的RPM，点击下载（需登录），浏览器跳出下载后暂定下载，复制下载链接。 在服务器上下载 1wget https://download.oracle.com/otn/linux/oracle19c/190000/oracle-database-ee-19c-1.0-1.x86_64.rpm?AuthParam=xxxx 上面的AuthParam需要更换为自己的 下载preinstall到下载地址搜索preinstall,下载最新版 1wget https://yum.oracle.com/repo/OracleLinux/OL7/latest/x86_64/getPackage/oracle-database-preinstall-19c-1.0-3.el7.x86_64.rpm 安装关闭防火墙和SELINUX1sed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config &amp; setenforce 0&amp;&amp; systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service 1234567[root@localhost src]# cat /etc/security/limits.d/20-nproc.conf# Default limit for number of user's processes to prevent# accidental fork bombs.# See rhbz #432903 for reasoning.* soft nproc 4096root soft nproc unlimited 安装preinstall1yum localinstall -y oracle-database-preinstall-19c-1.0-3.el7.x86_64.rpm 安装database1yum localinstall -y oracle-database-ee-19c-1.0-1.x86_64.rpm 查看实例端口和路径1234567891011121314[root@localhost src]# cat /etc/sysconfig/oracledb_ORCLCDB-19c.conf#This is a configuration file to setup the Oracle Database.#It is used when running '/etc/init.d/oracledb_ORCLCDB configure'.#Please use this file to modify the default listener port and the#Oracle data location.# LISTENER_PORT: Database listenerLISTENER_PORT=1521# ORACLE_DATA_LOCATION: Database oradata locationORACLE_DATA_LOCATION=/opt/oracle/oradata# EM_EXPRESS_PORT: Oracle EM Express listenerEM_EXPRESS_PORT=5500 配置创建数据库实例1/etc/init.d/oracledb_ORCLCDB-19c configure 过程很慢，静心等待 配置环境变量1vim /home/oracle/.bash_profile 添加如下内容： 123456export ORACLE_BASE=/opt/oracleexport ORACLE_HOME=/opt/oracle/product/19c/dbhome_1export ORACLE_SID=ORCLCDBexport PATH=$ORACLE_HOME/bin:$PATH:$HOME/.local/bin:$HOME/binexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/libexport NLS_LANG=AMERICAN_AMERICA.UTF8 使修改立即生效： 1source /home/oracle/.bash_profile 修改oracle密码1passwd oracle 登录数据库12su - oraclesqlplus / as sysdba sys默认密码:change_on_install使用SQL Plus登录数据库时，system使用密码manager可直接登录。但如果是sys用户，密码必须加上as sysdba，即完整密码为：change_on_install as sysdba 默认服务名：ORCLCDB 修改密码1SQL&gt; password system 监听123lsnrctl statuslsnrctl stoplsnrctl start 使用创建用户创建用户名为devops和密码为devops123的用户; 1grant create user,drop user,alter user,create any view,connect,resource,dba,create session,create any sequence to devops123; 修改密码： 1alter user devops identified by 123456; 授权1grant create user,drop user,alter user,create any view,connect,resource,dba,create session,create any sequence to c#devops ;","link":"/oracle-install/"},{"title":"内网穿透工具nps使用","text":"内网穿透工具有很多，相比frp,nps页面更友好，本文介绍nps的安装和简单使用。 准备工作 一台有公网IP的服务器，假设公网ip是1.1.1.1，系统为CentOS。 一台可连外网的内网服务器，假设内网IP是192.168.0.127，系统为CentOS。 本机，假设是一台Mac。 服务端请从github上下载最新版本。当前最新版本v0.26.10。 mac下载darwin开头的版本 在公网服务器上 下载123456cd /usr/local/src/wget https://github.com/ehang-io/nps/releases/download/v0.26.10/linux_amd64_server.tar.gzmkidr npsmv linux_amd64_server.tar.gz nps/cd npstar -zxvf linux_amd64_server.tar.gz 安装执行安装命令 1sudo ./nps install 对于windows，管理员身份运行cmd，进入安装目录 nps.exe install 安装后windows配置文件位于 C:\\Program Files\\nps，linux和darwin位于/etc/nps 修改配置1vim /etc/nps/conf/nps.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485appname = nps#Boot mode(dev|pro)runmode = pro(修改)#HTTP(S) proxy port, no startup if emptyhttp_proxy_ip=0.0.0.0http_proxy_port=https_proxy_port=https_just_proxy=true#default https certificate settinghttps_default_cert_file=conf/server.pemhttps_default_key_file=conf/server.key##bridgebridge_type=tcpbridge_port=8024bridge_ip=0.0.0.0# Public password, which clients can use to connect to the server# After the connection, the server will be able to open relevant ports and parse related domain names according to its own configuration file.public_vkey=test(修改)#Traffic data persistence interval(minute)#Ignorance means no persistence#flow_store_interval=1# log level LevelEmergency-&gt;0 LevelAlert-&gt;1 LevelCritical-&gt;2 LevelError-&gt;3 LevelWarning-&gt;4 LevelNotice-&gt;5 LevelInformational-&gt;6 LevelDebug-&gt;7log_level=7#log_path=nps.log#Whether to restrict IP access, true or false or ignore#ip_limit=true#p2p#p2p_ip=127.0.0.1#p2p_port=6000#webweb_host=1.1.1.1(公网IP)web_username=admin(建议修改)web_password=admin123456(建议修改)web_port = (建议修改)web_ip=0.0.0.0web_base_url=web_open_ssl=falseweb_cert_file=conf/server.pemweb_key_file=conf/server.key# if web under proxy use sub path. like http://host/nps need this.#web_base_url=/nps#Web API unauthenticated IP address(the len of auth_crypt_key must be 16)#Remove comments if needed#auth_key=testauth_crypt_key =1234567812345678(需修改)#allow_ports=9001-9009,10001,11000-12000#Web management multi-user loginallow_user_login=falseallow_user_register=falseallow_user_change_username=false#extensionallow_flow_limit=falseallow_rate_limit=falseallow_tunnel_num_limit=falseallow_local_proxy=falseallow_connection_num_limit=falseallow_multi_ip=falsesystem_info_display=false#cachehttp_cache=falsehttp_cache_length=100#get origin iphttp_add_origin_header=false#pprof debug options#pprof_ip=0.0.0.0#pprof_port=9999#client disconnect timeoutdisconnect_timeout=60 根据实际情况修改，通常需要修改的选项已在上面标注。 运行1nps start 对于windows，管理员身份运行cmd，进入程序目录 nps.exe start 停止和重启可用，stop和restart 访问服务端并添加客户端防火墙或云服务器开放外网端口8089和8024(根据安全性适当调整开放力度) 访问地址 1.1.1.1:8089,填写账号密码admin和admin123456 点击左侧客户端，新增，填写对应信息(唯一验证密钥填写test)后确定，即可添加客户端，添加成功后如下，点击左侧➕，展开可查看客户端命令，复制客户端命令： 客户端在内网服务器上下载安装最新客户端 123456cd /usr/local/src/wget https://github.com/ehang-io/nps/releases/download/v0.26.10/linux_amd64_client.tar.gzmkidr npcmv linux_amd64_client.tar.gz npc/cd npctar -zxvf linux_amd64_client.tar.gz 运行上面复制好的客户端命令： 1./npc -server=1.1.1.1:8024 -vkey=test -type=tcp 使用通过私密代理从本机访问连接内网机器SSH私密代理相比tcp隧道代理优点是更安全且无需占用多余端口，缺点是本机需要安装客户端。 本机安装客户端，安装步骤同上 在web管理页面，点击左侧私密代理，点击新增。填写客户端id，目标ip填写192.168.0.127:22，唯一标识密钥填写test123 添加成功后在私密代理列表页，点击ID前的➕，展开，复制访问端命令 在本机上粘贴并执行访问端命令./npc -server=1.1.1.1:8024 -vkey=test -type=tcp -password=test123 -local_type=secret 本机上执行ssh -p 2000 root@127.0.0.1，如需指定本地端口可加参数-local_port=xx，默认为2000 通过TCP隧道代理SSH通过外网ip代理8027端口到内网。需要在防火墙或云服务器开放8027端口。 在web管理页面，点击左侧TCP隧道，点击新增，服务端端口填写8027，目标ip填写192.168.0.127:22 本机上执行 ssh -p 8027 root@1.1.1.1 代理HTTP访问外网8028端口，代理到内网80端口。需要在防火墙或云服务器开放8028端口。 在web管理页面，点击左侧TCP隧道，点击新增，服务端端口填写8028，目标ip填写192.168.0.127:80 访问 http://1.1.1.1:8028 即会自动代理到内网 http://192.168.0.127:80","link":"/nps/"},{"title":"实战 SSH 端口转发","text":"当你在咖啡馆享受免费 WiFi 的时候，有没有想到可能有人正在窃取你的密码及隐私信息？当你发现实验室的防火墙阻止了你的网络应用端口，是不是有苦难言？来看看 SSH 的端口转发功能能给我们带来什么好处吧！ 第一部分 概述端口转发概述让我们先来了解一下端口转发的概念吧。我们知道，SSH 会自动加密和解密所有 SSH 客户端与服务端之间的网络数据。但是，SSH 还同时提供了一个非常有用的功能，这就是端口转发。它能够将其他 TCP 端口的网络数据通过 SSH 链接来转发，并且自动提供了相应的加密及解密服务。这一过程有时也被叫做“隧道”（tunneling），这是因为 SSH 为其他 TCP 链接提供了一个安全的通道来进行传输而得名。例如，Telnet，SMTP，LDAP 这些 TCP 应用均能够从中得益，避免了用户名，密码以及隐私信息的明文传输。而与此同时，如果您工作环境中的防火墙限制了一些网络端口的使用，但是允许 SSH 的连接，那么也是能够通过将 TCP 端口转发来使用 SSH 进行通讯。总的来说 SSH 端口转发能够提供两大功能： 加密 SSH Client 端至 SSH Server 端之间的通讯数据。 突破防火墙的限制完成一些之前无法建立的 TCP 连接。 图 1. SSH 端口转发 如上图所示，使用了端口转发之后，TCP 端口 A 与 B 之间现在并不直接通讯，而是转发到了 SSH 客户端及服务端来通讯，从而自动实现了数据加密并同时绕过了防火墙的限制。 第二部分 本地转发与远程转发本地转发实例分析我们先来看第一个例子，在实验室里有一台 LDAP 服务器（LdapServerHost），但是限制了只有本机上部署的应用才能直接连接此 LDAP 服务器。如果我们由于调试或者测试的需要想临时从远程机器（LdapClientHost）直接连接到这个 LDAP 服务器 , 有什么方法能够实现呢？ 答案无疑是本地端口转发了，它的命令格式是： 1ssh -L &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; 在 LdapClientHost 上执行如下命令即可建立一个 SSH 的本地端口转发，例如： 1$ ssh -L 7001:localhost:389 LdapServerHost 图 2. 本地端口转发 这里需要注意的是本例中我们选择了 7001 端口作为本地的监听端口，在选择端口号时要注意非管理员帐号是无权绑定 1-1023 端口的，所以一般是选用一个 1024-65535 之间的并且尚未使用的端口号即可。 然后我们可以将远程机器（LdapClientHost）上的应用直接配置到本机的 7001 端口上（而不是 LDAP 服务器的 389 端口上）。之后的数据流将会是下面这个样子： 我们在 LdapClientHost 上的应用将数据发送到本机的 7001 端口上， 而本机的 SSH Client 会将 7001 端口收到的数据加密并转发到 LdapServertHost 的 SSH Server 上。 SSH Server 会解密收到的数据并将之转发到监听的 LDAP 389 端口上， 最后再将从 LDAP 返回的数据原路返回以完成整个流程。 我们可以看到，这整个流程应用并没有直接连接 LDAP 服务器，而是连接到了本地的一个监听端口，但是 SSH 端口转发完成了剩下的所有事情，加密，转发，解密，通讯。 这里有几个地方需要注意： SSH 端口转发是通过 SSH 连接建立起来的，我们必须保持这个 SSH 连接以使端口转发保持生效。一旦关闭了此连接，相应的端口转发也会随之关闭。 我们只能在建立 SSH 连接的同时创建端口转发，而不能给一个已经存在的 SSH 连接增加端口转发。 你可能会疑惑上面命令中的 为什么用 localhost，它指向的是哪台机器呢？在本例中，它指向 LdapServertHost 。我们为什么用 localhost 而不是 IP 地址或者主机名呢？其实这个取决于我们之前是如何限制 LDAP 只有本机才能访问。如果只允许 lookback 接口访问的话，那么自然就只有 localhost 或者 IP 为 127.0.0.1 才能访问了，而不能用真实 IP 或者主机名。 命令中的 和 必须是同一台机器么？其实是不一定的，它们可以是两台不同的机器。我们在后面的例子里会详细阐述这点。 好了，我们已经在 LdapClientHost 建立了端口转发，那么这个端口转发可以被其他机器使用么？比如能否新增加一台 LdapClientHost2 来直接连接 LdapClientHost 的 7001 端口？答案是不行的，在主流 SSH 实现中，本地端口转发绑定的是 lookback 接口，这意味着只有 localhost 或者 127.0.0.1 才能使用本机的端口转发 , 其他机器发起的连接只会得到“ connection refused. ”。好在 SSH 同时提供了 GatewayPorts 关键字，我们可以通过指定它与其他机器共享这个本地端口转发。ssh -g -L &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; 远程转发实例分析我们来看第二个例子，这次假设由于网络或防火墙的原因我们不能用 SSH 直接从 LdapClientHost 连接到 LDAP 服务器（LdapServertHost），但是反向连接却是被允许的。那此时我们的选择自然就是远程端口转发了。 它的命令格式是： 1ssh -R &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; 例如在 LDAP 服务器（LdapServertHost）端执行如下命令： 1$ ssh -R 7001:localhost:389 LdapClientHost 图 3. 远程端口转发 和本地端口转发相比，这次的图里，SSH Server 和 SSH Client 的位置对调了一下，但是数据流依然是一样的。我们在 LdapClientHost 上的应用将数据发送到本机的 7001 端口上，而本机的 SSH Server 会将 7001 端口收到的数据加密并转发到 LdapServertHost 的 SSH Client 上。 SSH Client 会解密收到的数据并将之转发到监听的 LDAP 389 端口上，最后再将从 LDAP 返回的数据原路返回以完成整个流程。 看到这里，你是不是会有点糊涂了么？为什么叫本地转发，而有时又叫远程转发？这两者有什么区别？ 本地转发与远程转发的对比与分析不错，SSH Server，SSH Client，LdapServertHost，LdapClientHost，本地转发，远程转发，这么多的名词的确容易让人糊涂。让我们来分析一下其中的结构吧。首先，SSH 端口转发自然需要 SSH 连接，而 SSH 连接是有方向的，从 SSH Client 到 SSH Server 。而我们的应用也是有方向的，比如需要连接 LDAP Server 时，LDAP Server 自然就是 Server 端，我们应用连接的方向也是从应用的 Client 端连接到应用的 Server 端。如果这两个连接的方向一致，那我们就说它是本地转发。而如果两个方向不一致，我们就说它是远程转发。 我们可以回忆上面的两个例子来做个对照。 本地转发时： LdapClientHost 同时是应用的客户端，也是 SSH Client，这两个连接都从它指向 LdapServertHost（既是 LDAP 服务端，也是 SSH Server）。 远程转发时： LdapClientHost 是应用的客户端，但却是 SSH Server ；而 LdapServertHost 是 LDAP 的服务端，但却是 SSH Client 。这样两个连接的方向刚好相反。 另一个方便记忆的方法是，Server 端的端口都是预定义的固定端口（SSH Server 的端口 22，LDAP 的端口 389），而 Client 端的端口都是动态可供我们选择的端口（如上述例子中选用的 7001 端口）。如果 Server 端的两个端口都在同一台机器，Client 端的两个端口都在另一台机器上，那么这就是本地连接；如果这四个端口交叉分布在两个机器上，每台机器各有一个 Server 端端口，一个 Client 端端口，那就是远程连接。 弄清楚了两者的区别之后，再来看看两者的相同之处。如果你所在的环境下，既允许 LdapClientHost 发起 SSH 连接到 LdapServerHost，也允许 LdapServerHost 发起 SSH 连接到 LdapClientHost 。那么这时我们选择本地转发或远程转发都是可以的，能完成一样的功能。 接着让我们来看个进阶版的端口转发。我们之前涉及到的各种连接 / 转发都只涉及到了两台机器，还记得我们在本地转发中提到的一个问题么？本地转发命令中的 和 可以是不同的机器么？ 1ssh -L &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; 答案是可以的！让我们来看一个涉及到四台机器 (A,B,C,D) 的例子。 图 4. 多主机转发应用 在 SSH Client(C) 执行下列命令来建立 SSH 连接以及端口转发： 1$ ssh -g -L 7001:&lt;B&gt;:389 &lt;D&gt; 然后在我们的应用客户端（A）上配置连接机器（C ）的 7001 端口即可。注意我们在命令中指定了“ -g ”参数以保证机器（A）能够使用机器（C）建立的本地端口转发。而另一个值得注意的地方是，在上述连接中，（A）&lt;-&gt; (C) 以及 (B)&lt;-&gt;(D) 之间的连接并不是安全连接，它们之间没有经过 SSH 的加密及解密。如果他们之间的网络并不是值得信赖的网络连接，我们就需要谨慎使用这种连接方式了。 第三部分 其他类型的转发动态转发实例分析恩，动态转发，听上去很酷。当你看到这里时，有没有想过我们已经讨论过了本地转发，远程转发，但是前提都是要求有一个固定的应用服务端的端口号，例如前面例子中的 LDAP 服务端的 389 端口。那如果没有这个端口号怎么办？等等，什么样的应用会没有这个端口号呢？嗯，比如说用浏览器进行 Web 浏览，比如说 MSN 等等。 当我们在一个不安全的 WiFi 环境下上网，用 SSH 动态转发来保护我们的网页浏览及 MSN 信息无疑是十分必要的。让我们先来看一下动态转发的命令格式： 1$ ssh -D &lt;local port&gt; &lt;SSH Server&gt; 例如： 1$ ssh -D 7001 &lt;SSH Server&gt; 图 5. 动态端口转发 似乎很简单，我们依然选择了 7001 作为本地的端口号，其实在这里 SSH 是创建了一个 SOCKS 代理服务。来看看帮助文档中对 -D 参数的描述： 12-D port This works by allocating a socket to listen to port on the local side, and whenever a connection is made to this port, the connection is forwarded over the secure channel, and the application protocol is then used to determine where to connect to from the remote machine. Currently the SOCKS4 and SOCKS5 protocols are supported, and ssh will act as a SOCKS server. Only root can forward privileged ports. Dynamic port forwardings can also be specified in the configuration file. 之后的使用就简单了，我们可以直接使用 localhost:7001 来作为正常的 SOCKS 代理来使用，直接在浏览器或 MSN 上设置即可。在 SSH Client 端无法访问的网站现在也都可以正常浏览。而这里需要值得注意的是，此时 SSH 所包护的范围只包括从浏览器端（SSH Client 端）到 SSH Server 端的连接，并不包含从 SSH Server 端 到目标网站的连接。如果后半截连接的安全不能得到充分的保证的话，这种方式仍不是合适的解决方案。 X 协议转发实例分析好了，让我们来看最后一个例子 - X 协议转发。 我们日常工作当中，可能会经常会远程登录到 Linux/Unix/Solaris/HP 等机器上去做一些开发或者维护，也经常需要以 GUI 方式运行一些程序，比如要求图形化界面来安装 DB2/WebSphere 等等。这时候通常有两种选择来实现：VNC 或者 X 窗口，让我们来看看后者。 使用 X 窗口通常需要分别安装：X Client 和 X Server 。在本例中我们的 X Client 就是所访问的远程 Linux/Unix/Solaris/HP，而我们的 X Server 则是发起访问的本地机器（例如你面前正在使用的笔记本或台式机）。把 X Client 端的 X 窗口显示在 X Server 端需要先行在 X Client 端指定 X Server 的位置，命令格式如下： 1export DISPLAY=&lt;X Server IP&gt;:&lt;display #&gt;.&lt;virtual #&gt; 例如： 1export DISPLAY=myDesktop:1.0 然后直接运行 X 应用即可，X 窗口就会自动在我们的本地端打开。 一切运行正常，但是，这时候 IT 部门突然在远程 Linux/Unix/Solaris/HP 前面加了一道防火墙。非常不幸的是，X 协议并不在允许通过的列表之内。怎么办？只能使用 VNC 了么？不，其实只要使用了 SSH 端口转发即可通过，同时也对 X 通讯数据做了加密，真是一举两得。（当然，使用此方法前最好先咨询相关 IT 部门是否符合相应的安全条例，以免造成违规操作。） 建立命令也很简单，直接从本地机器（X Server 端）发起一个如下的 SSH 连接即可： 1$ ssh -X &lt;SSH Server&gt; 图 5. X 转发 建立连接之后就可以直接运行远程的 X 应用。注意建立 X 转发之后会自动设置 DISPLAY 环境变量，通常会被设置成localhost:10.0，我们无需也不应该在连接之后再进行修改此环境变量。 一个比较常见的场景是，我们的本地机器是 Windows 操作系统，这时可以选择开源的 XMing 来作为我们的 XServer，而 SSH Client 则可以任意选择了，例如 PuTTY，Cygwin 均可以配置 访问 SSH 的同时建立 X 转发。 第四部分 总结至此，我们已经完成了本地端口转发，远程端口转发，动态端口转发以及 X 转发的介绍。回顾起来，总的思路是通过将 TCP 连接转发到 SSH 通道上以解决数据加密以及突破防火墙的种种限制。对一些已知端口号的应用，例如 Telnet/LDAP/SMTP，我们可以使用本地端口转发或者远程端口转发来达到目的。动态端口转发则可以实现 SOCKS 代理从而加密以及突破防火墙对 Web 浏览的限制。对于 X 应用，无疑是 X 转发最为适用了。虽然每一部分我们都只是简单的介绍了一下，但如果能灵活应用这些技巧，相信对我们的日常生活 / 工作也是会有所帮助的。 相关主题 《 SSH 权威指南》（O’Reilly 图书）详细介绍了 SSH 相关的更多技术内幕及相关技巧。 在developerWorks中国网站Linux专区中学习更多 Linux 方面的知识。","link":"/ssh-forward/"},{"title":"SoftEther使用命令行搭建VPN服务器","text":"网上大部分都是介绍使用Management客户端配置SoftEther,本文介绍使用SoftEther命令行搭建VPN服务器的过程。 下载与安装从GitHub Release或下载中心，根据系统类型及实际情况选择稳定非beta的rtm版server安装包，例如centos选择 VPNserver,复制下载链接，下载到/usr/local/src目录 12345cd /usr/local/srcwget https://github.com/SoftEtherVPN/SoftEtherVPN_Stable/releases/download/v4.29-9680-rtm/softether-vpnserver-v4.29-9680-rtm-2019.02.28-linux-x64-64bit.tar.gztar -zxvf softether-vpnserver-v4.29-9680-rtm-2019.02.28-linux-x64-64bit.tar.gzcd vpnservermake 安装过程中，按照提示输入1即可。 通常来说，会自己选择语言，如果需要修改，则修改lang.config文件即可。 运行执行./vpnserver start让VPN Server在后台运行. 1./vpnserver start 配置经测试，MacOS的Management配置客户端不能打开，所以改为以命令行配置： 进入命令行配置界面执行./vpncmd SoftEther VPN命令行配置SoftEther VPN Server. 1./vpncmd 提示如下： 1234567通过使用 vpncmd 程序，可以取得以下成果。1. VPN Server 或 VPN Bridge 的管理。2. VPN Client 的管理。3. 使用 VPN 工具 (证书创建和网络传输速度测试工具)选择 1, 2 或 3: 输入1回车即可，继续提示 12345指定的主机名或目标 VPN Server 或 VPN Bridge 正在 运行的计算机 IP 地址。通过以 &quot;主机名:端口号&quot; 格式指定，您还可以指定端口号。(当没有指定端口号时，使用 443。)如果不输入任何内容并按下回车键，将连接到端口号为 443 的本地主机 (这台电脑)。目标 IP 地址的主机名: 输入localhost:5555。 回车 123如果通过虚拟 HUB 管理模式连接到服务器，请输入虚拟 HUB 的名称。如果通过服务器管理模式连接，无须输入任何内容请按回车键。指定虚拟 HUB 名称: 根据提示，可根据实际情况输入。一般直接回车，进入管理员控制台界面： 12345与服务器 &quot;localhost&quot; 的连接已建立 (端口 5555)。您有整个 VPN Server 的管理员权限。VPN Server&gt; 首次进入直接回车即可。设置hub后这里输入hub名和密码可直接进入hub内，或直接回车进入管理员。 设置管理服务端登录密码1ServerPasswordSet 输入密码即可 输入help可以查看全部命令。 所有命令不区分大小写 创建hub1HubCreate 输入hub名称及密码，例如devops-hub 1234567891011VPN Server&gt;HubCreateHubCreate 命令 - 创建新的虚拟 HUB新创建的虚拟 HUB 的名字: devops-hub请输入密码。要取消，请按下 Ctrl + D 键。密码 : ***********确认输入: ***********命令成功完成。 选择hub1Hub devops-hub 1234VPN Server&gt;Hub devops-hubHub 命令 - 选择拟管理的虚拟 HUB选择虚拟 HUB &quot;devops-hub&quot;。命令成功完成。 创建用户1UserCreate 例如创建用户yhan219 1234567891011VPN Server/devops-hub&gt;UserCreateUserCreate 命令 - 创建用户用户名: yhan219加入群组名称: 用户全名: yhan219用户描述: yhan219命令成功完成。 设置用户密码1UserPasswordSet 用户名 例如： 123456789VPN Server/devops-hub&gt;UserPasswordSet yhan219UserPasswordSet 命令 - 将用户身份验证方法设置为密码验证，并设定密码请输入密码。要取消，请按下 Ctrl + D 键。密码 : ***确认输入: ***命令成功完成。 启用L2TP服务器功能1IPsecEnable 如下： 12345678910111213VPN Server&gt;IPsecEnableIPsecEnable 命令 - 启用或禁用 IPsec VPN Server 功能启用 L2TP over IPsec 服务器功能(yes / no): yes启用原始 L2TP 服务器功能(yes / no): no启用 EtherIP / L2TPv3 over IPsec服务器功能(yes / no): noIPsec 的预共享密钥(推荐：最多 9 位)devops为避免在用户名中遗漏 HUB，请默认虚拟 HUB 。devops-hub命令成功完成。 如果忘了共享秘钥，可通过命令查看，如下： 123456789VPN Server&gt;IpSecGetIPsecGet 命令 - 获得当前IPsec VPN Server 设置项目 |价值--------------------------------------------+----------L2TP over IPsec 服务器功能已启用 |是原始 L2TP 服务器功能已启用 | 否EtherIP / L2TPv3 over IPsec 服务器功能已启用|否IPsec 预共享密钥字符串 |devops默认虚拟 HUB 名 |devops-hub 启用虚拟 NAT 和 DHCP 服务器功能 (安全网络功能)1SecureNatEnable 启动安全网络功能的虚拟 DHCP 服务器功能1DhcpEnable 删除无用TCP监听端口1ListenerList 除了5555端口其他都删除 1ListenerDelete 123VPN Server&gt;ListenerDeleteListenerDelete 命令 - 删除 TCP 监听器TCP/IP 侦听器端口号: 443 删除默认hub（可选）1HubList 1234567891011121314151617181920212223242526272829303132VPN Server&gt;hublistHubList 命令 - 获取一个虚拟 HUB 列表项目 |价值------------+-------------------虚拟 HUB 名 |DEFAULT状态 |在线类型 |独立用户 |0组 |0会话 |0MAC 表 |0IP 表 |0登录次数 |0最后登录时间|2021-01-26 15:02:42最后通信时间|2021-01-26 15:02:42传输字节 |0传输数据包 |0------------+-------------------虚拟 HUB 名 |devops-hub状态 |在线类型 |独立用户 |1组 |0会话 |1MAC 表 |1IP 表 |1登录次数 |0最后登录时间|2021-01-27 12:44:21最后通信时间|2021-01-27 12:51:02传输字节 |12,192传输数据包 |236命令成功完成。 123VPN Server&gt;HubDelete DEFAULTHubDelete 命令 - 删除虚拟 HUB命令成功完成。 修改网段（可选）如果发生IP冲突，可选择修改网段： 查看当前虚拟DHCP配置1DhcpGet 12345678910111213141516VPN Server/devops-hub&gt;DhcpGetDhcpGet 命令 - 获得安全网络功能的虚拟 DHCP 服务器功能的设置项目 |价值-------------------------+--------------使用虚拟 DHCP 功能 |是分发地址范围的开始 |192.168.30.10分发地址范围的结束 |192.168.30.200子网掩码 |255.255.255.0租赁期限 (秒) |7200默认网关地址 |192.168.30.1DNS 服务器地址 1 |192.168.30.1DNS 服务器地址 2 |无域名 |保存 NAT 和 DHCP 操作日志|是静态路由表推送 |命令成功完成。 修改虚拟DHCP1DhcpSet 123456789101112131415161718192021VPN Server/devops-hub&gt;DhcpSetDhcpSet 命令 - 更改安全网络功能的虚拟 DHCP 服务器功能的设置分发地址范围的开始: 192.168.29.10分发地址范围的结束: 192.168.29.200子网掩码: 255.255.255.0租赁期限 (补): 7200默认网关 (可以不设定): 192.168.29.1DNS 服务器 1 (可以不设定): 192.168.29.1DNS 服务器 2 (可以不设定):域名:保存日志 (yes/no): yes命令成功完成。 所有192.168.30修改为192.168.29，其他不变。 查看SecureNatHost配置1SecureNatHostGet 12345678VPN Server/devops-hub&gt;SecureNatHostGetSecureNatHostGet 命令 - 获取安全网络功能的虚拟主机的网络接口设置项目 |价值--------+-----------------MAC 地址|5A-5A-5B-56-5F-59IP 地址 |192.168.30.1子网掩码|255.255.255.0命令成功完成。 修改SecureNatHost1SecureNatHostSet 123456789VPN Server/devops-hub&gt;SecureNatHostSetSecureNatHostSet 命令 - 更改安全网络功能的虚拟主机的网络接口设置MAC 地址: 5A-5A-5B-56-5F-59IP 地址: 192.168.29.1子网掩码: 255.255.255.0命令成功完成。 IP地址修改为上面的网关，其他不变 保存配置1Flush 12345VPN Server/devops-hub&gt;FlushFlush 命令 - 保存 VPN Server / Bridge 全部不稳定数据到配置文件。从内存到磁盘写入不稳定数据...保存成功。文件大小是 15,654 字节。命令成功完成。 重启服务1Reboot 开放外网端口需要在云服务器安全组或服务器开放500和4500端口，协议为UDP 另外，如果需要用外网Management客户端配置，需要云服务器安全组规则中开放端口5555，协议为TCP。为了安全一般在配置完成后需要关闭，需要使用的时候，先连接VPN并用内网IP配置Management。 其他一些命令，例如用户有效期，删除用户，删除群组等，可根据help命令提示参考使用 客户端连接对于win10，Mac，安卓等操作系统，无需下载客户端，用操作系统自带的VPN连接即可。其他操作系统或想使用客户端的，也可以自到下载链接中根据实际情况选择适合自己的操作系统下载。 以MacOS连接为例。 系统偏好设置–网络–左下角加号+新增 接口选择VPN，vpn类型选择L2TP/IPSec，服务名称随意填写。 服务器地址填写VPN服务器公网ip，用户名填写刚才新增的用户名(yhan219) 点击认证设置填写用户名密码和L2TP共享秘钥，连接即可。 在高级设置中勾选通过VPN发送所有流量。 应用–确认—连接即可。 在macOS上，如果能连接但是不能访问内网，查看下是否是服务器内网ip和本地内网ip的网段是相同，如果相同则会出现这个问题，修改任意一个的网段即可。 最后附上所有命令列表1help 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206您可以使用下面的 205 命令: About - 显示版本信息 AcAdd - 添加规则到 IP 地址限制列表(IPv4) AcAdd6 - 添加规则到源 IP 地址访问限制列表(IPv6) AcDel - 源 IP 地址限制列表内的删除规则 AcList - 获取源 IP 地址访问限制列表的规则项目列表 AccessAdd - 添加规则到允许访问列表 (IPv4) AccessAdd6 - 添加访问列表规则 (IPv6) AccessAddEx - 添加扩展访问列表规则 (IPv4：延迟、时基误差/数据包丢失产生) AccessAddEx6 - 添加扩展访问列表规则 (IPv6，生成延迟，时基误差/数据包丢失) AccessDelete - 从访问列表中删除规则 AccessDisable - 禁用访问列表规则 AccessEnable - 启用访问列表规则功能 AccessList - 获取访问列表规则 AdminOptionList - 获取虚拟 HUB 管理选项列表 AdminOptionSet - 设置虚拟 HUB 管理选项的价值 BridgeCreate - 创建本地的网桥连接 BridgeDelete - 删除本地网桥连接 BridgeDeviceList - 获取可以在当地的网桥上使用的 LAN 卡一览 BridgeList - 获得当地网桥连接列表 CAAdd - 添加可以信任的机构颁发的证书 CADelete - 删除可以信任的机构颁发的证书 CAGet - 获得可信任机构颁发的证书。 CAList - 获取可以信任的机构颁发证书的列表 Caps - 获得服务器的功能性能一览表 CascadeAnonymousSet - 将级联连接的用户认证类型设置为匿名身份验证 CascadeCertGet - 获取级联连接所需的客户端证书 CascadeCertSet - 将级联连接时所需的用户验证设置为客户证书验证 CascadeCompressDisable - 级联通信是数据禁止压缩功能 CascadeCompressEnable - 启用级联通信是数据压缩功能 CascadeCreate - 创建一个新的级联接续 CascadeDelete - 删除级联连接 CascadeDetailSet - 级联通信的高级设置 CascadeEncryptDisable - 级联连接通信时，禁用加密 CascadeEncryptEnable - 启用级联通信时加密 CascadeGet - 获取级联连接的设置 CascadeList - 获取级联接续列表 CascadeOffline - 将级联设置为脱机状态 CascadeOnline - 设置级联接续的在线状态 CascadePasswordSet - 将级联连接时所需的用户验证设置为密码验证 CascadePolicySet - 设置级联连接的安全协议 CascadeProxyHttp - 将级联连接方法设定为通过 HTTP 代理服务器 CascadeProxyNone - 将级联的连接方法设置为直接与 TCP/IP 连接 CascadeProxySocks - 将级联连接方法设定为通过 SOCKS 代理服务器 CascadeRename - 更改级联的名称 CascadeServerCertDelete - 删除级联服务器固有的证书 CascadeServerCertDisable - 禁用级联服务器证书验证选项 CascadeServerCertEnable - 启用级联服务器证书验证选项 CascadeServerCertGet - 获取级联连接服务器的固有证书 CascadeServerCertSet - 设置级联连接的服务器特定证书 CascadeSet - 对级联连接方的设定 CascadeStatusGet - 获取级联的当前状态 CascadeUsernameSet - 设置级联连接的用户名 Check - 检测 SoftEther VPN 是否能正常运行 ClusterConnectionStatusGet - 获得群集控制器的连接状态的信息 ClusterMemberCertGet - 获得群集成员证书 ClusterMemberInfoGet - 会员信息的获取 ClusterMemberList - 获得群集成员名单 ClusterSettingController - 设置 VPN Server 类型为群集控制器 ClusterSettingGet - 获取当前 VPN Server 群集配置 ClusterSettingMember - VPN Server 类型设置为群集成员 ClusterSettingStandalone - 设置为独立的 VPN Server 类型 ConfigGet - 获取 VPN Server 当前系统配置 ConfigSet - 往 VPN Server 上写入系统配置内容 ConnectionDisconnect - 断开 VPN Server 和 TCP 的连接 ConnectionGet - 获取连接到 VPN Server 的 TCP 信息一览表 ConnectionList - 获取与 VPN Server 相连的 TCP 连接一览 Crash - 出现一个错误的 VPN Server / Bridge 强行终止该进程。 CrlAdd - 添加无效的证书 CrlDel - 删除无效的证书 CrlGet - 获取无效的证书 CrlList - 获取无效证书名单列表 Debug - 执行调试命令 DhcpDisable - 禁用安全网络功能的虚拟 DHCP 服务器功能 DhcpEnable - 启动安全网络功能的虚拟 DHCP 服务器功能 DhcpGet - 获得安全网络功能的虚拟 DHCP 服务器功能的设置 DhcpSet - 更改安全网络功能的虚拟 DHCP 服务器功能的设置 DhcpTable - 获取安全网络功能的虚拟 DHCP 服务器租约表格 DynamicDnsGetStatus - 显示动态 DNS 功能的当前状态 DynamicDnsSetHostname - 设置动态 DNS 主机名 EtherIpClientAdd - 添加新的 EtherIP / L2TPv3 over IPsec 客户端设置来接受 EtherIP / L2TPv3 客户端设备 EtherIpClientDelete - 删除一个 EtherIP / L2TPv3 over IPsec 客户端设置 EtherIpClientList - 获得当前 EtherIP / L2TPv3 客户端设备条目定义列表 ExtOptionList - 获取虚拟 HUB 扩展选项列表 ExtOptionSet - 设置虚拟 HUB 扩展选项的值 Flush - 保存 VPN Server / Bridge 全部不稳定数据到配置文件。 GroupCreate - 创建组 GroupDelete - 删除组 GroupGet - 获得组信息和所属用户列表 GroupJoin - 用户添加到组 GroupList - 获取组列表 GroupPolicyRemove - 删除组的安全策略 GroupPolicySet - 设置组的安全策略 GroupSet - 设置组信息 GroupUnjoin - 从组内删除用户 Hub - 选择拟管理的虚拟 HUB HubCreate - 创建新的虚拟 HUB HubCreateDynamic - 创建一个新的动态虚拟 HUB (集群) HubCreateStatic - 新创建一个静态虚拟 HUB (集群用) HubDelete - 删除虚拟 HUB HubList - 获取一个虚拟 HUB 列表 HubSetDynamic - 将虚拟 HUB 的类型变为动态虚拟型 HubSetStatic - 将虚拟 HUB 的类型变为静态虚拟型 IPsecEnable - 启用或禁用 IPsec VPN Server 功能 IPsecGet - 获得当前IPsec VPN Server 设置 IpDelete - 删除 IP 地址表项 IpTable - 获取 IP 地址表数据库 KeepDisable - 禁用保持互联网连接功能 KeepEnable - 启动 Internet 保持连接功能 KeepGet - 获取保持互联网连接的功能 KeepSet - 设置 Internet 保持连接功能 LicenseAdd - 注册新的许可证密钥 LicenseDel - 删除已注册许可 LicenseList - 获得已注册许可证的列表 LicenseStatus - 获取目前的 VPN Server 状态 ListenerCreate - 创建新的 TCP 监听器 ListenerDelete - 删除 TCP 监听器 ListenerDisable - 停止 TCP 监听器运行 ListenerEnable - 开始 TCP 监听器运行 ListenerList - 获取 TCP 监听器列表 LogDisable - 禁用安全日志或数据包日志 LogEnable - 启用安全日志或数据包日志 LogFileGet - 日志文件下载 LogFileList - 获取日志文件列表 LogGet - 获取虚拟 HUB 日志的保存设定 LogPacketSaveType - 设置保存为数据包日志文件的数据包种类及保存。 LogSwitchSet - 设定替换日志文件的周期 MacDelete - 删除 MAC 地址表项 MacTable - 获取 MAC 地址表数据库 MakeCert - 创建新的 X.509 证书和密钥 (1024 位) MakeCert2048 - 创建新的 X.509 证书和密钥 (2048 位) NatDisable - 禁用安全网络功能的虚拟 NAT 功能 NatEnable - 启用安全网络功能的虚拟 NAT 功能 NatGet - 获得安全网络功能的虚拟 NAT 功能的设置 NatSet - 更改安全网络功能的虚拟 NAT 功能的设置 NatTable - 获得安全网络功能的虚拟 NAT 功能会话表 Offline - 虚拟 HUB 脱机 Online - 虚拟 HUB 的联机 OpenVpnEnable - 启用/禁用 OpenVPN 克隆服务器功能 OpenVpnGet - 获取 OpenVPN 克隆服务器功能的当前设置 OpenVpnMakeConfig - 生成 OpenVPN Client 样本设置文件 OptionsGet - 获得虚拟 HUB 的设置选项 PolicyList - 查看安全协议和可以设置的值得列表 RadiusServerDelete - 删除应用于用户认证的 RADIUS 服务器设置 RadiusServerGet - 获取用于用户认证的 RADIUS 服务器设置 RadiusServerSet - 使用在用户认证中使用的 RADIUS 服务器设置 Reboot - VPN Server 服务重新启动 RouterAdd - 定义一个新的虚拟 3 层交换机 RouterDelete - 删除虚拟 3 层交换机 RouterIfAdd - 在虚拟 3 层交换机上添加一个虚拟远程接口 RouterIfDel - 删除虚拟 3 层交换机的虚拟远程接口 RouterIfList - 获取在虚拟 3 层交换机中注册的远程接口的清单 RouterList - 获取虚拟 3 层交换机列表 RouterStart - 开始运行虚拟 3 层交换机 RouterStop - 停止虚拟 3 层交换机的运行 RouterTableAdd - 添加一个路由表项到虚拟 3 层交换机 RouterTableDel - 删除虚拟 3 层交换机的路由表项 RouterTableList - 获取虚拟 3 层交换机的路由列表 SecureNatDisable - 禁用虚拟 NAT 和 DHCP 服务器功能 (安全网络功能) SecureNatEnable - 启用虚拟 NAT 和 DHCP 服务器功能 (安全网络功能) SecureNatHostGet - 获取安全网络功能的虚拟主机的网络接口设置 SecureNatHostSet - 更改安全网络功能的虚拟主机的网络接口设置 SecureNatStatusGet - 获取虚拟 NAT 和 DHCP 服务器功能 (安全网络功能) 的工作状态 ServerCertGet - 获得 VPN Server 的 SSL 证书 ServerCertRegenerate - 生成一个新的带有指定 CN (Common Name) 的自签名证书，并且在 VPN Server 上注册。 ServerCertSet - VPN Server 的 SSL 证书和密钥的设置 ServerCipherGet - 获取 VPN 通信中使用的加密程序 ServerCipherSet - 设置 VPN 通讯中使用的加密程序， ServerInfoGet - 获取服务器信息 ServerKeyGet - 获取 VPN Server SSL 证书的密钥 ServerPasswordSet - 设置 VPN Server 管理员密码 ServerStatusGet - 获取当前服务器状态 SessionDisconnect - 断开会话 SessionGet - 获取会话信息 SessionList - 获取连接会话的列表 SetEnumAllow - 设定虚拟 HUB 允许向匿名用户显示。 SetEnumDeny - 设定虚拟 HUB 禁止向匿名用户显示。 SetHubPassword - 设置虚拟 HUB 的管理密码 SetMaxSession - 设定虚拟 HUB 的最大同时在线用户数量 SstpEnable - 启用/禁用 Microsoft SSTP VPN 克隆服务器功能 SstpGet - 获得 Microsoft SSTP VPN 克隆服务器功能的当前设置 StatusGet - 获取虚拟 HUB 的当前状况 SyslogDisable - 禁用发送系统日志的功能 SyslogEnable - 设置发送系统日志功能 SyslogGet - 取得发送系统日志的功能 TrafficClient - 在用户模式下，运行网络流量速度测试工具 TrafficServer - 在服务器模式下，运行网络流量速度测试工具 UserAnonymousSet - 将用户身份验证方法设置为匿名验证 UserCertGet - 获取注册固有证书认证用户的证书 UserCertSet - 将用户身份验证方法设置为固有证书验证，并设定证书 UserCreate - 创建用户 UserDelete - 删除用户 UserExpiresSet - 设置用户的有效期限 UserGet - 获取用户信息 UserList - 获取用户列表 UserNTLMSet - 用户身份验证方法设置为 NT 域认证 UserPasswordSet - 将用户身份验证方法设置为密码验证，并设定密码 UserPolicyRemove - 删除用户的安全策略 UserPolicySet - 设置用户的安全策略 UserRadiusSet - 将用户的认证方法设定为半径认证 UserSet - 更改用户信息 UserSignedSet - 将用户身份验证方法设置为已签名证明书认证 VpnAzureGetStatus - 显示 VPN Azure 功能的当前状态 VpnAzureSetEnable - 启用/禁用 VPN Azure 功能 VpnOverIcmpDnsEnable - 启用/禁用 VPN over ICMP / VPN over DNS服务器功能 VpnOverIcmpDnsGet - 获取 VPN over ICMP / VPN over DNS 功能的当前设置","link":"/soft-ether-vpn/"},{"title":"ssh常用命令与配置","text":"记录一些在使用ssh过程中的一些常用命令与配置。只做命令记录，不做原理解析。 ssh跳板机a服务器通过ssh连接到b服务器跳转到c服务器，在a服务器上运行命令 1ssh -J root@b的ip root@c的ip ssh端口转发a服务器5308端口转发到192.168.0.101的5308端口，在a服务器上执行命令 1ssh -fCPN -L 0.0.0.0:5308:192.168.0.101:5308 root@192.168.0.101","link":"/ssh/"},{"title":"Systemd 入门教程：命令篇","text":"Systemd 是 Linux 系统工具，用来启动守护进程，已成为大多数发行版的标准配置。 本文介绍它的基本用法，分为上下两篇。今天介绍它的主要命令，下一篇介绍如何用于实战。 一、由来历史上，Linux 的启动一直采用init进程。 下面的命令用来启动服务。 123$ sudo /etc/init.d/apache2 start# 或者$ service apache2 start 这种方法有两个缺点。 一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 二、Systemd 概述Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。 （上图为 Systemd 作者 Lennart Poettering） 使用了 Systemd，就不需要再用init了。Systemd 取代了initd，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。 1$ systemctl --version 上面的命令查看 Systemd 的版本。 Systemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合，违反”keep simple, keep stupid”的Unix 哲学。 （上图为 Systemd 架构图） 三、系统管理Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。 3.1 systemctlsystemctl是 Systemd 的主命令，用于管理系统。 1234567891011121314151617181920# 重启系统$ sudo systemctl reboot# 关闭系统，切断电源$ sudo systemctl poweroff# CPU停止工作$ sudo systemctl halt# 暂停系统$ sudo systemctl suspend# 让系统进入冬眠状态$ sudo systemctl hibernate# 让系统进入交互式休眠状态$ sudo systemctl hybrid-sleep# 启动进入救援状态（单用户状态）$ sudo systemctl rescue 3.2 systemd-analyzesystemd-analyze命令用于查看启动耗时。 1234567891011# 查看启动耗时$ systemd-analyze # 查看每个服务的启动耗时$ systemd-analyze blame# 显示瀑布状的启动过程流$ systemd-analyze critical-chain# 显示指定服务的启动流$ systemd-analyze critical-chain atd.service 3.3 hostnamectlhostnamectl命令用于查看当前主机的信息。 12345# 显示当前主机的信息$ hostnamectl# 设置主机名。$ sudo hostnamectl set-hostname rhel7 3.4 localectllocalectl命令用于查看本地化设置。 123456# 查看本地化设置$ localectl# 设置本地化参数。$ sudo localectl set-locale LANG=en_GB.utf8$ sudo localectl set-keymap en_GB 3.5 timedatectltimedatectl命令用于查看当前时区设置。 12345678910# 查看当前时区设置$ timedatectl# 显示所有可用的时区$ timedatectl list-timezones # 设置当前时区$ sudo timedatectl set-timezone America/New_York$ sudo timedatectl set-time YYYY-MM-DD$ sudo timedatectl set-time HH:MM:SS 3.6 loginctlloginctl命令用于查看当前登录的用户。 12345678# 列出当前session$ loginctl list-sessions# 列出当前登录用户$ loginctl list-users# 列出显示指定用户的信息$ loginctl show-user ruanyf 四、Unit4.1 含义Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。 Unit 一共分成12种。 Service unit：系统服务 Target unit：多个 Unit 构成的一个组 Device Unit：硬件设备 Mount Unit：文件系统的挂载点 Automount Unit：自动挂载点 Path Unit：文件或路径 Scope Unit：不是由 Systemd 启动的外部进程 Slice Unit：进程组 Snapshot Unit：Systemd 快照，可以切回某个快照 Socket Unit：进程间通信的 socket Swap Unit：swap 文件 Timer Unit：定时器 systemctl list-units命令可以查看当前系统的所有 Unit 。 1234567891011121314# 列出正在运行的 Unit$ systemctl list-units# 列出所有Unit，包括没有找到配置文件的或者启动失败的$ systemctl list-units --all# 列出所有没有运行的 Unit$ systemctl list-units --all --state=inactive# 列出所有加载失败的 Unit$ systemctl list-units --failed# 列出所有正在运行的、类型为 service 的 Unit$ systemctl list-units --type=service 4.2 Unit 的状态systemctl status命令用于查看系统状态和单个 Unit 的状态。 12345678# 显示系统状态$ systemctl status# 显示单个 Unit 的状态$ sysystemctl status bluetooth.service# 显示远程主机的某个 Unit 的状态$ systemctl -H root@rhel7.example.com status httpd.service 除了status命令，systemctl还提供了三个查询状态的简单方法，主要供脚本内部的判断语句使用。 12345678# 显示某个 Unit 是否正在运行$ systemctl is-active application.service# 显示某个 Unit 是否处于启动失败状态$ systemctl is-failed application.service# 显示某个 Unit 服务是否建立了启动链接$ systemctl is-enabled application.service 4.3 Unit 管理对于用户来说，最常用的是下面这些命令，用于启动和停止 Unit（主要是 service）。 1234567891011121314151617181920212223242526# 立即启动一个服务$ sudo systemctl start apache.service# 立即停止一个服务$ sudo systemctl stop apache.service# 重启一个服务$ sudo systemctl restart apache.service# 杀死一个服务的所有子进程$ sudo systemctl kill apache.service# 重新加载一个服务的配置文件$ sudo systemctl reload apache.service# 重载所有修改过的配置文件$ sudo systemctl daemon-reload# 显示某个 Unit 的所有底层参数$ systemctl show httpd.service# 显示某个 Unit 的指定属性的值$ systemctl show -p CPUShares httpd.service# 设置某个 Unit 的指定属性$ sudo systemctl set-property httpd.service CPUShares=500 4.4 依赖关系Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。 systemctl list-dependencies命令列出一个 Unit 的所有依赖。 1$ systemctl list-dependencies nginx.service 上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用--all参数。 1$ systemctl list-dependencies --all nginx.service 五、Unit 的配置文件5.1 概述每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。 123$ sudo systemctl enable clamd@scan.service# 等同于$ sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。 与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 1$ sudo systemctl disable clamd@scan.service 配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。 5.2 配置文件的状态systemctl list-unit-files命令用于列出所有配置文件。 12345# 列出所有配置文件$ systemctl list-unit-files# 列出指定类型的配置文件$ systemctl list-unit-files --type=service 这个命令会输出一个列表。 123456$ systemctl list-unit-filesUNIT FILE STATEchronyd.service enabledclamd@.service staticclamd@scan.service disabled 这个列表显示每个配置文件的状态，一共有四种。 enabled：已建立启动链接 disabled：没建立启动链接 static：该配置文件没有[Install]部分（无法执行），只能作为其他配置文件的依赖 masked：该配置文件被禁止建立启动链接 注意，从配置文件的状态无法看出，该 Unit 是否正在运行。这必须执行前面提到的systemctl status命令。 1$ systemctl status bluetooth.service 一旦修改配置文件，就要让 SystemD 重新加载配置文件，然后重新启动，否则修改不会生效。 12$ sudo systemctl daemon-reload$ sudo systemctl restart httpd.service 5.3 配置文件的格式配置文件就是普通的文本文件，可以用文本编辑器打开。 systemctl cat命令可以查看配置文件的内容。 1234567891011$ systemctl cat atd.service[Unit]Description=ATD daemon[Service]Type=forkingExecStart=/usr/bin/atd[Install]WantedBy=multi-user.target 从上面的输出可以看到，配置文件分成几个区块。每个区块的第一行，是用方括号表示的区别名，比如[Unit]。注意，配置文件的区块名和字段名，都是大小写敏感的。 每个区块内部是一些等号连接的键值对。 12345[Section]Directive1=valueDirective2=value. . . 注意，键值对的等号两侧不能有空格。 5.4 配置文件的区块[Unit]区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。 Description：简短描述 Documentation：文档地址 Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败 BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行 Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动 After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动 Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行 Condition...：当前 Unit 运行必须满足的条件，否则不会运行 Assert...：当前 Unit 运行必须满足的条件，否则会报启动失败 [Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 + .wants后缀构成的子目录中 RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中 Alias：当前 Unit 可用于启动的别名 Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit [Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 Type：定义启动时的进程行为。它有以下几种值。 Type=simple：默认值，执行ExecStart指定的命令，启动主进程 Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出 Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行 Type=dbus：当前服务通过D-Bus启动 Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行 Type=idle：若有其他任务执行完毕，当前服务才会运行 ExecStart：启动当前服务的命令 ExecStartPre：启动当前服务之前执行的命令 ExecStartPost：启动当前服务之后执行的命令 ExecReload：重启当前服务时执行的命令 ExecStop：停止当前服务时执行的命令 ExecStopPost：停止当其服务之后执行的命令 RestartSec：自动重启当前服务间隔的秒数 Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数 Environment：指定环境变量 Unit 配置文件的完整字段清单，请参考官方文档。 六、Target启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。 简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于”状态点”，启动某个 Target 就好比启动到某种状态。 传统的init启动模式里面，有 RunLevel 的概念，跟 Target 的作用很类似。不同的是，RunLevel 是互斥的，不可能多个 RunLevel 同时启动，但是多个 Target 可以同时启动。 12345678910111213141516# 查看当前系统的所有 Target$ systemctl list-unit-files --type=target# 查看一个 Target 包含的所有 Unit$ systemctl list-dependencies multi-user.target# 查看启动时的默认 Target$ systemctl get-default# 设置启动时的默认 Target$ sudo systemctl set-default multi-user.target# 切换 Target 时，默认不关闭前一个 Target 启动的进程，# systemctl isolate 命令改变这种行为，# 关闭前一个 Target 里面所有不属于后一个 Target 的进程$ sudo systemctl isolate multi-user.target Target 与 传统 RunLevel 的对应关系如下。 123456789Traditional runlevel New target name Symbolically linked to...Runlevel 0 | runlevel0.target -&gt; poweroff.targetRunlevel 1 | runlevel1.target -&gt; rescue.targetRunlevel 2 | runlevel2.target -&gt; multi-user.targetRunlevel 3 | runlevel3.target -&gt; multi-user.targetRunlevel 4 | runlevel4.target -&gt; multi-user.targetRunlevel 5 | runlevel5.target -&gt; graphical.targetRunlevel 6 | runlevel6.target -&gt; reboot.target 它与init进程的主要差别如下。 （1）默认的 RunLevel（在/etc/inittab文件设置）现在被默认的 Target 取代，位置是/etc/systemd/system/default.target，通常符号链接到graphical.target（图形界面）或者multi-user.target（多用户命令行）。 （2）启动脚本的位置，以前是/etc/init.d目录，符号链接到不同的 RunLevel 目录 （比如/etc/rc3.d、/etc/rc5.d等），现在则存放在/lib/systemd/system和/etc/systemd/system目录。 （3）配置文件的位置，以前init进程的配置文件是/etc/inittab，各种服务的配置文件存放在/etc/sysconfig目录。现在的配置文件主要存放在/lib/systemd目录，在/etc/systemd目录里面的修改可以覆盖原始设置。 七、日志管理Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。 journalctl功能强大，用法非常多。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 查看所有日志（默认情况下 ，只保存本次启动的日志）$ sudo journalctl# 查看内核日志（不显示应用日志）$ sudo journalctl -k# 查看系统本次启动的日志$ sudo journalctl -b$ sudo journalctl -b -0# 查看上一次启动的日志（需更改设置）$ sudo journalctl -b -1# 查看指定时间的日志$ sudo journalctl --since=&quot;2012-10-30 18:17:16&quot;$ sudo journalctl --since &quot;20 min ago&quot;$ sudo journalctl --since yesterday$ sudo journalctl --since &quot;2015-01-10&quot; --until &quot;2015-01-11 03:00&quot;$ sudo journalctl --since 09:00 --until &quot;1 hour ago&quot;# 显示尾部的最新10行日志$ sudo journalctl -n# 显示尾部指定行数的日志$ sudo journalctl -n 20# 实时滚动显示最新日志$ sudo journalctl -f# 查看指定服务的日志$ sudo journalctl /usr/lib/systemd/systemd# 查看指定进程的日志$ sudo journalctl _PID=1# 查看某个路径的脚本的日志$ sudo journalctl /usr/bin/bash# 查看指定用户的日志$ sudo journalctl _UID=33 --since today# 查看某个 Unit 的日志$ sudo journalctl -u nginx.service$ sudo journalctl -u nginx.service --since today# 实时滚动显示某个 Unit 的最新日志$ sudo journalctl -u nginx.service -f# 合并显示多个 Unit 的日志$ journalctl -u nginx.service -u php-fpm.service --since today# 查看指定优先级（及其以上级别）的日志，共有8级# 0: emerg# 1: alert# 2: crit# 3: err# 4: warning# 5: notice# 6: info# 7: debug$ sudo journalctl -p err -b# 日志默认分页输出，--no-pager 改为正常的标准输出$ sudo journalctl --no-pager# 以 JSON 格式（单行）输出$ sudo journalctl -b -u nginx.service -o json# 以 JSON 格式（多行）输出，可读性更好$ sudo journalctl -b -u nginx.serviceqq -o json-pretty# 显示日志占据的硬盘空间$ sudo journalctl --disk-usage# 指定日志文件占据的最大空间$ sudo journalctl --vacuum-size=1G# 指定日志文件保存多久$ sudo journalctl --vacuum-time=1years （完）","link":"/systemd-tutorial-commands/"},{"title":"Systemd 入门教程：实战篇","text":"上一篇文章，我介绍了 Systemd 的主要命令，今天介绍如何使用它完成一些基本的任务。 一、开机启动对于那些支持 Systemd 的软件，安装的时候，会自动在/usr/lib/systemd/system目录添加一个配置文件。 如果你想让该软件开机启动，就执行下面的命令（以httpd.service为例）。 1$ sudo systemctl enable httpd 上面的命令相当于在/etc/systemd/system目录添加一个符号链接，指向/usr/lib/systemd/system里面的httpd.service文件。 这是因为开机时，Systemd只执行/etc/systemd/system目录里面的配置文件。这也意味着，如果把修改后的配置文件放在该目录，就可以达到覆盖原始配置的效果。 二、启动服务设置开机启动以后，软件并不会立即启动，必须等到下一次开机。如果想现在就运行该软件，那么要执行systemctl start命令。 1$ sudo systemctl start httpd 执行上面的命令以后，有可能启动失败，因此要用systemctl status命令查看一下该服务的状态。 123456789101112131415161718$ sudo systemctl status httpdhttpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled) Active: active (running) since 金 2014-12-05 12:18:22 JST; 7min ago Main PID: 4349 (httpd) Status: &quot;Total requests: 1; Current requests/sec: 0; Current traffic: 0 B/sec&quot; CGroup: /system.slice/httpd.service ├─4349 /usr/sbin/httpd -DFOREGROUND ├─4350 /usr/sbin/httpd -DFOREGROUND ├─4351 /usr/sbin/httpd -DFOREGROUND ├─4352 /usr/sbin/httpd -DFOREGROUND ├─4353 /usr/sbin/httpd -DFOREGROUND └─4354 /usr/sbin/httpd -DFOREGROUND12月 05 12:18:22 localhost.localdomain systemd[1]: Starting The Apache HTTP Server...12月 05 12:18:22 localhost.localdomain systemd[1]: Started The Apache HTTP Server.12月 05 12:22:40 localhost.localdomain systemd[1]: Started The Apache HTTP Server. 上面的输出结果含义如下。 Loaded行：配置文件的位置，是否设为开机启动 Active行：表示正在运行 Main PID行：主进程ID Status行：由应用本身（这里是 httpd ）提供的软件当前状态 CGroup块：应用的所有子进程 日志块：应用的日志 三、停止服务终止正在运行的服务，需要执行systemctl stop命令。 1$ sudo systemctl stop httpd.service 有时候，该命令可能没有响应，服务停不下来。这时候就不得不”杀进程”了，向正在运行的进程发出kill信号。 1$ sudo systemctl kill httpd.service 此外，重启服务要执行systemctl restart命令。 1$ sudo systemctl restart httpd.service 四、读懂配置文件一个服务怎么启动，完全由它的配置文件决定。下面就来看，配置文件有些什么内容。 前面说过，配置文件主要放在/usr/lib/systemd/system目录，也可能在/etc/systemd/system目录。找到配置文件以后，使用文本编辑器打开即可。 systemctl cat命令可以用来查看配置文件，下面以sshd.service文件为例，它的作用是启动一个 SSH 服务器，供其他用户以 SSH 方式登录。 12345678910111213141516171819$ systemctl cat sshd.service[Unit]Description=OpenSSH server daemonDocumentation=man:sshd(8) man:sshd_config(5)After=network.target sshd-keygen.serviceWants=sshd-keygen.service[Service]EnvironmentFile=/etc/sysconfig/sshdExecStart=/usr/sbin/sshd -D $OPTIONSExecReload=/bin/kill -HUP $MAINPIDType=simpleKillMode=processRestart=on-failureRestartSec=42s[Install]WantedBy=multi-user.target 可以看到，配置文件分成几个区块，每个区块包含若干条键值对。 下面依次解释每个区块的内容。 五、 [Unit] 区块：启动顺序与依赖关系。Unit区块的Description字段给出当前服务的简单描述，Documentation字段给出文档位置。 接下来的设置是启动顺序和依赖关系，这个比较重要。 After字段：表示如果network.target或sshd-keygen.service需要启动，那么sshd.service应该在它们之后启动。 相应地，还有一个Before字段，定义sshd.service应该在哪些服务之前启动。 注意，After和Before字段只涉及启动顺序，不涉及依赖关系。 举例来说，某 Web 应用需要 postgresql 数据库储存数据。在配置文件中，它只定义要在 postgresql 之后启动，而没有定义依赖 postgresql 。上线后，由于某种原因，postgresql 需要重新启动，在停止服务期间，该 Web 应用就会无法建立数据库连接。 设置依赖关系，需要使用Wants字段和Requires字段。 Wants字段：表示sshd.service与sshd-keygen.service之间存在”弱依赖”关系，即如果”sshd-keygen.service”启动失败或停止运行，不影响sshd.service继续执行。 Requires字段则表示”强依赖”关系，即如果该服务启动失败或异常退出，那么sshd.service也必须退出。 注意，Wants字段与Requires字段只涉及依赖关系，与启动顺序无关，默认情况下是同时启动的。 六、[Service] 区块：启动行为Service区块定义如何启动当前服务。 6.1 启动命令许多软件都有自己的环境参数文件，该文件可以用EnvironmentFile字段读取。 EnvironmentFile字段：指定当前服务的环境参数文件。该文件内部的key=value键值对，可以用$key的形式，在当前配置文件中获取。 上面的例子中，sshd 的环境参数文件是/etc/sysconfig/sshd。 配置文件里面最重要的字段是ExecStart。 ExecStart字段：定义启动进程时执行的命令。 上面的例子中，启动sshd，执行的命令是/usr/sbin/sshd -D $OPTIONS，其中的变量$OPTIONS就来自EnvironmentFile字段指定的环境参数文件。 与之作用相似的，还有如下这些字段。 ExecReload字段：重启服务时执行的命令 ExecStop字段：停止服务时执行的命令 ExecStartPre字段：启动服务之前执行的命令 ExecStartPost字段：启动服务之后执行的命令 ExecStopPost字段：停止服务之后执行的命令 请看下面的例子。 123456[Service]ExecStart=/bin/echo execstart1ExecStart=ExecStart=/bin/echo execstart2ExecStartPost=/bin/echo post1ExecStartPost=/bin/echo post2 上面这个配置文件，第二行ExecStart设为空值，等于取消了第一行的设置，运行结果如下。 123execstart2post1post2 所有的启动设置之前，都可以加上一个连词号（-），表示”抑制错误”，即发生错误的时候，不影响其他命令的执行。比如，EnvironmentFile=-/etc/sysconfig/sshd（注意等号后面的那个连词号），就表示即使/etc/sysconfig/sshd文件不存在，也不会抛出错误。 6.2 启动类型Type字段定义启动类型。它可以设置的值如下。 simple（默认值）：ExecStart字段启动的进程为主进程 forking：ExecStart字段将以fork()方式启动，此时父进程将会退出，子进程将成为主进程 oneshot：类似于simple，但只执行一次，Systemd 会等它执行完，才启动其他服务 dbus：类似于simple，但会等待 D-Bus 信号后启动 notify：类似于simple，启动结束后会发出通知信号，然后 Systemd 再启动其他服务 idle：类似于simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合 下面是一个oneshot的例子，笔记本电脑启动时，要把触摸板关掉，配置文件可以这样写。 123456789[Unit]Description=Switch-off Touchpad[Service]Type=oneshotExecStart=/usr/bin/touchpad-off[Install]WantedBy=multi-user.target 上面的配置文件，启动类型设为oneshot，就表明这个服务只要运行一次就够了，不需要长期运行。 如果关闭以后，将来某个时候还想打开，配置文件修改如下。 1234567891011[Unit]Description=Switch-off Touchpad[Service]Type=oneshotExecStart=/usr/bin/touchpad-off startExecStop=/usr/bin/touchpad-off stopRemainAfterExit=yes[Install]WantedBy=multi-user.target 上面配置文件中，RemainAfterExit字段设为yes，表示进程退出以后，服务仍然保持执行。这样的话，一旦使用systemctl stop命令停止服务，ExecStop指定的命令就会执行，从而重新开启触摸板。 6.3 重启行为Service区块有一些字段，定义了重启行为。 KillMode字段：定义 Systemd 如何停止 sshd 服务。 上面这个例子中，将KillMode设为process，表示只停止主进程，不停止任何sshd 子进程，即子进程打开的 SSH session 仍然保持连接。这个设置不太常见，但对 sshd 很重要，否则你停止服务的时候，会连自己打开的 SSH session 一起杀掉。 KillMode字段可以设置的值如下。 control-group（默认值）：当前控制组里面的所有子进程，都会被杀掉 process：只杀主进程 mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号 none：没有进程会被杀掉，只是执行服务的 stop 命令。 接下来是Restart字段。 Restart字段：定义了 sshd 退出后，Systemd 的重启方式。 上面的例子中，Restart设为on-failure，表示任何意外的失败，就将重启sshd。如果 sshd 正常停止（比如执行systemctl stop命令），它就不会重启。 Restart字段可以设置的值如下。 no（默认值）：退出后不会重启 on-success：只有正常退出时（退出状态码为0），才会重启 on-failure：非正常退出时（退出状态码非0），包括被信号终止和超时，才会重启 on-abnormal：只有被信号终止和超时，才会重启 on-abort：只有在收到没有捕捉到的信号终止时，才会重启 on-watchdog：超时退出，才会重启 always：不管是什么退出原因，总是重启 对于守护进程，推荐设为on-failure。对于那些允许发生错误退出的服务，可以设为on-abnormal。 最后是RestartSec字段。 RestartSec字段：表示 Systemd 重启服务之前，需要等待的秒数。上面的例子设为等待42秒。 七、[Install] 区块Install区块，定义如何安装这个配置文件，即怎样做到开机启动。 WantedBy字段：表示该服务所在的 Target。 Target的含义是服务组，表示一组服务。WantedBy=multi-user.target指的是，sshd 所在的 Target 是multi-user.target。 这个设置非常重要，因为执行systemctl enable sshd.service命令时，sshd.service的一个符号链接，就会放在/etc/systemd/system目录下面的multi-user.target.wants子目录之中。 Systemd 有默认的启动 Target。 12$ systemctl get-defaultmulti-user.target 上面的结果表示，默认的启动 Target 是multi-user.target。在这个组里的所有服务，都将开机启动。这就是为什么systemctl enable命令能设置开机启动的原因。 使用 Target 的时候，systemctl list-dependencies命令和systemctl isolate命令也很有用。 123456# 查看 multi-user.target 包含的所有服务$ systemctl list-dependencies multi-user.target# 切换到另一个 target# shutdown.target 就是关机状态$ sudo systemctl isolate shutdown.target 一般来说，常用的 Target 有两个：一个是multi-user.target，表示多用户命令行状态；另一个是graphical.target，表示图形用户状态，它依赖于multi-user.target。官方文档有一张非常清晰的 [Target 依赖关系图](https://www.freedesktop.org/software/systemd/man/bootup.html#System Manager Bootup)。 八、Target 的配置文件Target 也有自己的配置文件。 123456789$ systemctl cat multi-user.target[Unit]Description=Multi-User SystemDocumentation=man:systemd.special(7)Requires=basic.targetConflicts=rescue.service rescue.targetAfter=basic.target rescue.service rescue.targetAllowIsolate=yes 注意，Target 配置文件里面没有启动命令。 上面输出结果中，主要字段含义如下。 Requires字段：要求basic.target一起运行。 Conflicts字段：冲突字段。如果rescue.service或rescue.target正在运行，multi-user.target就不能运行，反之亦然。 After：表示multi-user.target在basic.target 、 rescue.service、 rescue.target之后启动，如果它们有启动的话。 AllowIsolate：允许使用systemctl isolate命令切换到multi-user.target。 九、修改配置文件后重启修改配置文件以后，需要重新加载配置文件，然后重新启动相关服务。 12345# 重新加载配置文件$ sudo systemctl daemon-reload# 重启相关服务$ sudo systemctl restart foobar （完）","link":"/systemd-tutorial-part-two/"},{"title":"xxl-job安装及使用","text":"XXL-JOB是一个分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。现已开放源代码并接入多家公司线上产品线，开箱即用。 admin下载及安装下载12git clone https://github.com/xuxueli/xxl-job.gitcd xxl-job 切换分支切换到最新分支,当前最新2.2.0 1git checkout 2.2.0 修改文件 根据自身情况修改文件xxl-job-admin/src/main/resources/application.properties,主要修改数据库及email相关配置 修改文件xxl-job-admin/src/main/resources/logback.xml中的log.path 打包1mvn clean install package 数据库将文件doc/db/tables_xxl_job.sql导入数据库 1mysql -uroot -p &lt; doc/db/tables_xxl_job.sql 运行1java -jar xxl-job-admin/target/xxl-job-admin-2.2.0.jar 查看访问http://localhost:8080/xxl-job-admin初始账号及密码admin,123456 与spring boot集成添加依赖gradle 1implementation 'com.xuxueli:xxl-job-core:2.2.0' maven 12345&lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 添加配置在application.properties中添加 1234567891011121314151617### xxl-job admin address list, such as &quot;http://address&quot; or &quot;http://address01,http://address02&quot;xxl.job.admin.addresses=http://127.0.0.1:8080/xxl-job-admin### xxl-job, access tokenxxl.job.accessToken=### xxl-job executor appnamexxl.job.executor.appname=xxl-job-executor-sample### xxl-job executor registry-address: default use address to registry , otherwise use ip:port if address is nullxxl.job.executor.address=### xxl-job executor server-infoxxl.job.executor.ip=xxl.job.executor.port=9999### xxl-job executor log-pathxxl.job.executor.logpath=/data/applogs/xxl-job/jobhandler### xxl-job executor log-retention-daysxxl.job.executor.logretentiondays=30 添加bean123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.xxl.job.executor.core.config;import com.xxl.job.core.executor.impl.XxlJobSpringExecutor;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * xxl-job config * * @author xuxueli 2017-04-28 */@Configurationpublic class XxlJobConfig { private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class); @Value(&quot;${xxl.job.admin.addresses}&quot;) private String adminAddresses; @Value(&quot;${xxl.job.accessToken}&quot;) private String accessToken; @Value(&quot;${xxl.job.executor.appname}&quot;) private String appname; @Value(&quot;${xxl.job.executor.address}&quot;) private String address; @Value(&quot;${xxl.job.executor.ip}&quot;) private String ip; @Value(&quot;${xxl.job.executor.port}&quot;) private int port; @Value(&quot;${xxl.job.executor.logpath}&quot;) private String logPath; @Value(&quot;${xxl.job.executor.logretentiondays}&quot;) private int logRetentionDays; @Bean public XxlJobSpringExecutor xxlJobExecutor() { logger.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; xxl-job config init.&quot;); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppname(appname); xxlJobSpringExecutor.setAddress(address); xxlJobSpringExecutor.setIp(ip); xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); xxlJobSpringExecutor.setLogPath(logPath); xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } /** * 针对多网卡、容器内部署等情况，可借助 &quot;spring-cloud-commons&quot; 提供的 &quot;InetUtils&quot; 组件灵活定制注册IP； * * 1、引入依赖： * &lt;dependency&gt; * &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; * &lt;artifactId&gt;spring-cloud-commons&lt;/artifactId&gt; * &lt;version&gt;${version}&lt;/version&gt; * &lt;/dependency&gt; * * 2、配置文件，或者容器启动变量 * spring.cloud.inetutils.preferred-networks: 'xxx.xxx.xxx.' * * 3、获取IP * String ip_ = inetUtils.findFirstNonLoopbackHostInfo().getIpAddress(); */} 使用案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236package com.xxl.job.executor.service.jobhandler;import com.xxl.job.core.biz.model.ReturnT;import com.xxl.job.core.handler.IJobHandler;import com.xxl.job.core.handler.annotation.XxlJob;import com.xxl.job.core.log.XxlJobLogger;import com.xxl.job.core.util.ShardingUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Component;import java.io.BufferedInputStream;import java.io.BufferedReader;import java.io.DataOutputStream;import java.io.InputStreamReader;import java.net.HttpURLConnection;import java.net.URL;import java.util.Arrays;import java.util.concurrent.TimeUnit;/** * XxlJob开发示例（Bean模式） * * 开发步骤： * 1、在Spring Bean实例中，开发Job方法，方式格式要求为 &quot;public ReturnT&lt;String&gt; execute(String param)&quot; * 2、为Job方法添加注解 &quot;@XxlJob(value=&quot;自定义jobhandler名称&quot;, init = &quot;JobHandler初始化方法&quot;, destroy = &quot;JobHandler销毁方法&quot;)&quot;，注解value值对应的是调度中心新建任务的JobHandler属性的值。 * 3、执行日志：需要通过 &quot;XxlJobLogger.log&quot; 打印执行日志； * * @author xuxueli 2019-12-11 21:52:51 */@Componentpublic class SampleXxlJob { private static Logger logger = LoggerFactory.getLogger(SampleXxlJob.class); /** * 1、简单任务示例（Bean模式） */ @XxlJob(&quot;demoJobHandler&quot;) public ReturnT&lt;String&gt; demoJobHandler(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); for (int i = 0; i &lt; 5; i++) { XxlJobLogger.log(&quot;beat at:&quot; + i); TimeUnit.SECONDS.sleep(2); } return ReturnT.SUCCESS; } /** * 2、分片广播任务 */ @XxlJob(&quot;shardingJobHandler&quot;) public ReturnT&lt;String&gt; shardingJobHandler(String param) throws Exception { // 分片参数 ShardingUtil.ShardingVO shardingVO = ShardingUtil.getShardingVo(); XxlJobLogger.log(&quot;分片参数：当前分片序号 = {}, 总分片数 = {}&quot;, shardingVO.getIndex(), shardingVO.getTotal()); // 业务逻辑 for (int i = 0; i &lt; shardingVO.getTotal(); i++) { if (i == shardingVO.getIndex()) { XxlJobLogger.log(&quot;第 {} 片, 命中分片开始处理&quot;, i); } else { XxlJobLogger.log(&quot;第 {} 片, 忽略&quot;, i); } } return ReturnT.SUCCESS; } /** * 3、命令行任务 */ @XxlJob(&quot;commandJobHandler&quot;) public ReturnT&lt;String&gt; commandJobHandler(String param) throws Exception { String command = param; int exitValue = -1; BufferedReader bufferedReader = null; try { // command process Process process = Runtime.getRuntime().exec(command); BufferedInputStream bufferedInputStream = new BufferedInputStream(process.getInputStream()); bufferedReader = new BufferedReader(new InputStreamReader(bufferedInputStream)); // command log String line; while ((line = bufferedReader.readLine()) != null) { XxlJobLogger.log(line); } // command exit process.waitFor(); exitValue = process.exitValue(); } catch (Exception e) { XxlJobLogger.log(e); } finally { if (bufferedReader != null) { bufferedReader.close(); } } if (exitValue == 0) { return IJobHandler.SUCCESS; } else { return new ReturnT&lt;String&gt;(IJobHandler.FAIL.getCode(), &quot;command exit value(&quot;+exitValue+&quot;) is failed&quot;); } } /** * 4、跨平台Http任务 * 参数示例： * &quot;url: http://www.baidu.com\\n&quot; + * &quot;method: get\\n&quot; + * &quot;data: content\\n&quot;; */ @XxlJob(&quot;httpJobHandler&quot;) public ReturnT&lt;String&gt; httpJobHandler(String param) throws Exception { // param parse if (param==null || param.trim().length()==0) { XxlJobLogger.log(&quot;param[&quot;+ param +&quot;] invalid.&quot;); return ReturnT.FAIL; } String[] httpParams = param.split(&quot;\\n&quot;); String url = null; String method = null; String data = null; for (String httpParam: httpParams) { if (httpParam.startsWith(&quot;url:&quot;)) { url = httpParam.substring(httpParam.indexOf(&quot;url:&quot;) + 4).trim(); } if (httpParam.startsWith(&quot;method:&quot;)) { method = httpParam.substring(httpParam.indexOf(&quot;method:&quot;) + 7).trim().toUpperCase(); } if (httpParam.startsWith(&quot;data:&quot;)) { data = httpParam.substring(httpParam.indexOf(&quot;data:&quot;) + 5).trim(); } } // param valid if (url==null || url.trim().length()==0) { XxlJobLogger.log(&quot;url[&quot;+ url +&quot;] invalid.&quot;); return ReturnT.FAIL; } if (method==null || !Arrays.asList(&quot;GET&quot;, &quot;POST&quot;).contains(method)) { XxlJobLogger.log(&quot;method[&quot;+ method +&quot;] invalid.&quot;); return ReturnT.FAIL; } // request HttpURLConnection connection = null; BufferedReader bufferedReader = null; try { // connection URL realUrl = new URL(url); connection = (HttpURLConnection) realUrl.openConnection(); // connection setting connection.setRequestMethod(method); connection.setDoOutput(true); connection.setDoInput(true); connection.setUseCaches(false); connection.setReadTimeout(5 * 1000); connection.setConnectTimeout(3 * 1000); connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;); connection.setRequestProperty(&quot;Content-Type&quot;, &quot;application/json;charset=UTF-8&quot;); connection.setRequestProperty(&quot;Accept-Charset&quot;, &quot;application/json;charset=UTF-8&quot;); // do connection connection.connect(); // data if (data!=null &amp;&amp; data.trim().length()&gt;0) { DataOutputStream dataOutputStream = new DataOutputStream(connection.getOutputStream()); dataOutputStream.write(data.getBytes(&quot;UTF-8&quot;)); dataOutputStream.flush(); dataOutputStream.close(); } // valid StatusCode int statusCode = connection.getResponseCode(); if (statusCode != 200) { throw new RuntimeException(&quot;Http Request StatusCode(&quot; + statusCode + &quot;) Invalid.&quot;); } // result bufferedReader = new BufferedReader(new InputStreamReader(connection.getInputStream(), &quot;UTF-8&quot;)); StringBuilder result = new StringBuilder(); String line; while ((line = bufferedReader.readLine()) != null) { result.append(line); } String responseMsg = result.toString(); XxlJobLogger.log(responseMsg); return ReturnT.SUCCESS; } catch (Exception e) { XxlJobLogger.log(e); return ReturnT.FAIL; } finally { try { if (bufferedReader != null) { bufferedReader.close(); } if (connection != null) { connection.disconnect(); } } catch (Exception e2) { XxlJobLogger.log(e2); } } } /** * 5、生命周期任务示例：任务初始化与销毁时，支持自定义相关逻辑； */ @XxlJob(value = &quot;demoJobHandler2&quot;, init = &quot;init&quot;, destroy = &quot;destroy&quot;) public ReturnT&lt;String&gt; demoJobHandler2(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); return ReturnT.SUCCESS; } public void init(){ logger.info(&quot;init&quot;); } public void destroy(){ logger.info(&quot;destory&quot;); }} 添加执行器 登录admin 点击左侧执行器管理–新增 AppName填写xxl-job-executor-sample,名称根据实际情况输入,注册方式选择自动注册,保存. 刷新列表即可看到OnLine机器地址 添加任务 点击新增按钮 执行器选中xxl-job-executor-sample 根据实际情况填写,其中JobHandler填写@XxlJob的值,如案例中的demoJobHandler 保存","link":"/xxl-job/"},{"title":"命令行的艺术","text":"熟练使用命令行是一种常常被忽视，或被认为难以掌握的技能，但实际上，它会提高你作为工程师的灵活性以及生产力。本文是一份我在 Linux 上工作时，发现的一些命令行使用技巧的摘要。有些技巧非常基础，而另一些则相当复杂，甚至晦涩难懂。这篇文章并不长，但当你能够熟练掌握这里列出的所有技巧时，你就学会了很多关于命令行的东西了。 前言涵盖范围： 这篇文章不仅能帮助刚接触命令行的新手，而且对具有经验的人也大有裨益。本文致力于做到覆盖面广（涉及所有重要的内容），具体（给出具体的最常用的例子），以及简洁（避免冗余的内容，或是可以在其他地方轻松查到的细枝末节）。在特定应用场景下，本文的内容属于基本功或者能帮助您节约大量的时间。 本文主要为 Linux 所写，但在仅限 OS X 系统章节和仅限 Windows 系统章节中也包含有对应操作系统的内容。除去这两个章节外，其它的内容大部分均可在其他类 Unix 系统或 OS X，甚至 Cygwin 中得到应用。 本文主要关注于交互式 Bash，但也有很多技巧可以应用于其他 shell 和 Bash 脚本当中。 除去“标准的”Unix 命令，本文还包括了一些依赖于特定软件包的命令（前提是它们具有足够的价值）。 注意事项： 为了能在一页内展示尽量多的东西，一些具体的信息可以在引用的页面中找到。我们相信机智的你知道如何使用 Google 或者其他搜索引擎来查阅到更多的详细信息。文中部分命令需要您使用 apt-get，yum，dnf，pacman，pip 或 brew（以及其它合适的包管理器）来安装依赖的程序。 遇到问题的话，请尝试使用 Explainshell 去获取相关命令、参数、管道等内容的解释。 基础 学习 Bash 的基础知识。具体地，在命令行中输入 man bash 并至少全文浏览一遍; 它理解起来很简单并且不冗长。其他的 shell 可能很好用，但 Bash 的功能已经足够强大并且到几乎总是可用的（ 如果你只学习 zsh，fish 或其他的 shell 的话，在你自己的设备上会显得很方便，但过度依赖这些功能会给您带来不便，例如当你需要在服务器上工作时）。 熟悉至少一个基于文本的编辑器。通常而言 Vim （vi） 会是你最好的选择，毕竟在终端中编辑文本时 Vim 是最好用的工具（甚至大部分情况下 Vim 要比 Emacs、大型 IDE 或是炫酷的编辑器更好用）。 学会如何使用 man 命令去阅读文档。学会使用 apropos 去查找文档。知道有些命令并不对应可执行文件，而是在 Bash 内置好的，此时可以使用 help 和 help -d 命令获取帮助信息。你可以用 type 命令 来判断这个命令到底是可执行文件、shell 内置命令还是别名。 学会使用 &gt; 和 &lt; 来重定向输出和输入，学会使用 | 来重定向管道。明白 &gt; 会覆盖了输出文件而 &gt;&gt; 是在文件末添加。了解标准输出 stdout 和标准错误 stderr。 学会使用通配符 * （或许再算上 ? 和 […]） 和引用以及引用中 ' 和 &quot; 的区别（后文中有一些具体的例子）。 熟悉 Bash 中的任务管理工具：&amp;，ctrl-z，ctrl-c，jobs，fg，bg，kill 等。 学会使用 ssh 进行远程命令行登录，最好知道如何使用 ssh-agent，ssh-add 等命令来实现基础的无密码认证登录。 学会基本的文件管理工具：ls 和 ls -l （了解 ls -l 中每一列代表的意义），less，head，tail 和 tail -f （甚至 less +F），ln 和 ln -s （了解硬链接与软链接的区别），chown，chmod，du （硬盘使用情况概述：du -hs *）。 关于文件系统的管理，学习 df，mount，fdisk，mkfs，lsblk。知道 inode 是什么（与 ls -i 和 df -i 等命令相关）。 学习基本的网络管理工具：ip 或 ifconfig，dig。 学习并使用一种版本控制管理系统，例如 git。 熟悉正则表达式，学会使用 grep／egrep，它们的参数中 -i，-o，-v，-A，-B 和 -C 这些是很常用并值得认真学习的。 学会使用 apt-get，yum，dnf 或 pacman （具体使用哪个取决于你使用的 Linux 发行版）来查找和安装软件包。并确保你的环境中有 pip 来安装基于 Python 的命令行工具 （接下来提到的部分程序使用 pip 来安装会很方便）。 日常使用 在 Bash 中，可以通过按 Tab 键实现自动补全参数，使用 ctrl-r 搜索命令行历史记录（按下按键之后，输入关键字便可以搜索，重复按下 ctrl-r 会向后查找匹配项，按下 Enter 键会执行当前匹配的命令，而按下右方向键会将匹配项放入当前行中，不会直接执行，以便做出修改）。 在 Bash 中，可以按下 ctrl-w 删除你键入的最后一个单词，ctrl-u 可以删除行内光标所在位置之前的内容，alt-b 和 alt-f 可以以单词为单位移动光标，ctrl-a 可以将光标移至行首，ctrl-e 可以将光标移至行尾，ctrl-k 可以删除光标至行尾的所有内容，ctrl-l 可以清屏。键入 man readline 可以查看 Bash 中的默认快捷键。内容有很多，例如 alt-. 循环地移向前一个参数，而 *alt-** 可以展开通配符。 你喜欢的话，可以执行 set -o vi 来使用 vi 风格的快捷键，而执行 set -o emacs 可以把它改回来。 为了便于编辑长命令，在设置你的默认编辑器后（例如 export EDITOR=vim），ctrl-x ctrl-e 会打开一个编辑器来编辑当前输入的命令。在 vi 风格下快捷键则是 escape-v。 键入 history 查看命令行历史记录，再用 !n（n 是命令编号）就可以再次执行。其中有许多缩写，最有用的大概就是 !$， 它用于指代上次键入的参数，而 !! 可以指代上次键入的命令了（参考 man 页面中的“HISTORY EXPANSION”）。不过这些功能，你也可以通过快捷键 ctrl-r 和 alt-. 来实现。 cd 命令可以切换工作路径，输入 cd ~ 可以进入 home 目录。要访问你的 home 目录中的文件，可以使用前缀 ~（例如 ~/.bashrc）。在 sh 脚本里则用环境变量 $HOME 指代 home 目录的路径。 回到前一个工作路径：cd -。 如果你输入命令的时候中途改了主意，按下 alt-# 在行首添加 # 把它当做注释再按下回车执行（或者依次按下 ctrl-a， #**， **enter）。这样做的话，之后借助命令行历史记录，你可以很方便恢复你刚才输入到一半的命令。 使用 xargs （ 或 parallel）。他们非常给力。注意到你可以控制每行参数个数（-L）和最大并行数（-P）。如果你不确定它们是否会按你想的那样工作，先使用 xargs echo 查看一下。此外，使用 -I{} 会很方便。例如： 12find . -name '*.py' | xargs grep some_functioncat hosts | xargs -I{} ssh root@{} hostname pstree -p 以一种优雅的方式展示进程树。 使用 pgrep 和 pkill 根据名字查找进程或发送信号（-f 参数通常有用）。 了解你可以发往进程的信号的种类。比如，使用 kill -STOP [pid] 停止一个进程。使用 man 7 signal 查看详细列表。 使用 nohup 或 disown 使一个后台进程持续运行。 使用 netstat -lntp 或 ss -plat 检查哪些进程在监听端口（默认是检查 TCP 端口; 添加参数 -u 则检查 UDP 端口）或者 lsof -iTCP -sTCP:LISTEN -P -n (这也可以在 OS X 上运行)。 lsof 来查看开启的套接字和文件。 使用 uptime 或 w 来查看系统已经运行多长时间。 使用 alias 来创建常用命令的快捷形式。例如：alias ll='ls -latr' 创建了一个新的命令别名 ll。 可以把别名、shell 选项和常用函数保存在 ~/.bashrc，具体看下这篇文章。这样做的话你就可以在所有 shell 会话中使用你的设定。 把环境变量的设定以及登陆时要执行的命令保存在 ~/.bash_profile。而对于从图形界面启动的 shell 和 cron 启动的 shell，则需要单独配置文件。 要想在几台电脑中同步你的配置文件（例如 .bashrc 和 .bash_profile），可以借助 Git。 当变量和文件名中包含空格的时候要格外小心。Bash 变量要用引号括起来，比如 &quot;$FOO&quot;。尽量使用 -0 或 -print0 选项以便用 NULL 来分隔文件名，例如 locate -0 pattern | xargs -0 ls -al 或 find / -print0 -type d | xargs -0 ls -al。如果 for 循环中循环访问的文件名含有空字符（空格、tab 等字符），只需用 IFS=$'\\n' 把内部字段分隔符设为换行符。 在 Bash 脚本中，使用 set -x 去调试输出（或者使用它的变体 set -v，它会记录原始输入，包括多余的参数和注释）。尽可能地使用严格模式：使用 set -e 令脚本在发生错误时退出而不是继续运行；使用 set -u 来检查是否使用了未赋值的变量；试试 set -o pipefail，它可以监测管道中的错误。当牵扯到很多脚本时，使用 trap 来检测 ERR 和 EXIT。一个好的习惯是在脚本文件开头这样写，这会使它能够检测一些错误，并在错误发生时中断程序并输出信息： 12set -euo pipefailtrap &quot;echo 'error: Script failed: see failed command above'&quot; ERR 在 Bash 脚本中，子 shell（使用括号 (...)）是一种组织参数的便捷方式。一个常见的例子是临时地移动工作路径，代码如下： 123# do something in current dir(cd /some/other/dir &amp;&amp; other-command)# continue in original dir 在 Bash 中，变量有许多的扩展方式。${name:?error message} 用于检查变量是否存在。此外，当 Bash 脚本只需要一个参数时，可以使用这样的代码 input_file=${1:?usage: $0 input_file}。在变量为空时使用默认值：${name:-default}。如果你要在之前的例子中再加一个（可选的）参数，可以使用类似这样的代码 output_file=${2:-logfile}，如果省略了 $2，它的值就为空，于是 output_file 就会被设为 logfile。数学表达式：i=$(( (i + 1) % 5 ))。序列：{1..10}。截断字符串：${var%suffix} 和 ${var#prefix}。例如，假设 var=foo.pdf，那么 echo ${var%.pdf}.txt 将输出 foo.txt。 使用括号扩展（{…}）来减少输入相似文本，并自动化文本组合。这在某些情况下会很有用，例如 mv foo.{txt,pdf} some-dir（同时移动两个文件），cp somefile{,.bak}（会被扩展成 cp somefile somefile.bak）或者 mkdir -p test-{a,b,c}/subtest-{1,2,3}（会被扩展成所有可能的组合，并创建一个目录树）。 通过使用 &lt;(some command) 可以将输出视为文件。例如，对比本地文件 /etc/hosts 和一个远程文件： 1diff /etc/hosts &lt;(ssh somehost cat /etc/hosts) 编写脚本时，你可能会想要把代码都放在大括号里。缺少右括号的话，代码就会因为语法错误而无法执行。如果你的脚本是要放在网上分享供他人使用的，这样的写法就体现出它的好处了，因为这样可以防止下载不完全代码被执行。 123{ # 在这里写代码} 了解 Bash 中的“here documents”，例如 cat &lt;&lt;EOF ...。 在 Bash 中，同时重定向标准输出和标准错误：some-command &gt;logfile 2&gt;&amp;1 或者 some-command &amp;&gt;logfile。通常，为了保证命令不会在标准输入里残留一个未关闭的文件句柄捆绑在你当前所在的终端上，在命令后添加 &lt;/dev/null 是一个好习惯。 使用 man ascii 查看具有十六进制和十进制值的ASCII表。man unicode，man utf-8，以及 man latin1 有助于你去了解通用的编码信息。 使用 screen 或 tmux 来使用多份屏幕，当你在使用 ssh 时（保存 session 信息）将尤为有用。而 byobu 可以为它们提供更多的信息和易用的管理工具。另一个轻量级的 session 持久化解决方案是 dtach。 ssh 中，了解如何使用 -L 或 -D（偶尔需要用 -R）开启隧道是非常有用的，比如当你需要从一台远程服务器上访问 web 页面。 对 ssh 设置做一些小优化可能是很有用的，例如这个 ~/.ssh/config 文件包含了防止特定网络环境下连接断开、压缩数据、多通道等选项： 1234567TCPKeepAlive=yesServerAliveInterval=15ServerAliveCountMax=6Compression=yesControlMaster autoControlPath /tmp/%r@%h:%pControlPersist yes 一些其他的关于 ssh 的选项是与安全相关的，应当小心翼翼的使用。例如你应当只能在可信任的网络中启用 StrictHostKeyChecking=no，ForwardAgent=yes。 考虑使用 mosh 作为 ssh 的替代品，它使用 UDP 协议。它可以避免连接被中断并且对带宽需求更小，但它需要在服务端做相应的配置。 获取八进制形式的文件访问权限（修改系统设置时通常需要，但 ls 的功能不那么好用并且通常会搞砸），可以使用类似如下的代码： 1stat -c '%A %a %n' /etc/timezone 使用 percol 或者 fzf 可以交互式地从另一个命令输出中选取值。 使用 fpp（PathPicker）可以与基于另一个命令(例如 git）输出的文件交互。 将 web 服务器上当前目录下所有的文件（以及子目录）暴露给你所处网络的所有用户，使用：python -m SimpleHTTPServer 7777 （使用端口 7777 和 Python 2）或python -m http.server 7777 （使用端口 7777 和 Python 3）。 以其他用户的身份执行命令，使用 sudo。默认以 root 用户的身份执行；使用 -u 来指定其他用户。使用 -i 来以该用户登录（需要输入_你自己的_密码）。 将 shell 切换为其他用户，使用 su username 或者 sudo - username。加入 - 会使得切换后的环境与使用该用户登录后的环境相同。省略用户名则默认为 root。切换到哪个用户，就需要输入_哪个用户的_密码。 了解命令行的 128K 限制。使用通配符匹配大量文件名时，常会遇到“Argument list too long”的错误信息。（这种情况下换用 find 或 xargs 通常可以解决。） 当你需要一个基本的计算器时，可以使用 python 解释器（当然你要用 python 的时候也是这样）。例如： 12&gt;&gt;&gt; 2+35 文件及数据处理 在当前目录下通过文件名查找一个文件，使用类似于这样的命令：find . -iname '*something*'。在所有路径下通过文件名查找文件，使用 locate something （但注意到 updatedb 可能没有对最近新建的文件建立索引，所以你可能无法定位到这些未被索引的文件）。 使用 ag 在源代码或数据文件里检索（grep -r 同样可以做到，但相比之下 ag 更加先进）。 将 HTML 转为文本：lynx -dump -stdin。 Markdown，HTML，以及所有文档格式之间的转换，试试 pandoc。 当你要处理棘手的 XML 时候，xmlstarlet 算是上古时代流传下来的神器。 使用 jq 处理 JSON。 使用 shyaml 处理 YAML。 要处理 Excel 或 CSV 文件的话，csvkit 提供了 in2csv，csvcut，csvjoin，csvgrep 等方便易用的工具。 当你要处理 Amazon S3 相关的工作的时候，s3cmd 是一个很方便的工具而 s4cmd 的效率更高。Amazon 官方提供的 aws 以及 saws 是其他 AWS 相关工作的基础，值得学习。 了解如何使用 sort 和 uniq，包括 uniq 的 -u 参数和 -d 参数，具体内容在后文单行脚本节中。另外可以了解一下 comm。 了解如何使用 cut，paste 和 join 来更改文件。很多人都会使用 cut，但遗忘了 join。 了解如何运用 wc 去计算新行数（-l），字符数（-m），单词数（-w）以及字节数（-c）。 了解如何使用 tee 将标准输入复制到文件甚至标准输出，例如 ls -al | tee file.txt。 要进行一些复杂的计算，比如分组、逆序和一些其他的统计分析，可以考虑使用 datamash。 注意到语言设置（中文或英文等）对许多命令行工具有一些微妙的影响，比如排序的顺序和性能。大多数 Linux 的安装过程会将 LANG 或其他有关的变量设置为符合本地的设置。要意识到当你改变语言设置时，排序的结果可能会改变。明白国际化可能会使 sort 或其他命令运行效率下降许多倍。某些情况下（例如集合运算）你可以放心的使用 export LC_ALL=C 来忽略掉国际化并按照字节来判断顺序。 你可以单独指定某一条命令的环境，只需在调用时把环境变量设定放在命令的前面，例如 TZ=Pacific/Fiji date 可以获取斐济的时间。 了解如何使用 awk 和 sed 来进行简单的数据处理。 参阅 One-liners 获取示例。 替换一个或多个文件中出现的字符串： 1perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt 使用 repren 来批量重命名文件，或是在多个文件中搜索替换内容。（有些时候 rename 命令也可以批量重命名，但要注意，它在不同 Linux 发行版中的功能并不完全一样。） 123456# 将文件、目录和内容全部重命名 foo -&gt; bar:repren --full --preserve-case --from foo --to bar .# 还原所有备份文件 whatever.bak -&gt; whatever:repren --renames --from '(.*)\\.bak' --to '\\1' *.bak# 用 rename 实现上述功能（若可用）:rename 's/\\.bak$//' *.bak 根据 man 页面的描述，rsync 是一个快速且非常灵活的文件复制工具。它闻名于设备之间的文件同步，但其实它在本地情况下也同样有用。在安全设置允许下，用 rsync 代替 scp 可以实现文件续传，而不用重新从头开始。它同时也是删除大量文件的最快方法之一： 1mkdir empty &amp;&amp; rsync -r --delete empty/ some-dir &amp;&amp; rmdir some-dir 若要在复制文件时获取当前进度，可使用 pv，pycp，progress，rsync --progress。若所执行的复制为block块拷贝，可以使用 dd status=progress。 使用 shuf 可以以行为单位来打乱文件的内容或从一个文件中随机选取多行。 了解 sort 的参数。显示数字时，使用 -n 或者 -h 来显示更易读的数（例如 du -h 的输出）。明白排序时关键字的工作原理（-t 和 -k）。例如，注意到你需要 -k1，1 来仅按第一个域来排序，而 -k1 意味着按整行排序。稳定排序（sort -s）在某些情况下很有用。例如，以第二个域为主关键字，第一个域为次关键字进行排序，你可以使用 sort -k1，1 | sort -s -k2，2。 如果你想在 Bash 命令行中写 tab 制表符，按下 ctrl-v [Tab] 或键入 $'\\t' （后者可能更好，因为你可以复制粘贴它）。 标准的源代码对比及合并工具是 diff 和 patch。使用 diffstat 查看变更总览数据。注意到 diff -r 对整个文件夹有效。使用 diff -r tree1 tree2 | diffstat 查看变更的统计数据。vimdiff 用于比对并编辑文件。 对于二进制文件，使用 hd，hexdump 或者 xxd 使其以十六进制显示，使用 bvi，hexedit 或者 biew 来进行二进制编辑。 同样对于二进制文件，strings（包括 grep 等工具）可以帮助在二进制文件中查找特定比特。 制作二进制差分文件（Delta 压缩），使用 xdelta3。 使用 iconv 更改文本编码。需要更高级的功能，可以使用 uconv，它支持一些高级的 Unicode 功能。例如，这条命令移除了所有重音符号： 1uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] &gt;; ::Any-NFC; ' &lt; input.txt &gt; output.txt 拆分文件可以使用 split（按大小拆分）和 csplit（按模式拆分）。 操作日期和时间表达式，可以用 dateutils 中的 dateadd、datediff、strptime 等工具。 使用 zless、zmore、zcat 和 zgrep 对压缩过的文件进行操作。 文件属性可以通过 chattr 进行设置，它比文件权限更加底层。例如，为了保护文件不被意外删除，可以使用不可修改标记：sudo chattr +i /critical/directory/or/file 使用 getfacl 和 setfacl 以保存和恢复文件权限。例如： 12getfacl -R /some/path &gt; permissions.txtsetfacl --restore=permissions.txt 为了高效地创建空文件，请使用 truncate（创建稀疏文件），fallocate（用于 ext4，xfs，btrf 和 ocfs2 文件系统），xfs_mkfile（适用于几乎所有的文件系统，包含在 xfsprogs 包中），mkfile（用于类 Unix 操作系统，比如 Solaris 和 Mac OS）。 系统调试 curl 和 curl -I 可以被轻松地应用于 web 调试中，它们的好兄弟 wget 也是如此，或者也可以试试更潮的 httpie。 获取 CPU 和硬盘的使用状态，通常使用使用 top（htop 更佳），iostat 和 iotop。而 iostat -mxz 15 可以让你获悉 CPU 和每个硬盘分区的基本信息和性能表现。 使用 netstat 和 ss 查看网络连接的细节。 dstat 在你想要对系统的现状有一个粗略的认识时是非常有用的。然而若要对系统有一个深度的总体认识，使用 glances，它会在一个终端窗口中向你提供一些系统级的数据。 若要了解内存状态，运行并理解 free 和 vmstat 的输出。值得留意的是“cached”的值，它指的是 Linux 内核用来作为文件缓存的内存大小，而与空闲内存无关。 Java 系统调试则是一件截然不同的事，一个可以用于 Oracle 的 JVM 或其他 JVM 上的调试的技巧是你可以运行 kill -3 &lt;pid&gt; 同时一个完整的栈轨迹和堆概述（包括 GC 的细节）会被保存到标准错误或是日志文件。JDK 中的 jps，jstat，jstack，jmap 很有用。SJK tools 更高级。 使用 mtr 去跟踪路由，用于确定网络问题。 用 ncdu 来查看磁盘使用情况，它比寻常的命令，如 du -sh *，更节省时间。 查找正在使用带宽的套接字连接或进程，使用 iftop 或 nethogs。 ab 工具（Apache 中自带）可以简单粗暴地检查 web 服务器的性能。对于更复杂的负载测试，使用 siege。 wireshark，tshark 和 ngrep 可用于复杂的网络调试。 了解 strace 和 ltrace。这俩工具在你的程序运行失败、挂起甚至崩溃，而你却不知道为什么或你想对性能有个总体的认识的时候是非常有用的。注意 profile 参数（-c）和附加到一个运行的进程参数 （-p）。 了解使用 ldd 来检查共享库。但是永远不要在不信任的文件上运行。 了解如何运用 gdb 连接到一个运行着的进程并获取它的堆栈轨迹。 学会使用 /proc。它在调试正在出现的问题的时候有时会效果惊人。比如：/proc/cpuinfo，/proc/meminfo，/proc/cmdline，/proc/xxx/cwd，/proc/xxx/exe，/proc/xxx/fd/，/proc/xxx/smaps（这里的 xxx 表示进程的 id 或 pid）。 当调试一些之前出现的问题的时候，sar 非常有用。它展示了 cpu、内存以及网络等的历史数据。 关于更深层次的系统分析以及性能分析，看看 stap（SystemTap），perf，以及sysdig。 查看你当前使用的系统，使用 uname，uname -a（Unix／kernel 信息）或者 lsb_release -a（Linux 发行版信息）。 无论什么东西工作得很欢乐（可能是硬件或驱动问题）时可以试试 dmesg。 如果你删除了一个文件，但通过 du 发现没有释放预期的磁盘空间，请检查文件是否被进程占用：lsof | grep deleted | grep &quot;filename-of-my-big-file&quot; 单行脚本一些命令组合的例子： 当你需要对文本文件做集合交、并、差运算时，sort 和 uniq 会是你的好帮手。具体例子请参照代码后面的，此处假设 a 与 b 是两内容不同的文件。这种方式效率很高，并且在小文件和上 G 的文件上都能运用（注意尽管在 /tmp 在一个小的根分区上时你可能需要 -T 参数，但是实际上 sort 并不被内存大小约束），参阅前文中关于 LC_ALL 和 sort 的 -u 参数的部分。 123sort a b | uniq &gt; c # c 是 a 并 bsort a b | uniq -d &gt; c # c 是 a 交 bsort a b b | uniq -u &gt; c # c 是 a - b 使用 grep . *（每行都会附上文件名）或者 head -100 *（每个文件有一个标题）来阅读检查目录下所有文件的内容。这在检查一个充满配置文件的目录（如 /sys、/proc、/etc）时特别好用。 计算文本文件第三列中所有数的和（可能比同等作用的 Python 代码快三倍且代码量少三倍）： 1awk '{ x += $3 } END { print x }' myfile 如果你想在文件树上查看大小/日期，这可能看起来像递归版的 ls -l 但比 ls -lR 更易于理解： 1find . -type f -ls 假设你有一个类似于 web 服务器日志文件的文本文件，并且一个确定的值只会出现在某些行上，假设一个 acct_id 参数在 URI 中。如果你想计算出每个 acct_id 值有多少次请求，使用如下代码： 1egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn 要持续监测文件改动，可以使用 watch，例如检查某个文件夹中文件的改变，可以用 watch -d -n 2 'ls -rtlh | tail'；或者在排查 WiFi 设置故障时要监测网络设置的更改，可以用 watch -d -n 2 ifconfig。 运行这个函数从这篇文档中随机获取一条技巧（解析 Markdown 文件并抽取项目）： 12345678function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README-zh.md| pandoc -f markdown -t html | iconv -f 'utf-8' -t 'unicode' | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v &quot;(html/body/ul/li[count(p)&gt;0])[$RANDOM mod last()+1]&quot; | xmlstarlet unesc | fmt -80} 冷门但有用 expr：计算表达式或正则匹配 m4：简单的宏处理器 yes：多次打印字符串 cal：漂亮的日历 env：执行一个命令（脚本文件中很有用） printenv：打印环境变量（调试时或在写脚本文件时很有用） look：查找以特定字符串开头的单词或行 cut，paste 和 join：数据修改 fmt：格式化文本段落 pr：将文本格式化成页／列形式 fold：包裹文本中的几行 column：将文本格式化成多个对齐、定宽的列或表格 expand 和 unexpand：制表符与空格之间转换 nl：添加行号 seq：打印数字 bc：计算器 factor：分解因数 gpg：加密并签名文件 toe：terminfo 入口列表 nc：网络调试及数据传输 socat：套接字代理，与 netcat 类似 slurm：网络流量可视化 dd：文件或设备间传输数据 file：确定文件类型 tree：以树的形式显示路径和文件，类似于递归的 ls stat：文件信息 time：执行命令，并计算执行时间 timeout：在指定时长范围内执行命令，并在规定时间结束后停止进程 lockfile：使文件只能通过 rm -f 移除 logrotate： 切换、压缩以及发送日志文件 watch：重复运行同一个命令，展示结果并／或高亮有更改的部分 when-changed：当检测到文件更改时执行指定命令。参阅 inotifywait 和 entr。 tac：反向输出文件 shuf：文件中随机选取几行 comm：一行一行的比较排序过的文件 strings：从二进制文件中抽取文本 tr：转换字母 iconv 或 uconv：文本编码转换 split 和 csplit：分割文件 sponge：在写入前读取所有输入，在读取文件后再向同一文件写入时比较有用，例如 grep -v something some-file | sponge some-file units：将一种计量单位转换为另一种等效的计量单位（参阅 /usr/share/units/definitions.units） apg：随机生成密码 xz：高比例的文件压缩 ldd：动态库信息 nm：提取 obj 文件中的符号 ab 或 wrk：web 服务器性能分析 strace：调试系统调用 mtr：更好的网络调试跟踪工具 cssh：可视化的并发 shell rsync：通过 ssh 或本地文件系统同步文件和文件夹 wireshark 和 tshark：抓包和网络调试工具 ngrep：网络层的 grep host 和 dig：DNS 查找 lsof：列出当前系统打开文件的工具以及查看端口信息 dstat：系统状态查看 glances：高层次的多子系统总览 iostat：硬盘使用状态 mpstat： CPU 使用状态 vmstat： 内存使用状态 htop：top 的加强版 last：登入记录 w：查看处于登录状态的用户 id：用户/组 ID 信息 sar：系统历史数据 iftop 或 nethogs：套接字及进程的网络利用情况 ss：套接字数据 dmesg：引导及系统错误信息 sysctl： 在内核运行时动态地查看和修改内核的运行参数 hdparm：SATA/ATA 磁盘更改及性能分析 lsblk：列出块设备信息：以树形展示你的磁盘以及磁盘分区信息 lshw，lscpu，lspci，lsusb 和 dmidecode：查看硬件信息，包括 CPU、BIOS、RAID、显卡、USB设备等 lsmod 和 modinfo：列出内核模块，并显示其细节 fortune，ddate 和 sl：额，这主要取决于你是否认为蒸汽火车和莫名其妙的名人名言是否“有用” 仅限 OS X 系统以下是仅限于 OS X 系统的技巧。 用 brew （Homebrew）或者 port （MacPorts）进行包管理。这些可以用来在 OS X 系统上安装以上的大多数命令。 用 pbcopy 复制任何命令的输出到桌面应用，用 pbpaste 粘贴输入。 若要在 OS X 终端中将 Option 键视为 alt 键（例如在上面介绍的 alt-b、alt-f 等命令中用到），打开 偏好设置 -&gt; 描述文件 -&gt; 键盘 并勾选“使用 Option 键作为 Meta 键”。 用 open 或者 open -a /Applications/Whatever.app 使用桌面应用打开文件。 Spotlight：用 mdfind 搜索文件，用 mdls 列出元数据（例如照片的 EXIF 信息）。 注意 OS X 系统是基于 BSD UNIX 的，许多命令（例如 ps，ls，tail，awk，sed）都和 Linux 中有微妙的不同（ Linux 很大程度上受到了 System V-style Unix 和 GNU 工具影响）。你可以通过标题为 “BSD General Commands Manual” 的 man 页面发现这些不同。在有些情况下 GNU 版本的命令也可能被安装（例如 gawk 和 gsed 对应 GNU 中的 awk 和 sed ）。如果要写跨平台的 Bash 脚本，避免使用这些命令（例如，考虑 Python 或者 perl ）或者经过仔细的测试。 用 sw_vers 获取 OS X 的版本信息。 仅限 Windows 系统以下是仅限于 Windows 系统的技巧。 在 Winodws 下获取 Unix 工具 可以安装 Cygwin 允许你在 Microsoft Windows 中体验 Unix shell 的威力。这样的话，本文中介绍的大多数内容都将适用。 在 Windows 10 上，你可以使用 Bash on Ubuntu on Windows，它提供了一个熟悉的 Bash 环境，包含了不少 Unix 命令行工具。好处是它允许 Linux 上编写的程序在 Windows 上运行，而另一方面，Windows 上编写的程序却无法在 Bash 命令行中运行。 如果你在 Windows 上主要想用 GNU 开发者工具（例如 GCC），可以考虑 MinGW 以及它的 MSYS 包，这个包提供了例如 bash，gawk，make 和 grep 的工具。MSYS 并不包含所有可以与 Cygwin 媲美的特性。当制作 Unix 工具的原生 Windows 端口时 MinGW 将特别地有用。 另一个在 Windows 下实现接近 Unix 环境外观效果的选项是 Cash。注意在此环境下只有很少的 Unix 命令和命令行可用。 实用 Windows 命令行工具 可以使用 wmic 在命令行环境下给大部分 Windows 系统管理任务编写脚本以及执行这些任务。 Windows 实用的原生命令行网络工具包括 ping，ipconfig，tracert，和 netstat。 可以使用 Rundll32 命令来实现许多有用的 Windows 任务 。 Cygwin 技巧 通过 Cygwin 的包管理器来安装额外的 Unix 程序。 使用 mintty 作为你的命令行窗口。 要访问 Windows 剪贴板，可以通过 /dev/clipboard。 运行 cygstart 以通过默认程序打开一个文件。 要访问 Windows 注册表，可以使用 regtool。 注意 Windows 驱动器路径 C:\\ 在 Cygwin 中用 /cygdrive/c 代表，而 Cygwin 的 / 代表 Windows 中的 C:\\cygwin。要转换 Cygwin 和 Windows 风格的路径可以用 cygpath。这在需要调用 Windows 程序的脚本里很有用。 学会使用 wmic，你就可以从命令行执行大多数 Windows 系统管理任务，并编成脚本。 要在 Windows 下获得 Unix 的界面和体验，另一个办法是使用 Cash。需要注意的是，这个环境支持的 Unix 命令和命令行参数非常少。 要在 Windows 上获取 GNU 开发者工具（比如 GCC）的另一个办法是使用 MinGW 以及它的 MSYS 软件包，该软件包提供了 bash、gawk、make、grep 等工具。然而 MSYS 提供的功能没有 Cygwin 完善。MinGW 在创建 Unix 工具的 Windows 原生移植方面非常有用。 更多资源 awesome-shell：一份精心组织的命令行工具及资源的列表。 awesome-osx-command-line：一份针对 OS X 命令行的更深入的指南。 Strict mode：为了编写更好的脚本文件。 shellcheck：一个静态 shell 脚本分析工具，本质上是 bash／sh／zsh 的 lint。 Filenames and Pathnames in Shell：有关如何在 shell 脚本里正确处理文件名的细枝末节。 Data Science at the Command Line：用于数据科学的一些命令和工具，摘自同名书籍。","link":"/the-art-of-command-line/"},{"title":"Windows系统一行命令激活","text":"windows一键激活。 步骤 以管理员身份打开命令提示符 运行激活命令 1slmgr /skms kms.v0v.bid &amp;&amp; slmgr /ato 查看激活状态 1slmgr.vbs -dlv 参考https://github.com/dylanbai8/kmspro","link":"/windows-active/"},{"title":"RabbitMQ七种队列模式介绍与应用场景","text":"RabbitMQ七种队列模式介绍与应用场景（通俗易懂） 七种模式介绍与应用场景简单模式（Hello World） 做最简单的事情，一个生产者对应一个消费者，RabbitMQ相当于一个消息代理，负责将A的消息转发给B 应用场景： 将发送的电子邮件放到消息队列，然后邮件服务在队列中获取邮件并发送给收件人 工作队列模式（Work queues） 在多个消费者之间分配任务（竞争的消费者模式），一个生产者对应多个消费者，一般适用于执行资源密集型任务，单个消费者处理不过来，需要多个消费者进行处理 应用场景： 一个订单的处理需要10s，有多个订单可以同时放到消息队列，然后让多个消费者同时处理，这样就是并行了，而不是单个消费者的串行情况 订阅模式（Publish/Subscribe） 一次向许多消费者发送消息，一个生产者发送的消息会被多个消费者获取，也就是将消息将广播到所有的消费者中。 应用场景： 更新商品库存后需要通知多个缓存和多个数据库，这里的结构应该是： 一个fanout类型交换机扇出两个个消息队列，分别为缓存消息队列、数据库消息队列 一个缓存消息队列对应着多个缓存消费者 一个数据库消息队列对应着多个数据库消费者 路由模式（Routing） 有选择地（Routing key）接收消息，发送消息到交换机并且要指定路由key ，消费者将队列绑定到交换机时需要指定路由key，仅消费指定路由key的消息 应用场景： 如在商品库存中增加了1台iphone12，iphone12促销活动消费者指定routing key为iphone12，只有此促销活动会接收到消息，其它促销活动不关心也不会消费此routing key的消息 主题模式（Topics） 根据主题（Topics）来接收消息，将路由key和某模式进行匹配，此时队列需要绑定在一个模式上，#匹配一个词或多个词，*只匹配一个词。 应用场景： 同上，iphone促销活动可以接收主题为iphone的消息，如iphone12、iphone13等 远程过程调用（RPC） 如果我们需要在远程计算机上运行功能并等待结果就可以使用RPC，具体流程可以看图。应用场景：需要等待接口返回数据，如订单支付 发布者确认（Publisher Confirms）与发布者进行可靠的发布确认，发布者确认是RabbitMQ扩展，可以实现可靠的发布。在通道上启用发布者确认后，RabbitMQ将异步确认发送者发布的消息，这意味着它们已在服务器端处理。（搜索公众号Java知音，回复“2021”，送你一份Java面试题宝典） 应用场景： 对于消息可靠性要求较高，比如钱包扣款 代码演示代码中没有对后面两种模式演示，有兴趣可以自己研究 简单模式12345678910111213141516171819202122232425262728293031323334353637import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Sender { private final static String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] args) throws IOException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // 声明队列 // queue：队列名 // durable：是否持久化 // exclusive：是否排外 即只允许该channel访问该队列 一般等于true的话用于一个队列只能有一个消费者来消费的场景 // autoDelete：是否自动删除 消费完删除 // arguments：其他属性 channel.queueDeclare(QUEUE_NAME, false, false, false, null); //消息内容 String message = &quot;simplest mode message&quot;; channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot;[x]Sent '&quot; + message + &quot;'&quot;); //最后关闭通关和连接 channel.close(); connection.close(); }} 1234567891011121314151617181920212223242526272829303132import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Receiver { private final static String QUEUE_NAME = &quot;simplest_queue&quot;; public static void main(String[] args) throws IOException, InterruptedException, TimeoutException { // 获取连接 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + delivery.getEnvelope().getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; { }); }} 工作队列模式123456789101112131415161718192021222324252627282930313233import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Receiver1 { private final static String QUEUE_NAME = &quot;queue_work&quot;; public static void main(String[] args) throws IOException, InterruptedException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 同一时刻服务器只会发送一条消息给消费者 channel.basicQos(1); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + delivery.getEnvelope().getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; { }); }} 1234567891011121314151617181920212223242526272829303132333435import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Receiver2 { private final static String QUEUE_NAME = &quot;queue_work&quot;; public static void main(String[] args) throws IOException, InterruptedException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 同一时刻服务器只会发送一条消息给消费者 channel.basicQos(1); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + delivery.getEnvelope().getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; { }); }} 1234567891011121314151617181920212223242526272829303132import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Sender { private final static String QUEUE_NAME = &quot;queue_work&quot;; public static void main(String[] args) throws IOException, InterruptedException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); for (int i = 0; i &lt; 100; i++) { String message = &quot;work mode message&quot; + i; channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot;[x] Sent '&quot; + message + &quot;'&quot;); Thread.sleep(i * 10); } channel.close(); connection.close(); }} 发布订阅模式123456789101112131415161718192021222324252627282930313233import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;public class Receive1 { private static final String EXCHANGE_NAME = &quot;logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, &quot;fanout&quot;); String queueName = channel.queueDeclare().getQueue(); channel.queueBind(queueName, EXCHANGE_NAME, &quot;&quot;); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); // 订阅消息的回调函数 DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); }; // 消费者，有消息时出发订阅回调函数 channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; { }); }} 123456789101112131415161718192021222324252627282930313233import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;public class Receive2 { private static final String EXCHANGE_NAME = &quot;logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, &quot;fanout&quot;); String queueName = channel.queueDeclare().getQueue(); channel.queueBind(queueName, EXCHANGE_NAME, &quot;&quot;); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); // 订阅消息的回调函数 DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received2 '&quot; + message + &quot;'&quot;); }; // 消费者，有消息时出发订阅回调函数 channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; { }); }} 12345678910111213141516171819202122232425import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class Sender { private static final String EXCHANGE_NAME = &quot;logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, &quot;fanout&quot;); String message = &quot;publish subscribe message&quot;; channel.basicPublish(EXCHANGE_NAME, &quot;&quot;, null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot; [x] Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); }} 路由模式123456789101112131415161718192021222324252627282930313233343536373839import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Receiver1 { private final static String QUEUE_NAME = &quot;queue_routing&quot;; private final static String EXCHANGE_NAME = &quot;exchange_direct&quot;; public static void main(String[] args) throws IOException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 指定路由的key，接收key和key2 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;key&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;key2&quot;); channel.basicQos(1); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + delivery.getEnvelope().getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; { }); }} 123456789101112131415161718192021222324252627282930313233343536373839import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Receiver2 { private final static String QUEUE_NAME = &quot;queue_routing2&quot;; private final static String EXCHANGE_NAME = &quot;exchange_direct&quot;; public static void main(String[] args) throws IOException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 仅接收key2 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;key2&quot;); channel.basicQos(1); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + delivery.getEnvelope().getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; { }); }} 12345678910111213141516171819202122232425262728293031323334import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Sender { private final static String EXCHANGE_NAME = &quot;exchange_direct&quot;; private final static String EXCHANGE_TYPE = &quot;direct&quot;; public static void main(String[] args) throws IOException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // 交换机声明 channel.exchangeDeclare(EXCHANGE_NAME, EXCHANGE_TYPE); // 只有routingKey相同的才会消费 String message = &quot;routing mode message&quot;; channel.basicPublish(EXCHANGE_NAME, &quot;key2&quot;, null, message.getBytes()); System.out.println(&quot;[x] Sent '&quot; + message + &quot;'&quot;);// channel.basicPublish(EXCHANGE_NAME, &quot;key&quot;, null, message.getBytes());// System.out.println(&quot;[x] Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); }} 主题模式123456789101112131415161718192021222324252627282930313233343536import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Receiver1 { private final static String QUEUE_NAME = &quot;queue_topic&quot;; private final static String EXCHANGE_NAME = &quot;exchange_topic&quot;; public static void main(String[] args) throws IOException, InterruptedException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 可以接收key.1 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;key.*&quot;); channel.basicQos(1); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + delivery.getEnvelope().getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; { }); }} 1234567891011121314151617181920212223242526272829303132333435363738import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.DeliverCallback;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Receiver2 { private final static String QUEUE_NAME = &quot;queue_topic2&quot;; private final static String EXCHANGE_NAME = &quot;exchange_topic&quot;; private final static String EXCHANGE_TYPE = &quot;topic&quot;; public static void main(String[] args) throws IOException, InterruptedException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); // *号代表单个单词，可以接收key.1 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;*.*&quot;); // #号代表多个单词，可以接收key.1.2 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;*.#&quot;); channel.basicQos(1); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + delivery.getEnvelope().getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; { }); }} 123456789101112131415161718192021222324252627282930313233import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Sender { private final static String EXCHANGE_NAME = &quot;exchange_topic&quot;; private final static String EXCHANGE_TYPE = &quot;topic&quot;; public static void main(String[] args) throws IOException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, EXCHANGE_TYPE); String message = &quot;topics model message with key.1&quot;; channel.basicPublish(EXCHANGE_NAME, &quot;key.1&quot;, null, message.getBytes()); System.out.println(&quot;[x] Sent '&quot; + message + &quot;'&quot;); String message2 = &quot;topics model message with key.1.2&quot;; channel.basicPublish(EXCHANGE_NAME, &quot;key.1.2&quot;, null, message2.getBytes()); System.out.println(&quot;[x] Sent '&quot; + message2 + &quot;'&quot;); channel.close(); connection.close(); }} 四种交换机介绍 直连交换机（Direct exchange）： 具有路由功能的交换机，绑定到此交换机的时候需要指定一个routing_key，交换机发送消息的时候需要routing_key，会将消息发送道对应的队列 扇形交换机（Fanout exchange）： 广播消息到所有队列，没有任何处理，速度最快 主题交换机（Topic exchange）： 在直连交换机基础上增加模式匹配，也就是对routing_key进行模式匹配，*代表一个单词，#代表多个单词 首部交换机（Headers exchange）： 忽略routing_key，使用Headers信息（一个Hash的数据结构）进行匹配，优势在于可以有更多更灵活的匹配规则 总结这么多种队列模式中都有其应用场景，大家可以根据应用场景示例中进行选择 参考 RabbitMQ官方教程 官方教程源码","link":"/rabbitmq-model/"},{"title":"smart-doc+torna搭建企业级接口管理系统","text":"torna搭配smart-doc搭建企业级接口管理系统 torna配置数据库 在数据库中新建名为torna的数据库，并创建torna用户权限 1234567891011# 新建数据库CREATE DATABASE `torna` CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_0900_ai_ci'# 创建用户CREATE USER 'torna'@'%' IDENTIFIED BY 'passwd';# 给用户赋予数据库权限GRANT ALL PRIVILEGES ON torna.* TO 'torna'@'%';# 刷新权限FLUSH PRIVILEGES; 创建表，构建脚本从mysql.sql下载。 下载镜像1docker pull tanghc2020/torna 启动123456789docker run --name torna --restart=always \\ -p 7700:7700 \\ -e JAVA_OPTS=&quot;-Xms256m -Xmx256m&quot; \\ -e MYSQL_HOST=&quot;192.168.0.1:3306&quot; \\ -e MYSQL_SCHEMA=&quot;torna&quot; \\ -e MYSQL_USERNAME=&quot;torna&quot; \\ -e MYSQL_PASSWORD='passwd' \\ -e torna.jwt.secret=&quot;jwtsecret&quot; -d tanghc2020/torna 登录访问localhost:7700 查看是否正常，默认账号密码 admin 123456 获取token登录后创建空间-项目-模块，在OpenAPI中获取到token复制备用。 smart-doc以gradle配置为例 添加并配置gradle插件123456789101112131415161718192021222324252627282930313233343536373839plugins { id 'org.springframework.boot' version '2.6.4' id 'io.spring.dependency-management' version '1.0.11.RELEASE' id &quot;com.github.shalousun.smart-doc&quot; version &quot;2.4.0&quot; id 'java'}group = 'com.example'version = '0.0.1-SNAPSHOT'sourceCompatibility = '11'configurations { compileOnly { extendsFrom annotationProcessor }}repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral()}dependencies { implementation('org.springframework.boot:spring-boot-starter-web') compileOnly 'org.projectlombok:lombok' annotationProcessor 'org.projectlombok:lombok'}tasks.named('test') { useJUnitPlatform()}smartdoc { configFile = file(&quot;src/main/resources/smart-doc.json&quot;) // include 'com.baomidou:mybatis-plus-extension' //如果添加了mybatis plus} 配置smart-doc.json1234567891011121314{ &quot;isStrict&quot;: false, &quot;pathPrefix&quot;: &quot;/&quot;,//是否开启严格模式 &quot;outPath&quot;: &quot;doc&quot;, //指定文档的输出路径,maven插件不需要，gradle插件必须 &quot;packageFilters&quot;: &quot;&quot;, //controller包过滤，多个包用英文逗号隔开 &quot;projectName&quot;: &quot;test&quot;, //配置自己的项目名称 &quot;appToken&quot;: &quot;上面获取到的token&quot;,//torna平台appToken,@since 2.0.9 &quot;openUrl&quot;: &quot;http://localhost:7700/api&quot;, //torna平台地址，填写自己的私有化部署地址@since 2.0.9 &quot;debugEnvName&quot;: &quot;测试环境&quot;, //torna测试环境 &quot;replace&quot;: true, //推送torna时替换旧的文档 &quot;debugEnvUrl&quot;: &quot;http://127.0.0.1&quot;, //torna inlineEnum&quot;:true //设置为true会将枚举详情展示到参数表中，默认关闭，//@since 1.8.8版本开始} 根据实际情况修改 推送接口文档","link":"/smart-doc-torna/"}],"tags":[{"name":"alpine","slug":"alpine","link":"/tags/alpine/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"时区","slug":"时区","link":"/tags/%E6%97%B6%E5%8C%BA/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"缓存","slug":"缓存","link":"/tags/%E7%BC%93%E5%AD%98/"},{"name":"arthas","slug":"arthas","link":"/tags/arthas/"},{"name":"运维","slug":"运维","link":"/tags/%E8%BF%90%E7%BB%B4/"},{"name":"alfred","slug":"alfred","link":"/tags/alfred/"},{"name":"workflow","slug":"workflow","link":"/tags/workflow/"},{"name":"show hosts","slug":"show-hosts","link":"/tags/show-hosts/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"docker compose","slug":"docker-compose","link":"/tags/docker-compose/"},{"name":"gitea","slug":"gitea","link":"/tags/gitea/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"云盘","slug":"云盘","link":"/tags/%E4%BA%91%E7%9B%98/"},{"name":"免费证书","slug":"免费证书","link":"/tags/%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"acme.sh","slug":"acme-sh","link":"/tags/acme-sh/"},{"name":"DateGrip","slug":"DateGrip","link":"/tags/DateGrip/"},{"name":"Idea","slug":"Idea","link":"/tags/Idea/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"时间","slug":"时间","link":"/tags/%E6%97%B6%E9%97%B4/"},{"name":"http","slug":"http","link":"/tags/http/"},{"name":"idea","slug":"idea","link":"/tags/idea/"},{"name":"Jetbrains","slug":"Jetbrains","link":"/tags/Jetbrains/"},{"name":"激活","slug":"激活","link":"/tags/%E6%BF%80%E6%B4%BB/"},{"name":"IntelliJ IDEA","slug":"IntelliJ-IDEA","link":"/tags/IntelliJ-IDEA/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"问题排查","slug":"问题排查","link":"/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"name":"分布式锁","slug":"分布式锁","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"索引","slug":"索引","link":"/tags/%E7%B4%A2%E5%BC%95/"},{"name":"navicat","slug":"navicat","link":"/tags/navicat/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"rabbitmq","slug":"rabbitmq","link":"/tags/rabbitmq/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"命令","slug":"命令","link":"/tags/%E5%91%BD%E4%BB%A4/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"锁","slug":"锁","link":"/tags/%E9%94%81/"},{"name":"sdkman","slug":"sdkman","link":"/tags/sdkman/"},{"name":"seaweedfs","slug":"seaweedfs","link":"/tags/seaweedfs/"},{"name":"小文件服务器","slug":"小文件服务器","link":"/tags/%E5%B0%8F%E6%96%87%E4%BB%B6%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"内网穿透","slug":"内网穿透","link":"/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"命令行","slug":"命令行","link":"/tags/%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"name":"bash","slug":"bash","link":"/tags/bash/"},{"name":"VPN","slug":"VPN","link":"/tags/VPN/"},{"name":"vpncmd","slug":"vpncmd","link":"/tags/vpncmd/"},{"name":"vps","slug":"vps","link":"/tags/vps/"},{"name":"systemd","slug":"systemd","link":"/tags/systemd/"},{"name":"systemctl","slug":"systemctl","link":"/tags/systemctl/"},{"name":"xxl-job","slug":"xxl-job","link":"/tags/xxl-job/"},{"name":"spring boot","slug":"spring-boot","link":"/tags/spring-boot/"},{"name":"windows","slug":"windows","link":"/tags/windows/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"中间件","slug":"中间件","link":"/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/%E5%AD%A6%E4%B9%A0/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]}